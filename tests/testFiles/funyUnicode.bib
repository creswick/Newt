@article{vanDeursen2000,
    abstract = {We survey the literature available on the topic of domain-specific languages as used for the construction and maintenance of software systems. We list a selection of 75 key publications in the area, and provide a summary for each of the papers. Moreover, we discuss terminology, risks and benefits, example domain-specific languages, design methodologies, and implementation techniques.},
    address = {New York, NY, USA},
    author = {van Deursen, Arie and Klint, Paul and Visser, Joost},
    citeulike-article-id = {143130},
    citeulike-linkout-0 = {http://portal.acm.org/citation.cfm?id=352035},
    citeulike-linkout-1 = {http://dx.doi.org/10.1145/352029.352035},
    doi = {10.1145/352029.352035},
    issn = {0362-1340},
    journal = {SIGPLAN Not.},
    keywords = {dsl, dsls},
    month = {June},
    number = {6},
    pages = {26--36},
    posted-at = {2010-06-10 22:25:32},
    priority = {2},
    publisher = {ACM},
    title = {Domain-specific languages: an annotated bibliography},
    url = {http://dx.doi.org/10.1145/352029.352035},
    volume = {35},
    year = {2000}
}

@inproceedings{peytonJones2000:composing,
    abstract = {Financial and insurance contracts do not sound like promising territory for functional programming and formal semantics, but in fact we have discovered that insights from programming languages bear directly on the complex subject of describing and valuing a large class of contracts.We introduce a combinator library that allows us to describe such contracts precisely, and a compositional denotational semantics that says what such contracts are worth. We sketch an implementation of our combinator library in Haskell. Interestingly, lazy evaluation plays a crucial role.},
    address = {New York, NY, USA},
    author = {Jones, Simon P. and Eber, Jean M. and Seward, Julian},
    booktitle = {ICFP '00: Proc. of the fifth ACM SIGPLAN intl. conf. on Functional programming},
    citeulike-article-id = {4221107},
    citeulike-linkout-0 = {http://portal.acm.org/citation.cfm?id=351240.351267},
    citeulike-linkout-1 = {http://dx.doi.org/10.1145/351240.351267},
    doi = {10.1145/351240.351267},
    isbn = {1-58113-202-6},
    keywords = {dsl, dsls},
    pages = {280--292},
    posted-at = {2010-06-10 22:24:16},
    priority = {2},
    publisher = {ACM},
    title = {Composing contracts: an adventure in financial engineering (functional pearl)},
    url = {http://dx.doi.org/10.1145/351240.351267},
    year = {2000}
}

@article{mernikEtAl2005:dsl,
    abstract = {Domain-specific languages (DSLs) are languages tailored to a specific application domain. They offer substantial gains in expressiveness and ease of use compared with general-purpose programming languages in their domain of application. DSL development is hard, requiring both domain knowledge and language development expertise. Few people have both. Not surprisingly, the decision to develop a DSL is often postponed indefinitely, if considered at all, and most DSLs never get beyond the application library stage.Although many articles have been written on the development of particular DSLs, there is very limited literature on DSL development methodologies and many questions remain regarding when and how to develop a DSL. To aid the DSL developer, we identify patterns in the decision, analysis, design, and implementation phases of DSL development. Our patterns improve and extend earlier work on DSL design patterns. We also discuss domain analysis tools and language development systems that may help to speed up DSL development. Finally, we present a number of open problems.},
    address = {New York, NY, USA},
    author = {Mernik, Marjan and Heering, Jan and Sloane, Anthony M.},
    citeulike-article-id = {539809},
    citeulike-linkout-0 = {http://portal.acm.org/citation.cfm?id=1118890.1118892},
    citeulike-linkout-1 = {http://dx.doi.org/10.1145/1118890.1118892},
    doi = {10.1145/1118890.1118892},
    issn = {0360-0300},
    journal = {ACM Comput. Surv.},
    keywords = {dsl, dsls},
    month = {December},
    number = {4},
    pages = {316--344},
    posted-at = {2010-06-10 22:22:53},
    priority = {2},
    publisher = {ACM},
    title = {When and how to develop domain-specific languages},
    url = {http://dx.doi.org/10.1145/1118890.1118892},
    volume = {37},
    year = {2005}
}

@inproceedings{hudak1998,
    address = {Washington, DC, USA},
    author = {Hudak, P.},
    booktitle = {ICSR '98: Proc. of the 5th Intl. Conf. on Software Reuse},
    citeulike-article-id = {3081457},
    citeulike-linkout-0 = {http://portal.acm.org/citation.cfm?id=853532},
    isbn = {0-8186-8377-5},
    keywords = {dsl, dsls},
    pages = {134+},
    posted-at = {2010-06-10 22:21:37},
    priority = {2},
    publisher = {IEEE},
    title = {Modular Domain Specific Languages and Tools},
    url = {http://portal.acm.org/citation.cfm?id=853532},
    year = {1998}
}

@article{hudak1996,
    abstract = {Note: OCR errors may be found in this Reference List extracted from the full text article.  ACM has opted to expose the complete List rather than only correct and linked references.},
    address = {New York, NY, USA},
    author = {Hudak, Paul},
    citeulike-article-id = {143132},
    citeulike-linkout-0 = {http://portal.acm.org/citation.cfm?id=242477},
    citeulike-linkout-1 = {http://dx.doi.org/10.1145/242224.242477},
    doi = {10.1145/242224.242477},
    issn = {0360-0300},
    journal = {ACM Comput. Surv.},
    keywords = {dsl, dsls, edsl},
    number = {4es},
    pages = {196+},
    posted-at = {2010-06-10 22:20:11},
    priority = {2},
    publisher = {ACM},
    title = {Building domain-specific embedded languages},
    url = {http://dx.doi.org/10.1145/242224.242477},
    volume = {28},
    year = {1996}
}

@inproceedings{harrisonAndHarrison2004:dsl,
    abstract = {Bioinformatics is the application of Computer Science techniques to problems in Biology, and this paper explores one such application with great potential: the modeling of life cycles of autonomous, intercommunicating cellular systems using domain-specific programming languages (DSLs). We illustrate this approach for the simple photo-synthetic bacterium R. Sphaeroides with a DSL called CellSys embedded in the programming language Haskell. 1.},
    author = {Harrison, William L. and Harrison, Robert W.},
    booktitle = {Proc. of the 26th Annual IEEE Intl. Conf. on Engineering in Medicine and Biology},
    citeulike-article-id = {7291526},
    citeulike-linkout-0 = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.111.8937},
    keywords = {dsl, dsls},
    pages = {3019--3022},
    posted-at = {2010-06-10 22:18:35},
    priority = {2},
    title = {Domain specific languages for cellular interactions},
    url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.111.8937},
    volume = {2004},
    year = {2004}
}

@book{feldman1979:make,
    address = {NJ},
    author = {Feldman, S. I.},
    booktitle = {UNIX Programmer's Manual},
    citeulike-article-id = {7291525},
    keywords = {dsl, unix},
    month = {January},
    posted-at = {2010-06-10 22:16:15},
    priority = {2},
    publisher = {Murray Hill},
    title = {Make--a program for maintaining computer programs},
    volume = {2a},
    year = {1979}
}

@book{bourne1979:unixShell,
    address = {NJ},
    author = {Bourne, S. R.},
    booktitle = {UNIX Programmer's Manual},
    citeulike-article-id = {7291520},
    edition = {7},
    keywords = {dsl, unix},
    month = {January},
    posted-at = {2010-06-10 22:13:56},
    priority = {2},
    publisher = {Murray Hill},
    title = {An Introduction to the UNIX Shell},
    volume = {2a},
    year = {1979}
}

@techreport{Tullsen:1996,
    abstract = {Two things have been accomplished in this work: the first is an implementation of a Haskell to Java compiler. The Glasgow Haskell compiler is used as a front end and outputs an intermediate language called the Spineless Tagless G-Machine language which is then parsed and then translated to Java source code. The Java compiler is then used to generate byte codes for the Java Virtual Machine. The compiler currently does not implement the IO monad nor all primitive functions to fully support Haskell. The second accomplishment is an investigation of the representation decisions in compiling a lazy, polymorphic, functional programming language to the Java Virtual Machine (e.g., representing thunks, first class functions, and algebraic data types). The salient properties of the Java Virtual Machine which influence these representation decisions are the following: it has no first class functions, no pointers, and no algebraic data types; it is dynamically typed and is garbage collected.},
    author = {Tullsen, Mark},
    citeulike-article-id = {7291470},
    institution = {Yale University},
    keywords = {dsls, haskell},
    location = {haskell-to-java.ps},
    month = {May},
    number = {YALEU/DCS/RR-1204},
    posted-at = {2010-06-10 20:50:14},
    priority = {2},
    title = {Compiling Haskell to Java},
    year = {1996}
}

@inproceedings{Tullsen-Hudak:PEPM99,
    abstract = {The best known approach to program transformation is the unfold/fold methodology of Burstall and Darlington: a simple, intuitive, and expressive approach which serves as the basis of many automatic program transformation algorithms (such as partial evaluation and deforestation). Unfortunately unfold/fold does not preserve total correctness and requires maintaining a transformation history of the program. Scherlis invented a similar approach, expression procedures, which solved these two problems: expression procedures preserve total correctness and require no transformation history. Motivated by our desire to make expression procedures more expressive by eliminating their one-directional nature (they are designed to specialize but not to generalize functions), we have developed an equational specification of expression procedures, in which the essence of expression procedures is expressed in a single transformation rule. Our approach has the following advantages over expression procedures: (1) all program derivations are reversible; (2) transformations can be done which expression procedures cannot do; (3) fewer and simpler rules are used; and (4) the proof of correctness is simpler.},
    address = {San Antonio, Texas},
    author = {Tullsen, Mark and Hudak, Paul},
    booktitle = {Proc. of the {ACM} {SIGPLAN} Workshop on Partial Evaluation and Semantics-Based Program Manipulation},
    citeulike-article-id = {7291469},
    editor = {Danvy, Olivier},
    keywords = {dsls, haskell},
    location = {shifting.ps},
    month = {January},
    pages = {95--104},
    posted-at = {2010-06-10 20:50:14},
    priority = {2},
    series = {Technical report BRICS-NS-99-1, University of Aarhus},
    title = {Shifting expression procedures into reverse},
    year = {1999}
}

@inproceedings{Tullsen:PADL2000,
    abstract = {Pattern matching is a great convenience in programming. However, pattern matching has its problems: it conflicts with data abstraction; it is complex (at least in Haskell, which has pattern guards, irrefutable patterns, n+k patterns, as patterns, etc.); it is a source of run-time errors; and lastly, one cannot abstract over patterns as they are not a first class language construct.This paper proposes a simplification of pattern matching that makes patterns first class. The key idea is to treat patterns as functions of type ``a -> Maybe b'' ---i.e., ``a -> (Nothing|Just b)''; thus, patterns and pattern combinators can be written as functions in the language.},
    author = {Tullsen, Mark},
    booktitle = {Practical Aspects of Declarative Languages, Second Intl. Workshop, PADL 2000},
    citeulike-article-id = {7291468},
    editor = {Pontelli, E. and Costa, V. Santos},
    keywords = {dsls, haskell},
    location = {patterns.ps},
    month = {January},
    pages = {1--15},
    posted-at = {2010-06-10 20:50:14},
    priority = {2},
    publisher = {Springer-Verlag},
    series = {Lecture Notes in Computer Science},
    title = {First Class Patterns},
    volume = {1753},
    year = {2000}
}

@inproceedings{Tullsen:MPC2000,
    abstract = {Many have recognized the need for genericity in programming and program transformation. Genericity over data types has been achieved with polymorphism. Genericity over type constructors, often called polytypism, is an area of active research. This paper proposes that another kind of genericity is needed: genericity over the length of tuples. Untyped languages allow for such genericity but typed languages do not (except for languages allowing dependent types). The contribution of this paper is to present the ``zip calculus,'' a typed lambda calculus that provides genericity over the length of tuples and yet does not require the full generality of dependent types.},
    author = {Tullsen, Mark},
    booktitle = {Mathematics of Program Construction, 5th Intl. Conf., MPC 2000},
    citeulike-article-id = {7291467},
    editor = {Backhouse, Roland and Oliveira, Jose N.},
    keywords = {dsls, haskell},
    location = {zip-mpc.ps},
    month = {July},
    pages = {28--44},
    posted-at = {2010-06-10 20:50:14},
    priority = {2},
    publisher = {Springer-Verlag},
    series = {Lecture Notes in Computer Science},
    title = {The Zip Calculus},
    volume = {1837},
    year = {2000}
}

@phdthesis{Tullsen:Thesis2002,
    author = {Tullsen, Mark},
    citeulike-article-id = {7291466},
    keywords = {dsls, haskell},
    month = {May},
    posted-at = {2010-06-10 20:50:14},
    priority = {2},
    school = {Yale University},
    title = {PATH, A Program Transformation System for Haskell},
    year = {2002}
}

@inproceedings{eisensteinEtAl2001:mobile,
    abstract = {Mobile computing poses a series of unique challenges for user interface design and development: user interfaces must now accommodate the capabilities of various access devices and be suitable for different contexts of use, while preserving consistency and usability. We propose a set of techniques that will aid UI designers who are working in the domain of mobile computing. These techniques will allow designers to build UIs across several platforms, while respecting the unique constraints posed by each platform. In addition, these techniques will help designers to recognize and accommodate the unique contexts in which mobile computing occurs. Central to our approach is the development of a user-interface model that serves to isolate those features that are common to the various contexts of use, and to specify how the user-interface should adjust when the context changes. We claim that without some abstract description of the UI, it is likely that the design and the development of user-interfaces for mobile computing will be very time consuming, error-prone or even doomed to failure.},
    address = {New York, NY, USA},
    author = {Eisenstein, Jacob and Vanderdonckt, Jean and Puerta, Angel},
    booktitle = {IUI '01: Proc. of the 6th intl. conf. on Intelligent user interfaces},
    citeulike-article-id = {684506},
    citeulike-linkout-0 = {http://portal.acm.org/citation.cfm?id=360122},
    citeulike-linkout-1 = {http://dx.doi.org/10.1145/359784.360122},
    doi = {10.1145/359784.360122},
    isbn = {1-58113-325-1},
    keywords = {interfaces, iui01, user\_interfaces},
    location = {Santa Fe, New Mexico, United States},
    pages = {69--76},
    posted-at = {2010-06-09 05:45:20},
    priority = {2},
    publisher = {ACM},
    title = {Applying model-based techniques to the development of UIs for mobile computers},
    url = {http://dx.doi.org/10.1145/359784.360122},
    year = {2001}
}

@article{puerta1997:modelBased,
    abstract = {The author describes Mobi-D (Model-Based Interface Designer), a
comprehensive environment that supports user-centered design through
model-based interface development. In the Mobi-D paradigm, a series of
declarative models, such as user-task, dialog, and presentation, are
interrelated to provide a formal representation of an interface design.
This contrasts to model-based systems, which use only one or two models
in isolation and have no explicit notion as to how the various model
elements are organized into an interface design},
    author = {Puerta, A. R.},
    citeulike-article-id = {7277729},
    citeulike-linkout-0 = {http://dx.doi.org/10.1109/52.595902},
    citeulike-linkout-1 = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=595902},
    doi = {10.1109/52.595902},
    issn = {07407459},
    journal = {IEEE Software},
    keywords = {consistency\_control, user\_interfaces},
    month = {July},
    number = {4},
    pages = {40--47},
    posted-at = {2010-06-09 05:42:26},
    priority = {2},
    title = {A model-based interface development environment},
    url = {http://dx.doi.org/10.1109/52.595902},
    volume = {14},
    year = {1997}
}

@inproceedings{puertaAndEisenstein2002:ximl,
    abstract = {We introduce XIML (eXtensible Interface Markup Language), a proposed common representation for interaction data. We claim that XIML fulfills the requirements that we have found essential for a language of its type: (1) it supports design, operation, organization, and evaluation functions, (2) it is able to relate the abstract and concrete data elements of an interface, and (3) it enables knowledge-based systems to exploit the captured data.},
    address = {New York, NY, USA},
    author = {Puerta, Angel and Eisenstein, Jacob},
    booktitle = {IUI '02: Proc. of the 7th intl. conf. on Intelligent user interfaces},
    citeulike-article-id = {2667730},
    citeulike-linkout-0 = {http://portal.acm.org/citation.cfm?id=502716.502763},
    citeulike-linkout-1 = {http://dx.doi.org/10.1145/502716.502763},
    doi = {10.1145/502716.502763},
    isbn = {1-58113-459-2},
    keywords = {consistency\_control, intelligent\_interfaces, user\_interfaces},
    location = {San Francisco, California, USA},
    pages = {214--215},
    posted-at = {2010-06-09 05:40:14},
    priority = {2},
    publisher = {ACM},
    title = {XIML: a common representation for interaction data},
    url = {http://dx.doi.org/10.1145/502716.502763},
    year = {2002}
}

@article{mahajanAndShneiderman1997:sherlock,
    abstract = {Designing user interfaces with consistent visual and textual
properties is difficult. To demonstrate the harmful effects of
inconsistency, we conducted an experiment with 60 subjects. Inconsistent
interface terminology slowed user performance by 10 to 25 percent.
Unfortunately, contemporary software tools provide only modest support
for consistency control. Therefore, we developed SHERLOCK, a family of
consistency analysis tools, which evaluates visual and textual
properties of user interfaces. It provides graphical analysis tools such
as a dialog box summary table that presents a compact overview of visual
properties of all dialog boxes. SHERLOCK provides terminology analysis
tools including an interface concordance, an interface spellchecker, and
terminology baskets to check for inconsistent use of familiar groups of
terms. Button analysis tools include a button concordance and a button
layout table to detect variant capitalization, distinct typefaces,
distinct colors, variant button sizes, and inconsistent button
placements. We describe the design, software architecture, and the use
of SHERLOCK. We tested SHERLOCK with four commercial prototypes. The
outputs, analysis, and feedback from designers of the applications are
presented},
    address = {Piscataway, NJ, USA},
    author = {Mahajan, R. and Shneiderman, B.},
    citeulike-article-id = {683098},
    citeulike-linkout-0 = {http://portal.acm.org/citation.cfm?id=269861},
    citeulike-linkout-1 = {http://dx.doi.org/10.1109/32.637386},
    citeulike-linkout-2 = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=637386},
    doi = {10.1109/32.637386},
    issn = {00985589},
    journal = {IEEE Trans. on Software Engineering},
    keywords = {consistency\_control, interfaces, user\_interfaces},
    month = {Nov},
    number = {11},
    OPTpages = {722--735},
    posted-at = {2010-06-09 05:37:54},
    priority = {2},
    publisher = {IEEE Press},
    title = {Visual and textual consistency checking tools for graphical user interfaces},
    url = {http://dx.doi.org/10.1109/32.637386},
    volume = {23},
    year = {1997}
}

@article{phelpsWilinsky2000:robustHyperlinks,
    author = {Phelps, T. A. and Wilensky, R.},
    citeulike-article-id = {3040714},
    citeulike-linkout-0 = {http://www.dlib.org/dlib/july00/wilensky/07wilensky.html},
    journal = {D-Lib Magazine},
    keywords = {hyperlinks, nlp, phishing, tf-idf},
    month = {July},
    number = {7/8},
    posted-at = {2010-06-08 22:50:00},
    priority = {2},
    title = {Robust Hyperlinks and Locations},
    url = {http://www.dlib.org/dlib/july00/wilensky/07wilensky.html},
    volume = {6},
    year = {2000}
}

@book{jakobssonAndMyers2006:phishing,
    citeulike-article-id = {7266004},
    citeulike-linkout-0 = {http://www.amazon.ca/exec/obidos/redirect?tag=citeulike09-20\&amp;path=ASIN/0471782459},
    citeulike-linkout-1 = {http://www.amazon.de/exec/obidos/redirect?tag=citeulike01-21\&amp;path=ASIN/0471782459},
    citeulike-linkout-2 = {http://www.amazon.fr/exec/obidos/redirect?tag=citeulike06-21\&amp;path=ASIN/0471782459},
    citeulike-linkout-3 = {http://www.amazon.jp/exec/obidos/ASIN/0471782459},
    citeulike-linkout-4 = {http://www.amazon.co.uk/exec/obidos/ASIN/0471782459/citeulike00-21},
    citeulike-linkout-5 = {http://www.amazon.com/exec/obidos/redirect?tag=citeulike07-20\&path=ASIN/0471782459},
    citeulike-linkout-6 = {http://www.worldcat.org/isbn/0471782459},
    citeulike-linkout-7 = {http://books.google.com/books?vid=ISBN0471782459},
    citeulike-linkout-8 = {http://www.amazon.com/gp/search?keywords=0471782459\&index=books\&linkCode=qs},
    citeulike-linkout-9 = {http://www.librarything.com/isbn/0471782459},
    day = {15},
    howpublished = {Hardcover},
    isbn = {0471782459},
    keywords = {book, email, phishing},
    month = {December},
    posted-at = {2010-06-08 02:07:27},
    priority = {2},
    publisher = {Wiley-Interscience},
    title = {Phishing and Countermeasures: Understanding the Increasing Problem of Electronic Identity Theft},
    url = {http://www.amazon.com/exec/obidos/redirect?tag=citeulike07-20\&path=ASIN/0471782459},
    year = {2006}
}

@inproceedings{cockburnMcKenzie2002:spatialMem,
    abstract = {User interfaces can improve task performance by exploiting the powerful human capabilities for spatial cognition. This opportunity has been demonstrated by many prior experiments. It is tempting to believe that providing greater spatial flexibility-by moving from flat 2D to 3D user interfaces-will further enhance user performance. This paper describes an experiment that investigates the effectiveness of spatial memory in real-world physical models and in equivalent computer-based virtual systems. The different models vary the user's freedom to use depth and perspective in spatial arrangements of images representing web pages. Results show that the subjects' performance deteriorated in both the physical and virtual systems as their freedom to locate items in the third dimension increased. Subjective measures reinforce the performance measures, indicating that users found interfaces with higher dimensions more 'cluttered' and less efficient},
    address = {New York, NY, USA},
    author = {Cockburn, Andy and McKenzie, Bruce},
    booktitle = {CHI '02: Proc. of the SIGCHI conf. on Human factors in computing systems},
    citeulike-article-id = {622857},
    citeulike-linkout-0 = {http://portal.acm.org/citation.cfm?id=503413},
    citeulike-linkout-1 = {http://dx.doi.org/10.1145/503376.503413},
    doi = {10.1145/503376.503413},
    isbn = {1-58113-453-3},
    keywords = {spatial\_cognition, usability, user\_interfaces},
    location = {Minneapolis, Minnesota, USA},
    pages = {203--210},
    posted-at = {2010-06-05 22:30:12},
    priority = {2},
    publisher = {ACM},
    title = {Evaluating the effectiveness of spatial memory in 2D and 3D physical and virtual environments},
    url = {http://dx.doi.org/10.1145/503376.503413},
    year = {2002}
}

@article{delineEtAl2006:codeThumbs,
    author = {DeLine, Robert and Czerwinski, Mary and Meyers, Brian and Venolia, Gina and Drucker, Steven and Robertson, George},
    booktitle = {Visual Languages and Human-Centric Computing (VL/HCC'06)},
    citeulike-article-id = {7258203},
    citeulike-linkout-0 = {http://doi.ieeecomputersociety.org/10.1109/VLHCC.2006.14},
    keywords = {spatial\_memory, usability, vlhcc06},
    pages = {11--18},
    posted-at = {2010-06-05 22:24:35},
    priority = {2},
    title = {Code Thumbnails: Using Spatial Memory to Navigate Source Code},
    url = {http://doi.ieeecomputersociety.org/10.1109/VLHCC.2006.14},
    year = {2006}
}

@article{nicholsTwidale2003:OSSusability,
    author = {Nichols, David and Twidale, Michael},
    citeulike-article-id = {7258199},
    journal = {First Monday},
    keywords = {interface\_design, open\_source\_software},
    number = {1-6},
    posted-at = {2010-06-05 22:02:11},
    priority = {2},
    title = {The Usability of Open Source Software},
    volume = {8},
    year = {2003}
}

@inproceedings{terryEtAl2010:OSSusability,
    abstract = {This paper presents results from a study examining perceptions and practices of usability in the free/open source software (FOSS) community. 27 individuals associated with 11 different FOSS projects were interviewed to understand how they think about, act on, and are motivated to address usability issues. Our results indicate that FOSS project members possess rather sophisticated notions of software usability, which collectively mirror definitions commonly found in HCI textbooks. Our study also uncovered a wide range of practices that ultimately work to improve software usability. Importantly, these activities are typically based on close, direct interpersonal relationships between developers and their core users, a group of users who closely follow the project and provide high quality, respected feedback. These relationships, along with positive feedback from other users, generate social rewards that serve as the primary motivations for attending to usability issues on a day-to-day basis. These findings suggest a need to reconceptualize HCI methods to better fit this culture of practice and its corresponding value system.},
    address = {New York, NY, USA},
    author = {Terry, Michael and Kay, Matthew and Lafreniere, Ben},
    booktitle = {CHI '10: Proc. of the 28th intl. conf. on Human factors in computing systems},
    citeulike-article-id = {7241605},
    citeulike-linkout-0 = {http://portal.acm.org/citation.cfm?id=1753326.1753476},
    citeulike-linkout-1 = {http://dx.doi.org/10.1145/1753326.1753476},
    doi = {10.1145/1753326.1753476},
    isbn = {978-1-60558-929-9},
    keywords = {chi10, open\_source\_software, usability},
    location = {Atlanta, Georgia, USA},
    pages = {999--1008},
    posted-at = {2010-06-03 20:20:41},
    priority = {2},
    publisher = {ACM},
    title = {Perceptions and practices of usability in the free/open source software (FoSS) community},
    url = {http://dx.doi.org/10.1145/1753326.1753476},
    year = {2010}
}

@article{citeulike:7241585,
    abstract = {In this paper, we discuss our experience in offering a usability course with projects taken from an active open source software development project. We describe what was done in the class inside the larger context of the usability of open source software. We conclude with an invitation for others to adopt this model and use it for their own purposes.},
    address = {New York, NY, USA},
    author = {Hepting, Daryl H. and Peng, Lijuan and Maciag, Timothy J. and Gerhard, David and Maguire, Brien},
    citeulike-article-id = {7241585},
    citeulike-linkout-0 = {http://portal.acm.org/citation.cfm?id=1383602.1383649},
    citeulike-linkout-1 = {http://dx.doi.org/10.1145/1383602.1383649},
    doi = {10.1145/1383602.1383649},
    issn = {0097-8418},
    journal = {SIGCSE Bull.},
    keywords = {usability},
    number = {2},
    pages = {120--123},
    posted-at = {2010-06-03 20:17:58},
    priority = {2},
    publisher = {ACM},
    title = {Creating synergy between usability courses and open source software projects},
    url = {http://dx.doi.org/10.1145/1383602.1383649},
    volume = {40},
    year = {2008}
}

@article{dagitEtAl2006:cogdims,
    abstract = {Many researchers have analyzed visual language design using Cognitive Dimensions (CDs), but some have reinterpreted the purpose, vocabulary, and use of CDs, potentially creating confusion. In particular, those who have used CDs to convince themselves or others that their language is usable have tended to ignore or downplay the tradeoffs inherent in design, resulting in evaluations that provide few insights. Researchers who do not consider who , when , and how best to analyze a visual language using CDs are likely to miss the most useful opportunities to uncover problems in their visual languages. In this paper, we consider common breakdowns when using CDs in analysis. Then, using three case studies, we demonstrate how the who , when , and how circumstances under which CDs are applied impact the gains that can be expected.},
    author = {Dagit, J. and Lawrance, J. and Neumann, C. and Burnett, M. and Metoyer, R. and Adams, S.},
    booktitle = {Ten Years of Cognitive Dimensions},
    citeulike-article-id = {2486374},
    citeulike-linkout-0 = {http://dx.doi.org/10.1016/j.jvlc.2006.04.006},
    citeulike-linkout-1 = {http://linkinghub.elsevier.com/retrieve/pii/S1045926X06000279},
    citeulike-linkout-2 = {http://www.sciencedirect.com/science/article/B6WMM-4KF7883-1/2/eac79235fdd4088c836abe4c24eee880},
    doi = {10.1016/j.jvlc.2006.04.006},
    issn = {1045926X},
    journal = {Journal of Visual Languages \& Computing},
    keywords = {cognitive\_dimensions, language\_design},
    month = {August},
    number = {4},
    pages = {302--327},
    posted-at = {2010-06-03 19:38:26},
    priority = {2},
    title = {Using cognitive dimensions: Advice from the trenches},
    url = {http://dx.doi.org/10.1016/j.jvlc.2006.04.006},
    volume = {17},
    year = {2006}
}

@inproceedings{thovtrupAndNielsen:ui_guidelines,
    abstract = {Note: OCR errors may be found in this Reference List extracted from the full text article.  ACM has opted to expose the complete List rather than only correct and linked references.},
    address = {New York, NY, USA},
    author = {Thovtrup, Henrik and Nielsen, Jakob},
    booktitle = {CHI '91: Proc. of the SIGCHI conf. on Human factors in computing systems},
    citeulike-article-id = {4642555},
    citeulike-linkout-0 = {http://portal.acm.org/citation.cfm?id=108844.108937},
    citeulike-linkout-1 = {http://dx.doi.org/10.1145/108844.108937},
    doi = {10.1145/108844.108937},
    isbn = {0-89791-383-3},
    keywords = {interface\_design},
    location = {New Orleans, Louisiana, United States},
    pages = {335--341},
    posted-at = {2010-06-02 22:56:09},
    priority = {2},
    publisher = {ACM},
    title = {Assessing the usability of a user interface standard},
    url = {http://dx.doi.org/10.1145/108844.108937},
    year = {1991}
}

@inproceedings{MandelbaumEtAl2006:pads,
    abstract = {Enormous amounts of data exist in "well-behaved" formats such as relational tables and XML, which come equipped with extensive tool support. However, vast amounts of data also exist in non-standard or  ad hoc  data formats, which often lack standard or extensible tools. This deficiency forces data analysts to implement their own tools for parsing, querying, and analyzing their ad hoc data. The resulting tools typically interleave parsing, querying, and analysis, obscuring the semantics of the data format and making it nearly impossible for others to resuse the tools. This proposal describes PADS, an end-to-end system for processing ad hoc data sources. The core of PADS is a declarative language for describing ad hoc data sources and a data-description compiler that produces customizable libraries for parsing the ad hoc data. A suite of tools built around this core includes statistical data-profiling tools, a query engine that permits viewing ad hoc sources as XML and for querying them with XQuery, and an interactive front-end that helps users produce PADS descriptions quickly.},
    address = {New York, NY, USA},
    author = {Daly, Mark and Mandelbaum, Yitzhak and Walker, David and Fern\'{a}ndez, Mary and Fisher, Kathleen and Gruber, Robert and Zheng, Xuan},
    booktitle = {SIGMOD '06: Proc. of the 2006 ACM SIGMOD intl. conf. on Management of data},
    citeulike-article-id = {7132656},
    citeulike-linkout-0 = {http://portal.acm.org/citation.cfm?id=1142473.1142568},
    citeulike-linkout-1 = {http://dx.doi.org/10.1145/1142473.1142568},
    doi = {10.1145/1142473.1142568},
    isbn = {1-59593-434-0},
    keywords = {data\_description, data\_formats, parsing},
    location = {Chicago, IL, USA},
    pages = {727--729},
    posted-at = {2010-05-06 18:15:03},
    priority = {0},
    publisher = {ACM},
    title = {PADS: an end-to-end system for processing ad hoc data},
    url = {http://dx.doi.org/10.1145/1142473.1142568},
    year = {2006}
}

@inproceedings{citeulike:7132619,
    author = {Bergholz, Andre and Paass, Gerhard and Reichartz, Frank and Strobel, Siehyun and Moens, Marie-Francine and Witten, Brian},
    booktitle = {Proc. of the Fifth Conf. on Email and Anti-Spam, CEAS},
    citeulike-article-id = {7132619},
    day = {21},
    key = {bergholzEtAl:salting},
    keywords = {ai, ocr, phishing, spam},
    location = {Mountain View, CA},
    month = {August},
    posted-at = {2010-05-06 17:45:13},
    priority = {2},
    title = {Detecting Known and New Salting Tricks in Unwanted Emails},
    year = {2008}
}

@inproceedings{bergholzEtAl2008:modelBased,
    author = {Bergholz, Andre and Chang, Jeong-Ho and Paass, Gerhard and Reichartz, Frank and Strobel, Siehyun},
    booktitle = {Proc. of the Fifth Conf. on Email and Anti-Spam, CEAS},
    citeulike-article-id = {7132609},
    day = {21},
    keywords = {ai, machine\_learning, phishing},
    location = {Mountain View, CA},
    month = {August},
    posted-at = {2010-05-06 17:40:17},
    priority = {2},
    title = {Improved Phishing Detection using Model-Based Features},
    year = {2008}
}

@inproceedings{zhangEtAl2007:phishing,
    abstract = {Phishing is a significant problem involving fraudulent email and web sites that trick unsuspecting users into revealing private information. In this paper, we present the design, implementation, and evaluation of CANTINA, a novel, content-based approach to detecting phishing web sites, based on the TF-IDF information retrieval algorithm. We also discuss the design and evaluation of several heuristics we developed to reduce false positives. Our experiments show that CANTINA is good at detecting phishing sites, correctly labeling approximately 95\% of phishing sites.},
    address = {New York, NY, USA},
    author = {Zhang, Yue and Hong, Jason I. and Cranor, Lorrie F.},
    booktitle = {WWW '07: Proc. of the 16th intl. conf. on World Wide Web},
    citeulike-article-id = {7130911},
    citeulike-linkout-0 = {http://portal.acm.org/citation.cfm?id=1242659},
    citeulike-linkout-1 = {http://dx.doi.org/10.1145/1242572.1242659},
    doi = {10.1145/1242572.1242659},
    isbn = {978-1-59593-654-7},
    keywords = {ai, machine\_learning, phishing},
    location = {Banff, Alberta, Canada},
    pages = {639--648},
    posted-at = {2010-05-06 01:20:12},
    priority = {2},
    publisher = {ACM},
    title = {Cantina: a content-based approach to detecting phishing web sites},
    url = {http://dx.doi.org/10.1145/1242572.1242659},
    year = {2007}
}

@inproceedings{lhuillierEtAl2009:phishing,
    abstract = {In adversarial systems, the performance of a classifier decreases after it is deployed, as the adversary learns to defeat it. Recently, adversarial data mining was introduced as a solution to this, where the classification problem is viewed as a game mechanism between an adversary and an intelligent and adaptive classifier. Over the last years, phishing fraud through malicious email messages has been a serious threat that affects global security and economy, where traditional spam filtering techniques have shown to be ineffective. In this domain, using dynamic games of incomplete information, a game theoretic data mining framework is proposed in order to build an adversary aware classifier for phishing fraud detection. To build the classifier, an online version of the Weighted Margin Support Vector Machines with a game theoretic prior knowledge function is proposed. In this paper, a new content-based feature extraction technique for phishing filtering is described. Experiments show that the proposed classifier is highly competitive compared with previously proposed online classification algorithms in this adversarial environment, and promising results where obtained using traditional machine learning techniques over extracted features.},
    address = {New York, NY, USA},
    author = {L'Huillier, Gaston and Weber, Richard and Figueroa, Nicolas},
    booktitle = {CSI-KDD '09: Proc. of the ACM SIGKDD Workshop on CyberSecurity and Intelligence Informatics},
    citeulike-article-id = {7130910},
    citeulike-linkout-0 = {http://portal.acm.org/citation.cfm?id=1599279},
    citeulike-linkout-1 = {http://dx.doi.org/10.1145/1599272.1599279},
    doi = {10.1145/1599272.1599279},
    isbn = {978-1-60558-669-4},
    keywords = {ai, machine\_learning, phishing},
    location = {Paris, France},
    pages = {33--42},
    posted-at = {2010-05-06 01:18:32},
    priority = {2},
    publisher = {ACM},
    title = {Online phishing classification using adversarial data mining and signaling games},
    url = {http://dx.doi.org/10.1145/1599272.1599279},
    year = {2009}
}

@inproceedings{abuNimehEtAl2007,
    abstract = {There are many applications available for phishing detection. However, unlike predicting spam, there are only few studies that compare machine learning techniques in predicting phishing. The present study compares the predictive accuracy of several machine learning methods including Logistic Regression (LR), Classification and Regression Trees (CART), Bayesian Additive Regression Trees (BART), Support Vector Machines (SVM), Random Forests (RF), and Neural Networks (NNet) for predicting phishing emails. A data set of 2889 phishing and legitimate emails is used in the comparative study. In addition, 43 features are used to train and test the classifiers.},
    address = {New York, NY, USA},
    author = {Nimeh, Saeed A. and Nappa, Dario and Wang, Xinlei and Nair, Suku},
    booktitle = {eCrime '07: Proc. of the anti-phishing working groups 2nd annual eCrime researchers summit},
    citeulike-article-id = {7130909},
    citeulike-linkout-0 = {http://portal.acm.org/citation.cfm?id=1299021},
    citeulike-linkout-1 = {http://dx.doi.org/10.1145/1299015.1299021},
    doi = {10.1145/1299015.1299021},
    isbn = {978-1-59593-939-8},
    keywords = {ai, machine\_learning, phishing},
    location = {Pittsburgh, Pennsylvania},
    pages = {60--69},
    posted-at = {2010-05-06 01:17:03},
    priority = {2},
    publisher = {ACM},
    title = {A comparison of machine learning techniques for phishing detection},
    url = {http://dx.doi.org/10.1145/1299015.1299021},
    year = {2007}
}

@inproceedings{fetteEtAl2007:phishing,
    abstract = {Each month, more attacks are launched with the aim of making web users believe that they are communicating with a trusted entity for the purpose of stealing account information, logon credentials, and identity information in general. This attack method, commonly known as "phishing," is most commonly initiated by sending out emails with links to spoofed websites that harvest information. We present a method for detecting these attacks, which in its most general form is an application of machine learning on a feature set designed to highlight user-targeted deception in electronic communication. This method is applicable, with slight modification, to detection of phishing websites, or the emails used to direct victims to these sites. We evaluate this method on a set of approximately 860 such phishing emails, and 6950 non-phishing emails, and correctly identify over 96\% of the phishing emails while only mis-classifying on the order of 0.1\% of the legitimate emails. We conclude with thoughts on the future for such techniques to specifically identify deception, specifically with respect to the evolutionary nature of the attacks and information available.},
    address = {New York, NY, USA},
    author = {Fette, Ian and Sadeh, Norman and Tomasic, Anthony},
    booktitle = {WWW '07: Proc. of the 16th intl. conf. on World Wide Web},
    citeulike-article-id = {7130906},
    citeulike-linkout-0 = {http://portal.acm.org/citation.cfm?id=1242572.1242660},
    citeulike-linkout-1 = {http://dx.doi.org/10.1145/1242572.1242660},
    doi = {10.1145/1242572.1242660},
    isbn = {978-1-59593-654-7},
    keywords = {ai, email, machine\_learning, phishing},
    location = {Banff, Alberta, Canada},
    pages = {649--656},
    posted-at = {2010-05-06 01:14:58},
    priority = {2},
    publisher = {ACM},
    title = {Learning to detect phishing emails},
    url = {http://dx.doi.org/10.1145/1242572.1242660},
    year = {2007}
}

@article{citeulike:6071377,
    abstract = {The Universal Core (UCore) is a central element of the National Information Sharing Strategy that is supported by multiple  U.S.  Federal  Government  Departments,  by  the intelligence community, and by a number of other national and intl.  institutions. The goal of  the UCore  initiative  is  to foster  information  sharing  by  means  of  an  XML  schema providing  consensus  representations  for  four  groups  of universally  understood  terms  under  the  headings  who,  what, when,  and  where.  We  here  describe  a  project  to  create  an ontology-based  supporting  layer  for UCore,  entitled  'Universal Core Semantic Layer' (UCore SL), and describe how UCore SL can be applied to further UCore's information sharing goals.},
    author = {Smith, Barry and Vizenor, Lowell and Schoening, James},
    booktitle = {Ontology for the Intelligence Community 2009},
    citeulike-article-id = {6071377},
    citeulike-linkout-0 = {http://c4i.gmu.edu/OIC09/papers/OIC2009\_5\_SmithEtAll.pdf},
    day = {20},
    keywords = {semantic\_web},
    location = {Fairfax, VA},
    month = {October},
    organization = {George Mason University},
    posted-at = {2010-05-06 01:12:15},
    priority = {2},
    title = {UCore Universal Core Semantic Layer},
    url = {http://c4i.gmu.edu/OIC09/papers/OIC2009\_5\_SmithEtAll.pdf},
    year = {2009}
}

@inproceedings{citeulike:4648377,
    abstract = {Large-scale knowledge representation (KR) with RDF unveils shortcomings which become obvious when facts have to be further qualified with contextual aspects to represent anything else but simplistic binary predicates. Motivated by requirements in the VIKEF project, in which we are required to store a large amount of information extracted from a heterogeneous set of documents and encoded in RDF triples, we are proposing an architecture for modelling context in RDF knowledge bases. Our approach -- based on well-researched theories of context in KR -- avoids issues that other approaches face, by preserving standard RDF within a context and adding context semantics and relations between contexts around the standard. In this paper we will present our approach, including formal definitions of our extensions and an illustration of further works.},
    author = {Bouquet, Paolo and Serafini, Luciano and Stoermer, Heiko},
    booktitle = {In Proc. of SWAP 2005, the 2nd Italian Semantic Web Workshop},
    citeulike-article-id = {4648377},
    citeulike-linkout-0 = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.60.6235},
    keywords = {rdf},
    pages = {14--16},
    posted-at = {2010-05-06 01:09:00},
    priority = {2},
    title = {Introducing Context into RDF Knowledge Bases},
    url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.60.6235},
    year = {2005}
}

@techreport{citeulike:7130900,
    abstract = {http://eprints.biblio.unitn.it/archive/00001540/},
    author = {Bazzanella, Barbara and Bouquet, Paolo and Stoermer, Heiko},
    citeulike-article-id = {7130900},
    citeulike-linkout-0 = {http://eprints.biblio.unitn.it/archive/00001540/},
    institution = {Ingegneria e Scienza dell'Informazione, University of Trento},
    keywords = {disambiguation, ontologies, rdf, semantic\_web},
    month = {January},
    posted-at = {2010-05-06 01:06:00},
    priority = {2},
    title = {A Cognitive Contribution to Entity Representation and Matching},
    url = {http://eprints.biblio.unitn.it/archive/00001540/},
    year = {2009}
}

@inproceedings{deepakEtAl2004,
    abstract = {It is a common experience while web searching thatone gets to see pages that are not of interest. Partlythese are due to a word or words in the search queryhaving different contexts, the user obviously expectingto find pages related to the context of interest. Thispaper proposes a method for disambiguating contextsin web search results.},
    address = {Washington, DC, USA},
    author = {Deepak, P. and John, Jyothi and Parameswaran, Sandeep},
    booktitle = {ICWS '04: Proc. of the IEEE Intl. Conf. on Web Services},
    citeulike-article-id = {7130897},
    citeulike-linkout-0 = {http://portal.acm.org/citation.cfm?id=1009386.1010216},
    citeulike-linkout-1 = {http://dx.doi.org/10.1109/ICWS.2004.41},
    doi = {10.1109/ICWS.2004.41},
    isbn = {0-7695-2167-3},
    keywords = {disambiguation, rdf},
    pages = {700+},
    posted-at = {2010-05-06 01:01:57},
    priority = {2},
    publisher = {IEEE Computer Society},
    title = {Context Disambiguation in Web Search Results},
    url = {http://dx.doi.org/10.1109/ICWS.2004.41},
    year = {2004}
}

@electronic{smartAndVercauteren2009:encr,
    abstract = {We present a fully homomorphic encryption scheme which has both relatively small key and ciphertext size. Our construction follows that of Gentry by producing a fully homomorphic scheme from a somewhat homomorphic scheme. For the somewhat homomorphic scheme the public and private keys consist of two large integers (one of which is shared by both the public and private key) and the ciphertext consists of one large integer. As such, our scheme has smaller message expansion and key size than Gentrys original scheme. In addition, our proposal allows efficient fully homomorphic encryption over any field of characteristic two.},
    author = {Smart, N. P. and Vercauteren, F.},
    citeulike-article-id = {6737524},
    citeulike-linkout-0 = {http://eprint.iacr.org/2009/571},
    day = {25},
    howpublished = {Cryptology ePrint Archive, Report 2009/571},
    keywords = {encryption, homomorphisms},
    month = {Nov},
    posted-at = {2010-02-26 18:24:51},
    priority = {2},
    title = {Fully Homomorphic Encryption with Relatively Small Key and Ciphertext Sizes},
    url = {http://eprint.iacr.org/2009/571},
    year = {2009}
}

@electronic{dijkEtAl2009:encr,
    abstract = {We construct a simple fully homomorphic encryption scheme, using only elementary modular arithmetic. We use Gentry's technique to construct fully homomorphic scheme from a \&quot;bootstrappable\&quot; somewhat homomorphic scheme. However, instead of using ideal lattices over a polynomial ring, our bootstrappable encryption scheme merely uses addition and multiplication over the integers. The main appeal of our scheme is the conceptual simplicity.
 <P>
 We reduce the security of our scheme to finding an approximate integer gcd -- i.e., given a list of integers that are near-multiples of a hidden integer, output that hidden integer.  We investigate the hardness of this task, building on earlier work of Howgrave-Graham.},
    author = {van Dijk, Marten and Gentry, Craig and Halevi, Shai and Vaikuntanathan, Vinod},
    citeulike-article-id = {6602884},
    citeulike-linkout-0 = {http://eprint.iacr.org/2009/616},
    day = {11},
    howpublished = {Cryptology ePrint Archive, Report 2009/616},
    keywords = {encryption, homomorphisms},
    month = {Dec},
    posted-at = {2010-02-26 18:20:14},
    priority = {2},
    title = {Fully Homomorphic Encryption over the Integers},
    url = {http://eprint.iacr.org/2009/616},
    year = {2009}
}

@inproceedings{creswickNovstrup2010:versionspaces,
    abstract = {Application customization has been extensively researched in the
  field of Programming by Demonstration (PBD), and Version Space
  Algebra has proven itself to be a viable means of quickly learning
  precise action sequences from user demonstrations.  However, this
  technique is not capable of handling user error in domains with
  actions that depend on parameters that accept myriad values.
  Activities such as image, audio and video editing require user
  actions that are difficult for users to precisely replicate in
  different circumstances.  Demonstrations that are off by a single
  pixel or a split-second cause traditional composite Version Spaces
  to collapse.

  We present a method of incorporating error tolerance into Version
  Space algebra.  This approach, termed Error-Tolerant Version Spaces,
  adapts Version Space Algebra to domains where the tactile
  capabilities of the user have a much greater chance of prematurely
  collapsing the hypothesis space that is being learned.  The
  resulting framework is capable of quickly learning in domains where
  perfectly consistent user input can not be expected.  We have
  successfully applied our technique in the domain of image redaction,
  allowing our users to quickly specify redactions that can be reliably
  applied to many images without the entry of explicit parameters.},
    author = {Creswick, Eugene R. and Novstrup, Aaron M.},
    booktitle = {Intl. Conf. on Intelligent User Interfaces (IUI 2010)},
    citeulike-article-id = {6600077},
    day = {7},
    keywords = {error\_tolerance, graphics, hypothesis\_space, learning, machine\_learning, version\_spaces},
    location = {Hong Kong, China},
    month = {February},
    posted-at = {2010-01-28 16:18:00},
    priority = {0},
    title = {Error-Tolerant Version Space Algebra},
    year = {2010}
}

@inproceedings{jonesDiatchki2008:fundeps,
    abstract = {Eight years ago, functional dependencies, a concept from the theory of relational databases, were proposed as a mechanism for avoiding common problems with multiple parameter type classes in Haskell. In this context, functional dependencies give programmers a means to specify the semantics of a type class more precisely, and to obtain more accurate inferred types as a result. As time passed, however, several issues were uncovered---both in the design of a language to support functional dependencies, and in the ways that programmers use them---that led some to search for new, better alternatives.

This paper focusses on two related aspects of design for functional dependencies: (i) the design of language/type system extensions that implement them; and (ii) the design of programs that use them. Our goal is to clarify the issues of what functional dependencies are, how they should be used, and how the problems encountered with initial proposals and implementations can be addressed.},
    author = {Jones, Mark P. and Diatchki, Iavor},
    booktitle = {Proc. of the ACM Haskell Symposium  (Haskell '08)},
    citeulike-article-id = {6417936},
    day = {25},
    keywords = {functional\_dependencies, functional\_programming, haskell},
    location = {Victoria, British Columbia, Canada},
    month = {September},
    posted-at = {2009-12-21 22:27:43},
    priority = {2},
    title = {Language and Program Design for Functional Dependencies},
    year = {2008}
}

@inproceedings{frankEtAl99:kea,
    abstract = {Keyphrases are an important means of document  summarization, clustering, and topic  search. Only a small minority of documents  have author-assigned keyphrases, and manually  assigning keyphrases to existing documents is  very laborious. Therefore it is highly desirable  to automate the keyphrase extraction process.  This paper shows that a simple procedure for  keyphrase extraction based on the naiveBayes  learning scheme performs comparably to the  state of the art. It goes on to explain how  this procedure\&\#039;s performance can be boosted by  automatically tailoring the extraction process  to the particular document collection at hand.  Results on a large collection of technical reports  in computer science show that the quality of  the extracted keyphrases improves signi\#cantly  when domain-speci\#c information is exploited.  1 Introduction  Keyphrases give a high-level description of a document\&\#039;s contents that is intended to make it easy for prospective readers to decide whether or no...},
    author = {Frank, Eibe and Paynter, Gordon W. and Witten, Ian H. and Gutwin, Carl and Nevill-Manning, Craig G.},
    booktitle = {Proc. of the 16 th Intl. Joint Conf. on Arti Intelligence (IJCAI99},
    citeulike-article-id = {3103859},
    citeulike-linkout-0 = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.29.6519},
    keywords = {keyphrase\_extraction, nlp},
    pages = {668--673},
    posted-at = {2009-10-11 23:34:54},
    priority = {2},
    title = {Domain-Specific Keyphrase Extraction},
    url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.29.6519},
    year = {1999}
}

@inproceedings{wittenEtAl99:kea,
    abstract = {Keyphrases provide semantic metadata that summarize and characterize documents. This paper describes Kea, an algorithm for automatically extracting keyphrases from text. Kea identifies candidate keyphrases using lexical methods, calculates feature values for each candidate, and uses a machine -learning algorithm to predict which candidates are good keyphrases. The machine learning scheme first builds a prediction model using training documents with known keyphrases, and then uses the model to find keyphrases in new documents. We use a large test corpus to evaluate Kea\&\#039;s effectiveness in terms of how many author-assigned keyphrases are correctly identified. The system is simple, robust, and publicly available. INTRODUCTION  Keyphrases provide a brief summary of a document\&\#039;s contents. As large document collections such as digital libraries become widespread, the value of such summary information increases. Keywords and keyphrases  1  are particularly useful because they can be interprete...},
    author = {Witten, Ian H. and Paynter, Gordon and Frank, Eibe and Gutwin, Carl and Nevill-Manning, Craig G.},
    booktitle = {Proc. of Digital Libraries 99 (DL\&\#039;99},
    citeulike-article-id = {5922836},
    citeulike-linkout-0 = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.55.3127},
    keywords = {keyphrase\_extraction, nlp},
    pages = {254--255},
    posted-at = {2009-10-11 23:30:40},
    priority = {2},
    title = {KEA: Practical Automatic Keyphrase Extraction},
    url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.55.3127},
    year = {1999}
}

@article{smithEtAl:kidsim,
    address = {New York, NY, USA},
    author = {Smith, David C. and Cypher, Allen and Spohrer, Jim},
    citeulike-article-id = {2734214},
    citeulike-linkout-0 = {http://portal.acm.org/citation.cfm?id=176789.176795},
    citeulike-linkout-1 = {http://dx.doi.org/10.1145/176789.176795},
    doi = {10.1145/176789.176795},
    issn = {0001-0782},
    journal = {Commun. ACM},
    keywords = {end\_user\_programmers},
    month = {July},
    number = {7},
    pages = {54--67},
    posted-at = {2009-10-11 21:41:02},
    priority = {2},
    publisher = {ACM},
    title = {KidSim: programming agents without a programming language},
    url = {http://dx.doi.org/10.1145/176789.176795},
    volume = {37},
    year = {1994}
}

@article{myers93:peridot,
    address = {Cambridge, MA, USA},
    author = {Myers, Brad A.},
    citeulike-article-id = {5841931},
    citeulike-linkout-0 = {http://portal.acm.org/citation.cfm?id=168102},
    isbn = {0-262-03213-9},
    keywords = {pbd, peridot},
    pages = {125--153},
    posted-at = {2009-09-26 05:37:30},
    priority = {2},
    publisher = {MIT Press},
    title = {Peridot: creating user interfaces by demonstration},
    url = {http://portal.acm.org/citation.cfm?id=168102},
    year = {1993}
}

@book{myers88:peridot,
    address = {San Diego, CA, USA},
    author = {Myers, Brad A.},
    citeulike-article-id = {5841929},
    citeulike-linkout-0 = {http://portal.acm.org/citation.cfm?id=43392},
    isbn = {0-12-512305-1},
    keywords = {pbd, peridot},
    posted-at = {2009-09-26 05:36:37},
    priority = {2},
    publisher = {Academic Press Professional, Inc.},
    title = {Creating user interfaces by demonstration},
    url = {http://portal.acm.org/citation.cfm?id=43392},
    year = {1988}
}

@techreport{michail:06,
    address = {TR\# UW-CSE-98-08-06},
    author = {Michail, Amir},
    citeulike-article-id = {5841925},
    institution = {University of Washington School of Computer Science and Engineering},
    keywords = {pbd},
    posted-at = {2009-09-26 05:27:09},
    priority = {2},
    title = {Imitation: An Alternative to Generalization in Programming by Demonstration Systems},
    year = {2006}
}

@inproceedings{lauEtAl2000:vsa,
    address = {San Francisco, CA, USA},
    author = {Lau, Tessa A. and Domingos, Pedro and Weld, Daniel S.},
    booktitle = {ICML '00: Proc. of the Seventeenth Intl. Conf. on Machine Learning},
    citeulike-article-id = {5841878},
    citeulike-linkout-0 = {http://portal.acm.org/citation.cfm?id=657973},
    isbn = {1-55860-707-2},
    keywords = {pbd, version\_space, version\_spaces},
    pages = {527--534},
    posted-at = {2009-09-26 03:10:02},
    priority = {2},
    publisher = {Morgan Kaufmann Publishers Inc.},
    title = {Version Space Algebra and its Application to Programming by Demonstration},
    url = {http://portal.acm.org/citation.cfm?id=657973},
    year = {2000}
}

@book{lieberman2001:wish,
    author = {Lieberman, Henry},
    citeulike-article-id = {5841748},
    citeulike-linkout-0 = {http://www.amazon.ca/exec/obidos/redirect?tag=citeulike09-20\&amp;path=ASIN/1558606882},
    citeulike-linkout-1 = {http://www.amazon.de/exec/obidos/redirect?tag=citeulike01-21\&amp;path=ASIN/1558606882},
    citeulike-linkout-2 = {http://www.amazon.fr/exec/obidos/redirect?tag=citeulike06-21\&amp;path=ASIN/1558606882},
    citeulike-linkout-3 = {http://www.amazon.jp/exec/obidos/ASIN/1558606882},
    citeulike-linkout-4 = {http://www.amazon.co.uk/exec/obidos/ASIN/1558606882/citeulike00-21},
    citeulike-linkout-5 = {http://www.amazon.com/exec/obidos/redirect?tag=citeulike07-20\&path=ASIN/1558606882},
    citeulike-linkout-6 = {http://www.worldcat.org/isbn/1558606882},
    citeulike-linkout-7 = {http://books.google.com/books?vid=ISBN1558606882},
    citeulike-linkout-8 = {http://www.amazon.com/gp/search?keywords=1558606882\&index=books\&linkCode=qs},
    citeulike-linkout-9 = {http://www.librarything.com/isbn/1558606882},
    day = {26},
    edition = {1st},
    howpublished = {Paperback},
    isbn = {1558606882},
    keywords = {book, pbd},
    month = {February},
    posted-at = {2009-09-26 01:33:09},
    priority = {2},
    publisher = {Morgan Kaufmann},
    title = {Your Wish is My Command: Programming By Example (Interactive Technologies)},
    url = {http://www.amazon.com/exec/obidos/redirect?tag=citeulike07-20\&path=ASIN/1558606882},
    year = {2001}
}

@inproceedings{millerMyers2001:outliers,
    abstract = {When users handle large amounts of data, errors are hard to notice.  Outlier finding  is a new way to reduce errors by directing the user's attention to inconsistent data which may indicate errors. We have implemented an outlier finder for text, which can detect both unusual matches and unusual mismatches to a text pattern. When integrated into the user interface of a PBD text editor and tested in a user study, outlier finding substantially reduced errors.},
    address = {New York, NY, USA},
    author = {Miller, Robert C. and Myers, Brad A.},
    booktitle = {UIST '01: Proc. of the 14th annual ACM symposium on User interface software and technology},
    citeulike-article-id = {3360250},
    citeulike-linkout-0 = {http://portal.acm.org/citation.cfm?id=502348.502361},
    citeulike-linkout-1 = {http://dx.doi.org/10.1145/502348.502361},
    doi = {10.1145/502348.502361},
    isbn = {1-58113-438-X},
    keywords = {debugging, end\_user\_programmers},
    location = {Orlando, Florida},
    pages = {81--90},
    posted-at = {2009-09-15 18:14:29},
    priority = {2},
    publisher = {ACM},
    title = {Outlier finding: focusing user attention on possible errors},
    url = {http://dx.doi.org/10.1145/502348.502361},
    year = {2001}
}

@inproceedings{carr2003:eup,
    author = {Carr, D. A.},
    booktitle = {CHI 2003 Workshop on Perspectives in End-User Development},
    citeulike-article-id = {5788852},
    keywords = {end\_user\_programmers, tools},
    month = {April},
    pages = {16--18},
    posted-at = {2009-09-15 18:13:17},
    priority = {2},
    title = {End--user programmers need improved development support},
    year = {2003}
}

@article{davis1996:tools,
    address = {Duluth, MN, USA},
    author = {Davis, J. Steve},
    citeulike-article-id = {5788849},
    citeulike-linkout-0 = {http://portal.acm.org/citation.cfm?id=244921},
    citeulike-linkout-1 = {http://dx.doi.org/10.1006/ijhc.1996.0061},
    doi = {10.1006/ijhc.1996.0061},
    issn = {1071-5819},
    journal = {Int. J. Hum.-Comput. Stud.},
    keywords = {spreadsheet, tools},
    number = {4},
    pages = {429--442},
    posted-at = {2009-09-15 18:08:22},
    priority = {2},
    publisher = {Academic Press, Inc.},
    title = {Tools for spreadsheet auditing},
    url = {http://dx.doi.org/10.1006/ijhc.1996.0061},
    volume = {45},
    year = {1996}
}

@electronic{sajaniemi2000:spreadsheets,
    abstract = {:  Computations in spreadsheets are hard to grasp and consequently many errors remain unnoticed. The problem with the hidden errors lies in the invisibility of the structure of calculations. As a result, auditing and visualization tools are required to make spreadsheets easier to comprehend and errors easier to be detected. This paper presents a theoretical model of spreadsheets, and describes various spreadsheet auditing mechanisms employing the model. Moreover, two new visualization mechanisms are introduced. The model reflects not only current spreadsheet systems but also the way people actually use spreadsheets. Theoretically, it is impossible to check the correctness of a spreadsheet without a formal definition of its computations, but our hope is to find visualizations that point out parts of spreadsheets that contain anomalies, i.e., potential locations of errors. The model helps us to understand how such anomalies can be defined.  Keywords: spreadsheets, auditing, visualization...},
    author = {Sajaniemi, Jorma},
    citeulike-article-id = {5788847},
    citeulike-linkout-0 = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.35.2163},
    keywords = {end\_user\_programmers, spreadsheet, spreadsheet\_developers, visualization},
    posted-at = {2009-09-15 18:07:03},
    priority = {2},
    title = {Modeling Spreadsheet Audit: A Rigorous Approach to Automatic Visualization},
    url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.35.2163},
    year = {1998}
}

@inproceedings{igarashiEtAl1998:viz,
    abstract = {Spreadsheets augment a visible tabular layout with invisible formulas. Direct manipulations of the tabular layout may or may not result in the desired changes to the formulas. The user is forced to explore the individual cells to find, verify, and modify the formulas, which causes heavy cognitive overhead. We present a set of techniques that make these formulas and their resulting dataflow structure easily accessible while maintaining the natural appearance of the spreadsheet. Transient local views visualize dataflow structures associated with individual cells, while static global views and animated global explanations visually present the entire dataflow structure at once. Semantic navigation enables the user to navigate through the dataflow structure interactively, and visual editing techniques make it possible to construct formulas using graphical editing techniques. Central to these techniques is the use of animation and lightweight interaction for rapid and non-intrusive visualization. Our prototype implementation suggests that these techniques can greatly improve the expressive power of current spreadsheets as well as other applications that have rich underlying structures.},
    address = {Washington, DC, USA},
    author = {Igarashi, Takeo and Mackinlay, Jock D. and Chang, Bay W. and Zellweger, Polle T.},
    booktitle = {VL '98: Proc. of the IEEE Symposium on Visual Languages},
    citeulike-article-id = {5788842},
    citeulike-linkout-0 = {http://portal.acm.org/citation.cfm?id=834481},
    isbn = {0-8186-8712-6},
    keywords = {spreadsheet, visualization},
    pages = {118+},
    posted-at = {2009-09-15 18:05:57},
    priority = {2},
    publisher = {IEEE Computer Society},
    title = {Fluid Visualization of Spreadsheet Structures},
    url = {http://portal.acm.org/citation.cfm?id=834481},
    year = {1998}
}

@book{reiss1998:softvis,
    address = {Cambridge, MA},
    author = {Reiss, S.},
    booktitle = {Software Visualization: Programming as a Multimedia Experience},
    citeulike-article-id = {5788824},
    editor = {Stasko, J. and Domingue, J. and Brown, M. and Price, B.},
    keywords = {programming\_environments, visual\_programming\_languages, visualization},
    pages = {259--276},
    posted-at = {2009-09-15 18:00:50},
    priority = {2},
    publisher = {MIT Press},
    title = {Visualization for software engineering---programming environments},
    year = {1998}
}

@article{panko1998:errors,
    abstract = {http://panko.shidler.hawaii.edu/SSR/Mypapers/whatknow.htm

Although spreadsheet programs are used for small "scratchpad" applications, they are also used to develop many large applications. In recent years, we have learned a good deal about the errors that people make when they develop spreadsheets. In general, errors seem to occur in a few percent of all cells, meaning that for large spreadsheets, the issue is how many errors there are, not whether an error exists. These error rates, although troubling, are in line with those in programming and other human cognitive domains. In programming, we have learned to follow strict development disciplines to eliminate most errors. Surveys of spreadsheet developers indicate that spreadsheet creation, in contrast, is informal, and few organizations have comprehensive policies for spreadsheet development. Although prescriptive articles have focused on such disciplines as modularization and having assumptions sections, these may be far less important than other innovations, especially cell-by-cell code inspection after the development phase.},
    author = {Panko, Raymond R.},
    citeulike-article-id = {5781754},
    comment = {http://panko.shidler.hawaii.edu/SSR/Mypapers/whatknow.htm},
    journal = {Journal of End User Computing},
    keywords = {end\_user\_programmers, error\_rates, spreadsheet\_creation, spreadsheet\_errors, spreadsheet\_programs, testing},
    number = {2},
    posted-at = {2009-09-14 19:47:32},
    priority = {2},
    title = {What We Know About Spreadsheet Errors},
    volume = {10},
    year = {1998}
}

@article{panko2008:overconfidence,
    abstract = {Despite strong evidence of widespread errors, spreadsheet developers rarely subject their spreadsheets to post-development testing to reduce errors. This may be because spreadsheet developers are overconfident in the accuracy of their spreadsheets. This conjecture is plausible because overconfidence is present in a wide variety of human cognitive domains, even among experts. This paper describes two experiments in overconfidence in spreadsheet development. The first is a pilot study to determine the existence of overconfidence. The second tests a manipulation to reduce overconfidence and errors. The manipulation is modestly successful, indicating that overconfidence reduction is a promising avenue to pursue.},
    author = {Panko, Raymond R.},
    citeulike-article-id = {5781746},
    citeulike-linkout-0 = {http://cdsweb.cern.ch/record/1097980},
    day = {8},
    journal = {Proc. of the European Spreadsheet Risks Interest Group},
    keywords = {end\_user\_programmers, spreadsheets, testing},
    month = {April},
    posted-at = {2009-09-14 19:40:10},
    priority = {2},
    title = {Reducing Overconfidence in Spreadsheet Development},
    url = {http://cdsweb.cern.ch/record/1097980},
    year = {2008}
}

@article{panko2007:overconfidence,
    abstract = {This paper describes two experiments that examined overconfidence in spreadsheet
development. Overconfidence has been seen widely in spreadsheet development and
could account for the rarity of testing by end-user spreadsheet developers. The first
experiment studied a new way of measuring overconfidence. It demonstrated that
overconfidence really is strong among spreadsheet developers. The second experi-
ment attempted to reduce overconfidence by telling subjects in the treatment group
the percentage of students who made errors on the task in the past. This warning did
reduce overconfidence, and it reduced errors somewhat, although not enough to make
spreadsheet development safe.},
    author = {Panko, Raymond R.},
    citeulike-article-id = {5781730},
    journal = {Journal of Organizational and End User Computing},
    keywords = {end\_user\_programmers, overconfidence, spreadsheet\_developers, spreadsheet\_development, testing},
    month = {January},
    number = {1},
    pages = {1--23},
    posted-at = {2009-09-14 19:23:12},
    priority = {2},
    title = {Two Experiments in Reducing Overconfidence in Spreadsheet Development},
    volume = {19},
    year = {2007}
}

@inproceedings{Mikolajczyk03a,
    author = {Mikolajczyk, K. and Zisserman, A. and Schmid, C.},
    booktitle = {Proc. of the British Machine Vision Conf.},
    citeulike-article-id = {5253766},
    keywords = {computer\_vision, image-processing, object\_detection},
    pages = {779--788},
    posted-at = {2009-07-24 21:25:31},
    priority = {2},
    title = {Shape recognition with edge-based features},
    volume = {2},
    year = {2003}
}

@article{Sivic08b,
    author = {Sivic, J. and Zisserman, A.},
    citeulike-article-id = {5253716},
    journal = {Proc. of the {IEEE}},
    keywords = {computer\_vision, image-processing},
    number = {4},
    pages = {548--566},
    posted-at = {2009-07-24 21:19:01},
    priority = {2},
    title = {Efficient Visual Search for Objects in Videos},
    volume = {96},
    year = {2008}
}

@book{shah1997:comuterVision,
    address = {Orlando, FL},
    author = {Shah, Mubarak},
    citeulike-article-id = {5253651},
    keywords = {book, computer\_vision},
    month = {December},
    organization = {University of Central Florida},
    posted-at = {2009-07-24 21:11:21},
    priority = {2},
    publisher = {University of Central Florida},
    title = {Fundamentals of Computer Vision},
    year = {1997}
}

@article{Smith97,
    author = {Smith, S. M. and Brady, J. M.},
    citeulike-article-id = {5253162},
    journal = {Int. Journal of Computer Vision},
    keywords = {image-processing, text-detection},
    month = {May},
    number = {1},
    pages = {45--78},
    posted-at = {2009-07-24 20:02:59},
    priority = {2},
    title = {{SUSAN} - A New Approach to Low Level Image Processing},
    volume = {23},
    year = {1997}
}

@inproceedings{bb96446,
    author = {Park, T. and Kim, D. and Chung, K.},
    citeulike-article-id = {5252841},
    keywords = {text-detection},
    pages = {xx--yy},
    posted-at = {2009-07-24 19:20:04},
    priority = {2},
    title = {Orientation and Scale Invariant Text Region Extraction in WWW Images},
    year = {1998}
}

@inproceedings{Gentry2009:homomorphicEnc,
    abstract = {We propose a fully homomorphic encryption scheme -- i.e., a scheme that allows one to evaluate circuits over encrypted data without being able to decrypt. Our solution comes in three steps. First, we provide a general result -- that, to construct an encryption scheme that permits evaluation of  arbitrary circuits , it suffices to construct an encryption scheme that can evaluate (slightly augmented versions of) its  own decryption circuit ; we call a scheme that can evaluate its (augmented) decryption circuit  bootstrappable .},
    address = {New York, NY, USA},
    author = {Gentry, Craig},
    booktitle = {STOC '09: Proc. of the 41st annual ACM symposium on Theory of computing},
    citeulike-article-id = {4817890},
    citeulike-linkout-0 = {http://portal.acm.org/citation.cfm?id=1536414.1536440},
    citeulike-linkout-1 = {http://dx.doi.org/10.1145/1536414.1536440},
    doi = {10.1145/1536414.1536440},
    isbn = {978-1-60558-506-2},
    keywords = {encryption, homomorphisms, security, stoc09},
    location = {Bethesda, MD, USA},
    pages = {169--178},
    posted-at = {2009-07-15 21:36:15},
    priority = {2},
    publisher = {ACM},
    title = {Fully homomorphic encryption using ideal lattices},
    url = {http://dx.doi.org/10.1145/1536414.1536440},
    year = {2009}
}

@inproceedings{lin1993:blmAxioms,
    abstract = {Ideally secure systems must, be provable secure, so they are all defined by mathematical models. Most of current systems are based on the Bell and LaPadula Model (BLM), however, many usages are not logically sound. In this paper, a new paradigm is proposed to reinterpret the BLM. BLM is treated as axioms to define the multilevel security, in the same spirit as Hilbert axioms to the Euc1idean geometry. Absolutely no violations are tolerated. So many usual trusted subjects are no longer admissible in this new BLM. Three layer architecture is proposed to accommodate such requirements.},
    address = {New York, NY, USA},
    author = {Lin, T. Y.},
    booktitle = {NSPW '92-93: Proc. on the 1992-1993 workshop on New security paradigms},
    citeulike-article-id = {5094167},
    citeulike-linkout-0 = {http://portal.acm.org/citation.cfm?id=283789},
    citeulike-linkout-1 = {http://dx.doi.org/10.1145/283751.283789},
    doi = {10.1145/283751.283789},
    isbn = {0-8186-5430-9},
    keywords = {aggregation, security},
    location = {Little Compton, Rhode Island, United States},
    pages = {82--93},
    posted-at = {2009-07-08 21:32:14},
    priority = {2},
    publisher = {ACM},
    title = {Bell and LaPadula axioms: a ``new'' paradigm for an ``old'' model},
    url = {http://dx.doi.org/10.1145/283751.283789},
    year = {1993}
}

@techreport{ncsc1996:Inference_aggregation,
    citeulike-article-id = {5094162},
    citeulike-linkout-0 = {http://citeseer.ist.psu.edu/did/68177},
    keywords = {aggregation, security},
    month = {May},
    organization = {National Computer Security Center},
    posted-at = {2009-07-08 21:30:09},
    priority = {2},
    title = {Inference and Aggregation Issues In Secure Database Management Systems},
    url = {http://citeseer.ist.psu.edu/did/68177},
    year = {1996}
}

@inproceedings{buiEtAl2008:pexa,
    abstract = {We demonstrate an intelligent personal assistant agent that has been developed to aid a busy knowledge worker in managing time commitments and performing tasks. The  PExA  agent draws on a diverse set of AI technologies that are linked within the SPARK BDI agent framework. We focus on our agent's ability to provide assistance within the context of current user activities, based on its recognition of user workflows and their progress, and on its context-sensitive proactive suggestions. We have instrumented a common suite of desktop applications so that, endowed with a sophisticated workflow tracker, PExA has the ability to pervasively monitor the user's desktop activities. PExA follows and responds to the user's progress on shared tasks, and is highly user-centric in its support for user needs and its adaptivity to user working style and preferences.},
    address = {Richland, SC},
    author = {Bui, Hung H. and Cesari, Federico and Elenius, Daniel and Morley, David N. and Natarajan, Sriraam and Saadati, Shahin and Yeh, Eric and Smith, Neil Y.},
    booktitle = {AAMAS '08: Proc. of the 7th intl. joint conf. on Autonomous agents and multiagent systems},
    citeulike-article-id = {4896480},
    citeulike-linkout-0 = {http://portal.acm.org/citation.cfm?id=1402744.1402762},
    keywords = {context\_aware, context\_switching, intelligent\_interfaces},
    location = {Estoril, Portugal},
    pages = {1679--1680},
    posted-at = {2009-06-18 21:13:56},
    priority = {5},
    publisher = {Intl. Foundation for Autonomous Agents and Multiagent Systems},
    title = {A context-aware personal desktop assistant},
    url = {http://portal.acm.org/citation.cfm?id=1402744.1402762},
    year = {2008}
}

@inproceedings{dahlbackEtAl1993:wizard,
    abstract = {We discuss current approaches to the development of natural language dialogue systems, and claim that they do not sufficiently consider the unique qualities of man-machine interaction as distinct from general human discourse. We conclude that empirical studies of this unique communication situation is required for the development of user-friendly interactive systems. One way of achieving this is through the use of so-called Wizard of Oz studies. We describe our work in this area. The focus is...},
    address = {Orlando, FL},
    author = {Dahlback, N. and Jonsson, A. and Ahrenberg, L.},
    booktitle = {Workshop on Intelligent User Interfaces},
    citeulike-article-id = {542214},
    citeulike-linkout-0 = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.56.3398},
    keywords = {interface\_design, prototyping, wizard-of-oz},
    posted-at = {2009-06-15 05:36:36},
    priority = {2},
    title = {Wizard of Oz-studies -- why and how},
    url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.56.3398},
    year = {1993}
}

@inproceedings{kerstenMurphy2005:mylyn,
    address = {New York, NY, USA},
    author = {Kersten, Mik and Murphy, Gail C.},
    booktitle = {AOSD '05: Proc. of the 4th intl. conf. on Aspect-oriented software development},
    citeulike-article-id = {678834},
    citeulike-linkout-0 = {http://portal.acm.org/citation.cfm?id=1052912},
    citeulike-linkout-1 = {http://dx.doi.org/10.1145/1052898.1052912},
    doi = {10.1145/1052898.1052912},
    isbn = {1595930426},
    keywords = {context, context\_aware, context\_switching, eclipse, pbd},
    pages = {159--168},
    posted-at = {2009-06-15 00:30:30},
    priority = {2},
    publisher = {ACM Press},
    title = {Mylar: a degree-of-interest model for IDEs},
    url = {http://dx.doi.org/10.1145/1052898.1052912},
    year = {2005}
}

@inproceedings{weldGajos2004:supple,
    abstract = {In order to give people ubiquitous access to software applications, device controllers, and Internet services, it will be necessary to automatically adapt user interfaces to the computational devices at hand (e.g., cell phones, PDAs, touch panels, etc.). While previous researchers have proposed solutions to this problem, each has limitations. This paper proposes a novel solution based on treating interface adaptation as an optimization problem. When asked to render an interface on a specific device, our Supple system searches for the rendition that meets the device's constraints and minimizes the estimated effort for the user's expected interface actions. We make several contributions: 1) precisely defining the interface rendition problem, 2) demonstrating how user traces can be used to customize interface rendering to particular user's usage pattern, 3) presenting an efficient interface rendering algorithm, 4) performing experiments that demonstrate the utility of our approach.},
    address = {New York, NY, USA},
    author = {Gajos, Krzysztof and Weld, Daniel S.},
    booktitle = {IUI '04: Proc. of the 9th intl. conf. on Intelligent user interfaces},
    citeulike-article-id = {430254},
    citeulike-linkout-0 = {http://www.cs.washington.edu/homes/kgajos/papers/supple-iui04.html},
    citeulike-linkout-1 = {http://portal.acm.org/citation.cfm?id=964461},
    citeulike-linkout-2 = {http://dx.doi.org/10.1145/964442.964461},
    doi = {10.1145/964442.964461},
    isbn = {1-58113-815-6},
    keywords = {automation, interface\_design, interfaces},
    location = {Funchal, Madeira, Portugal},
    pages = {93--100},
    posted-at = {2009-06-15 00:20:30},
    priority = {2},
    publisher = {ACM},
    title = {SUPPLE: automatically generating user interfaces},
    url = {http://www.cs.washington.edu/homes/kgajos/papers/supple-iui04.html},
    year = {2004}
}

@inproceedings{CzerwinskiEtAl2004:interruption,
    author = {Czerwinski, Mary and Horvitz, Eric and Wilhite, Susan},
    booktitle = {Proc. of ACM Human Factors in Computing Systems CHI 2004},
    citeulike-article-id = {4806721},
    citeulike-linkout-0 = {http://research.microsoft.com/users/marycz/chi2004diarystudyfinal.pdf},
    keywords = {interruption, task\_switching},
    pages = {175--182},
    posted-at = {2009-06-15 00:14:13},
    priority = {2},
    title = {A Diary Study of Task Switching and Interruptions},
    url = {http://research.microsoft.com/users/marycz/chi2004diarystudyfinal.pdf},
    year = {2004}
}

@inproceedings{horvitzApacible2003:interruption,
    address = {New York, NY, USA},
    author = {Horvitz, Eric and Apacible, Johnson},
    booktitle = {ICMI '03: Proc. of the 5th intl. conf. on Multimodal interfaces},
    citeulike-article-id = {960068},
    citeulike-linkout-0 = {http://portal.acm.org/citation.cfm?id=958440},
    citeulike-linkout-1 = {http://dx.doi.org/10.1145/958432.958440},
    doi = {10.1145/958432.958440},
    isbn = {1581136218},
    keywords = {automation, interfaces, interruption, task\_switching},
    pages = {20--27},
    posted-at = {2009-06-15 00:13:30},
    priority = {2},
    publisher = {ACM Press},
    title = {Learning and reasoning about interruption},
    url = {http://dx.doi.org/10.1145/958432.958440},
    year = {2003}
}

@article{horvitzEtAl1998:lumiere,
    abstract = {The Lumi`ere Project centers on harnessing probability and utility to provide assistance to computer software users. We review work on Bayesian user models that can be employed to infer a user's needs by considering a user's background, actions, and queries. Several problems were tackled in Lumi`ere research, including (1) the construction of Bayesian models for reasoning about the time-varying goals of computer users from their observed actions and queries, (2) gaining access to a stream of events from software applications, (3) developing a language for transforming system events into observational variables represented in Bayesian user models, (4) developing persistent profiles to capture changes in a user's expertise, and (5) the development of an overall architecture for an intelligent user interface. Lumi`ere prototypes served as the basis for the Office Assistant in the Microsoft Office '97 suite of productivity applications. 1 Introduction Uncertainty is...},
    author = {Horvitz, Eric and Breese, Jack and Heckerman, David and Hovel, David and Rommelse, Koos},
    citeulike-article-id = {4330797},
    keywords = {automation, interface\_design, interfaces, pbd},
    posted-at = {2009-06-15 00:09:18},
    priority = {0},
    title = {The Lumiere Project: Bayesian User Modeling for Inferring the Goals and Needs of Software Users},
    year = {1998}
}

@book{appleGuide,
    author = {Apple},
    citeulike-article-id = {4849166},
    keywords = {book},
    posted-at = {2009-06-15 00:05:57},
    priority = {2},
    title = {Apple Guide Complete: Designing and Developing Onscreen Assistance.},
    year = {1996}
}

@article{furnas1986:fisheye,
    address = {New York, NY, USA},
    author = {Furnas, G. W.},
    citeulike-article-id = {997621},
    citeulike-linkout-0 = {http://portal.acm.org/citation.cfm?id=22339.22342},
    citeulike-linkout-1 = {http://dx.doi.org/10.1145/22339.22342},
    doi = {10.1145/22339.22342},
    issn = {0736-6906},
    journal = {SIGCHI Bull.},
    keywords = {context, fisheye, interface\_design, interfaces},
    month = {April},
    number = {4},
    pages = {16--23},
    posted-at = {2009-06-12 04:57:44},
    priority = {2},
    publisher = {ACM Press},
    title = {Generalized fisheye views},
    url = {http://dx.doi.org/10.1145/22339.22342},
    volume = {17},
    year = {1986}
}

@book{csikszentmihalyi1990:flow,
    abstract = {{You have heard about how a musician loses herself in her music, how a painter becomes one with the process of painting. In work, sport, conversation or hobby, you have experienced, yourself, the suspension of time, the freedom of complete absorption in activity. This is "flow," an experience that is at once demanding and rewarding--an experience that Mihaly Csikszentmihalyi demonstrates is one of the most enjoyable and valuable experiences a person can have. The exhaustive case studies, controlled experiments and innumerable references to historical figures, philosophers and scientists through the ages prove Csikszentmihalyi's point that flow is a singularly productive and desirable state. But the implications for its application to society are what make the book revolutionary.} {The bestselling introduction to "flow"--a groundbreaking psychological theory that shows readers how to improve the quality of life. "The way to happiness lies not in mindless hedonism, but in mindful change<I>."--New York Times Book Review</I>}},
    author = {Csikszentmihalyi, Mihaly},
    citeulike-article-id = {410780},
    citeulike-linkout-0 = {http://www.amazon.ca/exec/obidos/redirect?tag=citeulike09-20\&amp;path=ASIN/0060920432},
    citeulike-linkout-1 = {http://www.amazon.de/exec/obidos/redirect?tag=citeulike01-21\&amp;path=ASIN/0060920432},
    citeulike-linkout-2 = {http://www.amazon.fr/exec/obidos/redirect?tag=citeulike06-21\&amp;path=ASIN/0060920432},
    citeulike-linkout-3 = {http://www.amazon.jp/exec/obidos/ASIN/0060920432},
    citeulike-linkout-4 = {http://www.amazon.co.uk/exec/obidos/ASIN/0060920432/citeulike00-21},
    citeulike-linkout-5 = {http://www.amazon.com/exec/obidos/redirect?tag=citeulike07-20\&path=ASIN/0060920432},
    citeulike-linkout-6 = {http://www.worldcat.org/isbn/0060920432},
    citeulike-linkout-7 = {http://books.google.com/books?vid=ISBN0060920432},
    citeulike-linkout-8 = {http://www.amazon.com/gp/search?keywords=0060920432\&index=books\&linkCode=qs},
    citeulike-linkout-9 = {http://www.librarything.com/isbn/0060920432},
    day = {13},
    howpublished = {Paperback},
    isbn = {0060920432},
    keywords = {book, flow},
    month = {March},
    posted-at = {2009-06-11 20:19:57},
    priority = {2},
    publisher = {Harper Perennial},
    title = {Flow: The Psychology of Optimal Experience},
    url = {http://www.amazon.com/exec/obidos/redirect?tag=citeulike07-20\&path=ASIN/0060920432},
    year = {1991}
}

@book{Csikszentmihalyi1975:beyondBoredom,
    abstract = {Now in a special 25th anniversary edition and filled with brilliant wisdom and
insights, \_Beyond Boredom and Anxiety\_ offers a timeless introduction to the
concept of flow and the scientific basis behind it-all through the work of one
of the field's great scientists, Mihaly Csikzentmihalyi. Through real-life
examples, discover how enjoyable activities provide a common experience-a
satisfying, often exhilarating, feeling of creative accomplishment and
heightened functioning-and under what conditions 'serious' work can also
provide this intrinsic enjoyment.},
    author = {Csikszentmihalyi, Mihaly},
    citeulike-article-id = {4815771},
    citeulike-linkout-0 = {http://www.amazon.ca/exec/obidos/redirect?tag=citeulike09-20\&amp;path=ASIN/0787951404},
    citeulike-linkout-1 = {http://www.amazon.de/exec/obidos/redirect?tag=citeulike01-21\&amp;path=ASIN/0787951404},
    citeulike-linkout-2 = {http://www.amazon.fr/exec/obidos/redirect?tag=citeulike06-21\&amp;path=ASIN/0787951404},
    citeulike-linkout-3 = {http://www.amazon.jp/exec/obidos/ASIN/0787951404},
    citeulike-linkout-4 = {http://www.amazon.co.uk/exec/obidos/ASIN/0787951404/citeulike00-21},
    citeulike-linkout-5 = {http://www.amazon.com/exec/obidos/redirect?tag=citeulike07-20\&path=ASIN/0787951404},
    citeulike-linkout-6 = {http://www.worldcat.org/isbn/0787951404},
    citeulike-linkout-7 = {http://books.google.com/books?vid=ISBN0787951404},
    citeulike-linkout-8 = {http://www.amazon.com/gp/search?keywords=0787951404\&index=books\&linkCode=qs},
    citeulike-linkout-9 = {http://www.librarything.com/isbn/0787951404},
    day = {15},
    edition = {25th Anniversary},
    howpublished = {Hardcover},
    isbn = {0787951404},
    keywords = {book, flow, performance},
    month = {April},
    posted-at = {2009-06-11 19:54:13},
    priority = {2},
    publisher = {Jossey-Bass},
    title = {Beyond Boredom and Anxiety: Experiencing Flow in Work and Play},
    url = {http://www.amazon.com/exec/obidos/redirect?tag=citeulike07-20\&path=ASIN/0787951404},
    year = {2000}
}

@inproceedings{dragunovEtAl2005:taskTracer,
    abstract = {This paper reports on TaskTracer --- a software system being designed to help highly multitasking knowledge workers rapidly locate, discover, and reuse past processes they used to successfully complete tasks. The system monitors users' interaction with a computer, collects detailed records of users' activities and resources accessed, associates (automatically or with users' assistance) each interaction event with a particular task, enables users to access records of past activities and quickly restore task contexts. We present a novel Publisher-Subscriber architecture for collecting and processing users' activity data, describe several different user interfaces tried with TaskTracer, and discuss the possibility of applying machine learning techniques to recognize/predict users' tasks.},
    address = {New York, NY, USA},
    author = {Dragunov, Anton N. and Dietterich, Thomas G. and Johnsrude, Kevin and McLaughlin, Matthew and Li, Lida and Herlocker, Jonathan L.},
    booktitle = {IUI '05: Proc. of the 10th intl. conf. on Intelligent user interfaces},
    citeulike-article-id = {233531},
    citeulike-linkout-0 = {http://portal.acm.org/citation.cfm?id=1040855},
    citeulike-linkout-1 = {http://dx.doi.org/10.1145/1040830.1040855},
    doi = {10.1145/1040830.1040855},
    isbn = {1-58113-894-6},
    keywords = {aaai05, context\_aware, context\_switching},
    location = {San Diego, California, USA},
    pages = {75--82},
    posted-at = {2009-06-11 06:45:01},
    priority = {2},
    publisher = {ACM},
    title = {TaskTracer: a desktop environment to support multi-tasking knowledge workers},
    url = {http://dx.doi.org/10.1145/1040830.1040855},
    year = {2005}
}

@inproceedings{shenEtAl2006:taskTracer,
    abstract = {The TaskTracer system seeks to help multi-tasking users manage the resources that they create and access while carrying out their work activities. It does this by associating with each user-defined activity the set of files, folders, email messages, contacts, and web pages that the user accesses when performing that activity. The initial TaskTracer system relies on the user to notify the system each time the user changes activities. However, this is burdensome, and users often forget to tell TaskTracer what activity they are working on. This paper introduces TaskPredictor, a machine learning system that attempts to predict the user's current activity. TaskPredictor has two components: one for general desktop activity and another specifically for email. TaskPredictor achieves high prediction precision by combining three techniques: (a) feature selection via mutual information, (b) classification based on a confidence threshold, and (c) a hybrid design in which a Naive Bayes classifier estimates the classification confidence but where the actual classification decision is made by a support vector machine. This paper provides experimental results on data collected from TaskTracer users.},
    address = {New York, NY, USA},
    author = {Shen, Jianqiang and Li, Lida and Dietterich, Thomas G. and Herlocker, Jonathan L.},
    booktitle = {IUI '06: Proc. of the 11th intl. conf. on Intelligent user interfaces},
    citeulike-article-id = {780693},
    citeulike-linkout-0 = {http://portal.acm.org/citation.cfm?id=1111473},
    citeulike-linkout-1 = {http://dx.doi.org/10.1145/1111449.1111473},
    doi = {10.1145/1111449.1111473},
    isbn = {1-59593-287-9},
    keywords = {context\_aware, context\_switching, iui06},
    location = {Sydney, Australia},
    pages = {86--92},
    posted-at = {2009-06-11 06:37:09},
    priority = {2},
    publisher = {ACM},
    title = {A hybrid learning system for recognizing user tasks from desktop activities and email messages},
    url = {http://dx.doi.org/10.1145/1111449.1111473},
    year = {2006}
}

@inproceedings{baoEtAl2006:taskTracer,
    address = {New York, NY, USA},
    author = {Bao, Xinlong and Herlocker, Jonathan L. and Dietterich, Thomas G.},
    booktitle = {IUI '06: Proc. of the 11th intl. conf. on Intelligent user interfaces},
    citeulike-article-id = {852563},
    citeulike-linkout-0 = {http://portal.acm.org/citation.cfm?id=1111490},
    citeulike-linkout-1 = {http://dx.doi.org/10.1145/1111449.1111490},
    doi = {10.1145/1111449.1111490},
    isbn = {1595932879},
    keywords = {context\_aware, context\_switching, iui06},
    pages = {178--185},
    posted-at = {2009-06-11 06:35:58},
    priority = {2},
    publisher = {ACM Press},
    title = {Fewer clicks and less frustration: reducing the cost of reaching the right folder},
    url = {http://dx.doi.org/10.1145/1111449.1111490},
    year = {2006}
}

@inproceedings{letkemanEtAl2006:taskTracer,
    author = {Lettkeman, Twinkle A. and Stumpf, Simone and Irvine, Jed and Herlocker, Jon},
    booktitle = {National Conf. on Artificial Intelligence (AAAI-06)},
    citeulike-article-id = {4808139},
    day = {16},
    keywords = {aaai06, context\_aware, context\_switching},
    location = {Boston, MA},
    month = {July},
    posted-at = {2009-06-11 06:34:26},
    priority = {2},
    title = {Predicting Task-Specific Webpages for Revisiting},
    year = {2006}
}

@electronic{wired1996:flow,
    abstract = {According to Mihaly Csikszentmihalyi, great Web sites are not about navigating content, but staging experience. A compelling Web site transforms a random walk into an exhilarating chase. The key, says psychologist Mihaly Csikszentmihalyi, is a finely tuned sense of rhythm, involvement, and anticipation known as "flow." Csikszentmihalyi (pronounced "CHICK-sent-me-high-ee"), a professor at the University of Chicago, has spent more than 25 years researching flow, a state of "intense emotional involvement" and timelessness that comes from immersive and challenging activities such as software coding or rock climbing. His work is studied by marketing specialists like Vanderbilt University's Donna Hoffman and Thomas Novak, who write that flow is "a central construct when considering consumer navigation on commercial Web sites." In books like Creativity: Flow and the Psychology of Discovery and Invention, Csikszentmihalyi explores the implications of flow for personal and societal evolution.},
    address = {http://www.wired.com/wired/archive/4.09/czik\_pr.html},
    author = {Geirland, John},
    citeulike-article-id = {4803664},
    journal = {Wired},
    keywords = {flow, performance},
    number = {9},
    posted-at = {2009-06-11 00:00:14},
    priority = {2},
    title = {Go With The Flow},
    volume = {4},
    year = {1996}
}

@book{kosko2006:noise,
    abstract = {**From the well-known science commentator and bestselling author of \_FuzzyThinking\_ comes a revelatory look at the phenomenon of noise**A celebrated maverick in the world of science, Bart Kosko introduced—andcontinues to popularize in print and television media—the revolutionaryconcept of fuzzy logic. In his latest book, he provides the first scientifichistory of noise aimed at the general reader.Noise is a social nuisance, a cause of deafness and high blood pressure, andan all-around annoyance. But what is noise really? As Kosko simply states,"Noise is a signal that you don't like." It occurs at every level of thephysical universe, from the big bang to blaring car alarms. Today, noise isconsidered the curse of the information age, but, in fact, not all noise isbad. Debunking this and many other commonly held beliefs about noise, Koskogives readers a vivid sense of how deeply noise permeates both the worldaround us and within us. Along the way he covers many compelling topics, fromnoise's possible role in the ice ages to noise pollution laws, the use ofnoise to generate synthetic speech, and Hedy Lamarr's contribution to noisywireless communication. The result is a vastly entertaining and illuminatingscientific journey that promises to do for noise what James Gleick did forchaos—make it vital, fascinating, and relevant.},
    author = {Kosko, Bart},
    citeulike-article-id = {4800280},
    citeulike-linkout-0 = {http://www.amazon.ca/exec/obidos/redirect?tag=citeulike09-20\&amp;path=ASIN/0670034959},
    citeulike-linkout-1 = {http://www.amazon.de/exec/obidos/redirect?tag=citeulike01-21\&amp;path=ASIN/0670034959},
    citeulike-linkout-2 = {http://www.amazon.fr/exec/obidos/redirect?tag=citeulike06-21\&amp;path=ASIN/0670034959},
    citeulike-linkout-3 = {http://www.amazon.jp/exec/obidos/ASIN/0670034959},
    citeulike-linkout-4 = {http://www.amazon.co.uk/exec/obidos/ASIN/0670034959/citeulike00-21},
    citeulike-linkout-5 = {http://www.amazon.com/exec/obidos/redirect?tag=citeulike07-20\&path=ASIN/0670034959},
    citeulike-linkout-6 = {http://www.worldcat.org/isbn/0670034959},
    citeulike-linkout-7 = {http://books.google.com/books?vid=ISBN0670034959},
    citeulike-linkout-8 = {http://www.amazon.com/gp/search?keywords=0670034959\&index=books\&linkCode=qs},
    citeulike-linkout-9 = {http://www.librarything.com/isbn/0670034959},
    day = {17},
    howpublished = {Hardcover},
    isbn = {0670034959},
    keywords = {book, context\_switching, performance},
    month = {August},
    posted-at = {2009-06-10 19:35:06},
    priority = {2},
    publisher = {Viking Adult},
    title = {Noise},
    url = {http://www.amazon.com/exec/obidos/redirect?tag=citeulike07-20\&path=ASIN/0670034959},
    year = {2006}
}

@inproceedings{maalaEtAl2007:flikrRDF,
    abstract = {The recent evolution of the Web, now designated by the term Web 2.0, has seen the appearance of a huge number of
resources created and annotated by users. However the annotations consist only in simple tags that are gathered in
unstructured sets called folksonomies. The use of more complex languages to annotate resources and to define semantics
according to the vision of the Semantic Web, would improve the understanding by machines and programs, like search
engines, of what is on the Web. Indeed tags expressivity is very low compared to the representation standards of the
Semantic Web, like RDF and OWL. But users appear to be still reluctant to annotate resources with RDF, and it should
be recognized that Semantic Web, contrary to Web 2.0, is still not a reality of today's Web. One way to take advantage of
Semantic Web capabilities right now, without waiting for a change of the annotation usages, would be to be able to
generate RDF annotations from tags. As a first step toward this direction, this paper presents a tentative to automatically
convert a set of tags into a RDF description in the context of photos on Flickr. Such a method exploits some specificity of
tags used on Flickr, some basic natural language processing tools and some semantic resources, in order to relate
semantically tags describing a given photo and build a pertinent RDF annotation for this photo.},
    address = {Poznan, Poland},
    author = {Maala, Mohamed Z. and Delteil, Alexandre and Azough, Ahmed},
    booktitle = {10th Intl. Conf. on Business Information Systems},
    citeulike-article-id = {4710654},
    keywords = {rdf, semantic\_web},
    posted-at = {2009-06-01 20:04:12},
    priority = {2},
    title = {A conversion process from Flickr tags to RDF descriptions},
    year = {2007}
}

@article{trilloEtAl2007:keywordSemantics,
    abstract = {Abstract: The technology in the ﬁeld of digital media generates huge amounts of tex-
tual information every day, so mechanisms to retrieve relevant information are needed.
Under these circumstances, many times current web search engines do not provide
users with the information they seek, because these search tools mainly use syntax
based techniques. However, search engines based on semantic and context information
can help overcome some of the limitations of current alternatives.
In this paper, we propose a system that takes as input a list of plain keywords pro-
vided by a user and translates them into a query expressed in a formal language
without ambiguity. Our system discovers the semantics of user keywords by consulting
the knowledge represented by many (heterogeneous and distributed) ontologies. Then,
context information is used to remove ambiguity and build the most probable query.
Our experiments indicate that our system discovers the user's information need bet-
ter than traditional search engines when the semantics of the request is not the most
popular on the Web.},
    author = {Trillo, Raquel and Gracia, Jorge and Espinoza, Mauricio and Mena, Eduardo},
    citeulike-article-id = {4710618},
    citeulike-linkout-0 = {http://www.jucs.org/jucs\_13\_12/discovering\_the\_semantics\_of},
    journal = {Journal of Universal Computer Science},
    keywords = {information\_retrieval, semantic\_web},
    number = {12},
    pages = {1908--1935},
    posted-at = {2009-06-01 19:58:49},
    priority = {2},
    title = {Discovering the Semantics of User Keywords},
    url = {http://www.jucs.org/jucs\_13\_12/discovering\_the\_semantics\_of},
    volume = {13},
    year = {2007}
}

@techreport{schatz2001:interspaceTR,
    abstract = {A global information infrastructure for knowledge manipulation must support effectiveanalysis to correlate related objects. The Interspace is the coming global network, whereknowledge manipulation is supported by concept navigation across community spaces. We haveproduced a working Interspace Prototype, an analysis environment supporting semantic indexingon community repositories. Scalable technologies have been implemented for concept extractionand concept spaces, which use semantic indexing to facilitate concept navigation. Thesetechnologies have been tested on discipline-scale, real-world document collections. Thetechnologies use statistical clustering, on contextual frequency of document phrases within acollection. Computer trends show that semantic indexing technologies will be practical foreveryday use on community knowledge in the foreseeable future. Thus, concept navigationacross community repositories will become a routine operation.},
    author = {Schatz, Bruce R.},
    citeulike-article-id = {4710415},
    institution = {University of Illinois at Urbana},
    keywords = {information\_retrieval},
    organization = {CANIS (Community Architectures for Network Information Systems) Laboratory},
    posted-at = {2009-06-01 19:39:38},
    priority = {2},
    title = {Concepts across the Interspace:Information Instrastructure for Community Knowledge.},
    year = {2001}
}

@article{schatz2002:interspace,
    abstract = {Within the next decade, computing technology will transform the Internet into the Interspace, an information infrastructure that supports semantic indexing and concept navigation across widely distributed community repositories. With the Interspace, the global information infrastructure will, for the first time, directly support interaction with abstraction. This infrastructure uses technologies that go beyond searching individual repositories to analyze and correlate knowledge across multiple sources and subjects. The Interspace will offer distributed services to transfer concepts across domains, just as Arpanet used distributed services to transfer files across machines and the Internet uses distributed services to transfer objects across repositories. The Community Architectures for Network Information Systems Laboratory has developed a working Interspace prototype that uses scalable technologies for concept extraction and navigation. They have successfully tested these technologies, which compute contextual frequency of document phrases within a community repository, on discipline-scale, real-world collections},
    author = {Schatz, B. R.},
    booktitle = {Computer},
    citeulike-article-id = {3104872},
    citeulike-linkout-0 = {http://dx.doi.org/10.1109/2.976919},
    citeulike-linkout-1 = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=976919},
    doi = {10.1109/2.976919},
    journal = {Computer},
    keywords = {information\_retrieval},
    number = {1},
    pages = {54--62},
    posted-at = {2009-06-01 19:32:16},
    priority = {2},
    title = {The Interspace: concept navigation across distributed communities},
    url = {http://dx.doi.org/10.1109/2.976919},
    volume = {35},
    year = {2002}
}

@proceedings{jansenEtAl2007:queryMod,
    abstract = {We examine 2,465,145 interactions from 534,507 users of Dogpile.com submitted 6 May 2005. We compare query reformulation patterns. We investigate the type of query modifications and query modification transitions within sessions. Searchers most often modified their query by changing query terms (nearly 23\% of all query modifications). Searchers' queries undergoing modification typically transition from Web to image collections in content shifts (37\% of all content transitions), and searchers typically implement assistance at the start of a session or when switching content collections (21\% of all assistance usage). This research sheds light on the more complex aspects of Web searching involving query modifications},
    author = {Jansen, Bernard J. and Spink, Amanda and Narayan, Bhuva},
    booktitle = {Information Technology, 2007. ITNG '07. Fourth Intl. Conf. on},
    citeulike-article-id = {3453343},
    citeulike-linkout-0 = {http://portal.acm.org/citation.cfm?id=1262308},
    citeulike-linkout-1 = {http://dx.doi.org/10.1109/ITNG.2007.164},
    citeulike-linkout-2 = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=4151723},
    doi = {10.1109/ITNG.2007.164},
    journal = {Information Technology, 2007. ITNG '07. Fourth Intl. Conf. on},
    keywords = {information\_extraction, information\_retrieval, query\_formation},
    pages = {439--444},
    posted-at = {2009-06-01 19:26:23},
    priority = {2},
    title = {Query Modifications Patterns During Web Searching},
    url = {http://dx.doi.org/10.1109/ITNG.2007.164},
    year = {2007}
}

@inproceedings{huaEtAl2001:textLocation,
    address = {Ottawa, CA},
    author = {Hua, Xian-Sheng and Chen, Xiang-Rong and Wenyin, Liu and Zhang, Hong-Jiang},
    booktitle = {Workshop on Multimedia Information Retrieval},
    citeulike-article-id = {4544016},
    day = {5},
    keywords = {computer\_vision, text\_detection},
    month = {October},
    posted-at = {2009-05-19 00:06:46},
    priority = {2},
    title = {Automatic Location of Text in Video Frames},
    year = {2001}
}

@proceedings{JainAndYu1998:textLocation,
    abstract = {Automatic text location (without character recognition capabilities) deals with extracting image regions that contain text only. The images of these regions can then be fed to an optical character recognition module or highlighted for users. This is very useful in a number of applications such as database indexing and converting paper documents to their electronic versions. The performance of our automatic text location algorithm is shown in several applications. Compared with some traditional text location methods, our method has the following advantages: 1) low computational cost; 2) robust to font size; and 3) high accuracy},
    author = {Jain, A. K. and Yu, Bin},
    booktitle = {Pattern Recognition, 1998. Proceedings. Fourteenth Intl. Conf. on},
    citeulike-article-id = {4544012},
    citeulike-linkout-0 = {http://dx.doi.org/10.1109/ICPR.1998.711990},
    citeulike-linkout-1 = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=711990},
    doi = {10.1109/ICPR.1998.711990},
    journal = {Pattern Recognition, 1998. Proceedings. Fourteenth Intl. Conf. on},
    keywords = {computer\_vision, text\_detection},
    pages = {1497--1499 vol.2},
    posted-at = {2009-05-18 23:59:38},
    priority = {2},
    title = {Automatic text location in images and video frames},
    url = {http://dx.doi.org/10.1109/ICPR.1998.711990},
    volume = {2},
    year = {1998}
}

@article{hopper1999:sentientComputing,
    abstract = {10.1098/rsta.2000.0652 Sentient computing is the proposition that applications can be made more responsive and useful by observing and reacting to the physical world. It is particularly attractive in a world of mobile users and computers. The paper presents a classification and quantification of sensor information together with a description of a method for altering the behaviour of arbitrary terminal devices. It also presents a framework for 'programming with space' which can associate space–related events with actions. Consideration is given to the applications made possible by such systems.},
    author = {Hopper, Andy},
    citeulike-article-id = {4450687},
    citeulike-linkout-0 = {http://dx.doi.org/10.1098/rsta.2000.0652},
    citeulike-linkout-1 = {http://rsta.royalsocietypublishing.org/content/358/1773/2349.abstract},
    citeulike-linkout-2 = {http://rsta.royalsocietypublishing.org/content/358/1773/2349.full.pdf},
    day = {15},
    doi = {10.1098/rsta.2000.0652},
    journal = {Philosophical Transactions of the Royal Society of London. Series A: Mathematical, Physical and Engineering Sciences},
    keywords = {sentient\_computing},
    month = {August},
    number = {1773},
    pages = {2349--2358},
    posted-at = {2009-05-02 00:19:04},
    priority = {2},
    title = {The Clifford Paterson Lecture, 1999. Sentient computing},
    url = {http://dx.doi.org/10.1098/rsta.2000.0652},
    volume = {358},
    year = {2000}
}

@article{SchmidtEtAl1999:moreThanLoc,
    abstract = {Context is a key issue in interaction between human and computer, describing the surrounding facts that add meaning. In mobile computing research published the parameter location is most often used to approximate context and to implement context-aware applications. We propose that ultra-mobile computing, characterized by devices that are operational and operated while on the move (e.g. PDAs, mobile phones, wearable computers), can significantly benefit from a wider notion of context. To structure the field we introduce a working model for context, discuss mechanisms to acquire context beyond location, and application of context-awareness in ultra-mobile computing. We investigate the utility of sensors for context-awareness and present two prototypical implementations - a light sensitive display and an orientation aware PDA interface. The concept is then extended to a model for sensor fusion to enable more sophisticated context recognition. Based on an implementation of the model an exp...},
    author = {Schmidt, Albrecht and Beigl, Michael and Hans-W},
    citeulike-article-id = {4450683},
    citeulike-linkout-0 = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.37.2933},
    journal = {Computers and Graphics},
    keywords = {context\_aware, graphics},
    pages = {893--901},
    posted-at = {2009-05-02 00:16:19},
    priority = {2},
    title = {There is more to Context than Location},
    url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.37.2933},
    volume = {23},
    year = {1998}
}

@article{JahnkeEtAl2005:contextAwareInfoDelivery,
    author = {Jahnke, J. and Bychkov, Y. and Dahlem, D. and Kawasme, L.},
    citeulike-article-id = {4450682},
    citeulike-linkout-0 = {http://ria.revuesonline.com/article.jsp?articleId=5985},
    journal = {Revue d'Intelligence Artificielle},
    keywords = {context\_aware},
    number = {3},
    pages = {459--478},
    posted-at = {2009-05-02 00:09:17},
    priority = {2},
    title = {Context-Aware Information Delivery. An Application in the Health Care Domain},
    url = {http://ria.revuesonline.com/article.jsp?articleId=5985},
    volume = {19},
    year = {2005}
}

@inproceedings{SchilitEtAl1994:contextaware,
    abstract = {This paper describes software that examines and reacts to an individual's changing context. Such software  can promote and mediate people's interactions with devices, computers, and other people, and it can help  navigate unfamiliar places. We believe that a limited amount of information covering a person's proximate  environment is most important for this form of computing since the interesting part of the world around  us is what we can see, hear, and touch. In this paper we define context-aware computing, and describe  four categories of context-aware applications: proximate selection, automatic contextual reconfiguration,  contextual information and commands, and context-triggered actions. Instances of these application types  have been prototyped on the PARCTAB, a wireless, palm-sized computer.  1 Introduction  Our investigation focuses on an extended form of mobile computing in which users employ many different mobile, stationary and embedded computers over the course of the day....},
    author = {Schilit, Bill N. and Adams, Norman and Want, Roy},
    booktitle = {In Proc. of the Workshop on Mobile Computing Systems and Applications},
    citeulike-article-id = {4450680},
    citeulike-linkout-0 = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.34.8950},
    keywords = {context\_aware},
    pages = {85--90},
    posted-at = {2009-05-02 00:04:43},
    priority = {2},
    title = {Context-Aware Computing Applications},
    url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.34.8950},
    year = {1994}
}

@inproceedings{hoffmannEtAl2008:windowSwitching,
    address = {New York, NY, USA},
    author = {Hoffmann, Raphael and Baudisch, Patrick and Weld, Daniel S.},
    booktitle = {CHI '08: Proceeding of the twenty-sixth annual SIGCHI conf. on Human factors in computing systems},
    citeulike-article-id = {2718538},
    citeulike-linkout-0 = {http://portal.acm.org/citation.cfm?id=1357054.1357199},
    citeulike-linkout-1 = {http://dx.doi.org/10.1145/1357054.1357199},
    doi = {10.1145/1357054.1357199},
    isbn = {9781605580111},
    keywords = {chi08, interface\_design},
    pages = {929--938},
    posted-at = {2009-04-30 22:22:34},
    priority = {2},
    publisher = {ACM},
    title = {Evaluating visual cues for window switching on large screens},
    url = {http://dx.doi.org/10.1145/1357054.1357199},
    year = {2008}
}

@inproceedings{GajosEtAl2008:adaptiveUI,
    abstract = {While proponents of adaptive user interfaces tout potential performance gains, critics argue that adaptation's unpredictability may disorient users, causing more harm than good. We present a study that examines the relative effects of predictability and accuracy on the usability of adaptive UIs. Our results show that increasing predictability and accuracy led to strongly improved satisfaction. Increasing accuracy also resulted in improved performance and higher utilization of the adaptive interface. Contrary to our expectations, improvement in accuracy had a stronger effect on performance, utilization and some satisfaction ratings than the improvement in predictability.},
    address = {New York, NY, USA},
    author = {Gajos, Krzysztof Z. and Everitt, Katherine and Tan, Desney S. and Czerwinski, Mary and Weld, Daniel S.},
    booktitle = {CHI '08: Proceeding of the twenty-sixth annual SIGCHI conf. on Human factors in computing systems},
    citeulike-article-id = {2862354},
    citeulike-linkout-0 = {http://portal.acm.org/citation.cfm?id=1357054.1357252},
    citeulike-linkout-1 = {http://dx.doi.org/10.1145/1357054.1357252},
    doi = {10.1145/1357054.1357252},
    isbn = {978-1-60558-011-1},
    keywords = {adaptive\_interfaces, chi08, interface\_design},
    location = {Florence, Italy},
    pages = {1271--1274},
    posted-at = {2009-04-30 22:20:04},
    priority = {2},
    publisher = {ACM},
    title = {Predictability and accuracy in adaptive user interfaces},
    url = {http://dx.doi.org/10.1145/1357054.1357252},
    year = {2008}
}

@inproceedings{WuEtAl2008:wikipedia_tail,
    abstract = {Not only is Wikipedia a comprehensive source of quality information, it has several kinds of internal structure (e.g., relational summaries known as  infoboxes ), which enable self-supervised information extraction. While previous efforts at extraction from Wikipedia achieve high precision and recall on well-populated classes of articles, they fail in a larger number of cases, largely because incomplete articles and infrequent use of infoboxes lead to insufficient training data. This paper presents three novel techniques for increasing recall from Wikipedia's long tail of sparse classes: (1) shrinkage over an automatically-learned subsumption taxonomy, (2) a retraining technique for improving the training data, and (3) supplementing results by extracting from the broader Web. Our experiments compare design variations and show that, used in concert, these techniques increase recall by a factor of 1.76 to 8.71 while maintaining or increasing precision.},
    address = {New York, NY, USA},
    author = {Wu, Fei and Hoffmann, Raphael and Weld, Daniel S.},
    booktitle = {KDD '08: Proceeding of the 14th ACM SIGKDD intl. conf. on Knowledge discovery and data mining},
    citeulike-article-id = {3196704},
    citeulike-linkout-0 = {http://portal.acm.org/citation.cfm?id=1401890.1401978},
    citeulike-linkout-1 = {http://dx.doi.org/10.1145/1401890.1401978},
    doi = {10.1145/1401890.1401978},
    isbn = {978-1-60558-193-4},
    keywords = {information\_extraction},
    location = {Las Vegas, Nevada, USA},
    pages = {731--739},
    posted-at = {2009-04-30 22:18:32},
    priority = {2},
    publisher = {ACM},
    title = {Information extraction from Wikipedia: moving down the long tail},
    url = {http://dx.doi.org/10.1145/1401890.1401978},
    year = {2008}
}

@inproceedings{WeldEtAl2006:EUP_UW,
    abstract = {Over the past decade our research group at the University of Washington has investigated a number of techniques for improving end-user customization and programming. Much of this work has been reported in the AI literature, and we seek to participate in the Second Workshop on End-User Software Engineering in order to expand our understanding of existing work and alternative approaches.},
    address = {Montreal, Canada},
    author = {Weld, Daniel S. and Domingos, Pedro and Hoffmann, Raphael and Sanghai, Sumit},
    booktitle = {CHI'06 Workshop on End-User Software Engineering},
    citeulike-article-id = {4446414},
    keywords = {chi06, end\_user\_programmers, pbd, version\_spaces},
    posted-at = {2009-04-30 22:11:09},
    priority = {2},
    title = {End-user Programming at the University of Washington},
    year = {2006}
}

@proceedings{frankFoley1994:pdbInference,
    author = {Frank, Martin R. and Foley, James D.},
    booktitle = {UIST '94: Proc. of the 7th annual ACM symposium on User interface software and technology},
    citeulike-article-id = {107640},
    citeulike-linkout-0 = {http://portal.acm.org/citation.cfm?id=192426.192466},
    citeulike-linkout-1 = {http://dx.doi.org/10.1145/192426.192466},
    doi = {10.1145/192426.192466},
    isbn = {0897916573},
    keywords = {ai, pbd, uist94},
    pages = {95--101},
    posted-at = {2009-04-10 00:09:02},
    priority = {2},
    publisher = {ACM Press},
    title = {A pure reasoning engine for programming by demonstration},
    url = {http://dx.doi.org/10.1145/192426.192466},
    year = {1994}
}

@inproceedings{chenWeld2008:recovering-pbd,
    abstract = {Many end-users wish to customize their applications, automating common tasks and routines. Unfortunately, this automation is difficult today --- users must choose between brittle macros and complex scripting languages. Programming by demonstration (PBD) offers a middle ground, allowing users to demonstrate a procedure multiple times and generalizing the requisite behavior with machine learning. Unfortunately, many PBD systems are almost as brittle as macro recorders, offering few ways for a user to control the learning process or correct the demonstrations used as training examples. This paper presents CHINLE, a system which automatically constructs PBD systems for applications based on their interface specification. The resulting PBD systems have novel interaction and visualization methods, which allow the user to easily monitor and guide the learning process, facilitating error recovery during training. CHINLE-constructed PBD systems learn procedures with conditionals and perform partial learning if the procedure is too complex to learn completely.},
    address = {New York, NY, USA},
    author = {Chen, Jiun H. and Weld, Daniel S.},
    booktitle = {IUI '08: Proc. of the 13th intl. conf. on Intelligent user interfaces},
    citeulike-article-id = {4295490},
    citeulike-linkout-0 = {http://portal.acm.org/citation.cfm?id=1378794},
    citeulike-linkout-1 = {http://dx.doi.org/10.1145/1378773.1378794},
    doi = {10.1145/1378773.1378794},
    isbn = {978-1-59593-987-6},
    keywords = {ai, algorithms, iui08, pbd, version\_spaces},
    location = {Gran Canaria, Spain},
    pages = {159--168},
    posted-at = {2009-04-09 22:49:21},
    priority = {2},
    publisher = {ACM},
    title = {Recovering from errors during programming by demonstration},
    url = {http://dx.doi.org/10.1145/1378773.1378794},
    year = {2008}
}

@article{erramiEtAl2008:dejavu,
    abstract = {MOTIVATION: Duplicate publication impacts the quality of the scientific corpus, has been difficult to detect, and studies this far have been limited in scope and size. Using text similarity searches, we were able to identify signatures of duplicate citations among a body of abstracts. RESULTS: A sample of 62,213 Medline citations was examined and a database of manually verified duplicate citations was created to study author publication behavior. We found that 0.04\% of the citations with no shared authors were highly similar and are thus potential cases of plagiarism. 1.35\% with shared authors were sufficiently similar to be considered a duplicate. Extrapolating, this would correspond to 3500 and 117,500 duplicate citations in total, respectively. AVAILABILITY: eTBLAST, an automated citation matching tool, and D\'{e}j\`{a} vu, the duplicate citation database, are freely available at http://invention.swmed.edu/ and http://spore.swmed.edu/dejavu},
    author = {Errami, Mounir and Hicks, Justin M. and Fisher, Wayne and Trusty, David and Wren, Jonathan D. and Long, Tara C. and Garner, Harold R.},
    citeulike-article-id = {2048100},
    citeulike-linkout-0 = {http://dx.doi.org/10.1093/bioinformatics/btm574},
    citeulike-linkout-1 = {http://bioinformatics.oxfordjournals.org/cgi/content/abstract/24/2/243},
    citeulike-linkout-2 = {http://view.ncbi.nlm.nih.gov/pubmed/18056062},
    citeulike-linkout-3 = {http://www.hubmed.org/display.cgi?uids=18056062},
    day = {15},
    doi = {10.1093/bioinformatics/btm574},
    issn = {1367-4811},
    journal = {Bioinformatics (Oxford, England)},
    keywords = {plagiarism},
    month = {January},
    number = {2},
    pages = {243--249},
    posted-at = {2009-03-09 18:45:00},
    priority = {5},
    title = {D\'{e}j\`{a} vu--a study of duplicate citations in Medline.},
    url = {http://dx.doi.org/10.1093/bioinformatics/btm574},
    volume = {24},
    year = {2008}
}

@article{citeulike:4140336,
    abstract = {10.1126/science.1167408},
    author = {Long, Tara C. and Errami, Mounir and George, Angela C. and Sun, Zhaohui and Garner, Harold R.},
    citeulike-article-id = {4140336},
    citeulike-linkout-0 = {http://dx.doi.org/10.1126/science.1167408},
    citeulike-linkout-1 = {http://www.sciencemag.org/content/323/5919/1293.abstract},
    citeulike-linkout-2 = {http://www.sciencemag.org/content/323/5919/1293.full.pdf},
    citeulike-linkout-3 = {http://www.sciencemag.org/cgi/content/abstract/323/5919/1293},
    citeulike-linkout-4 = {http://view.ncbi.nlm.nih.gov/pubmed/19265004},
    citeulike-linkout-5 = {http://www.hubmed.org/display.cgi?uids=19265004},
    day = {6},
    doi = {10.1126/science.1167408},
    journal = {Science},
    keywords = {plagiarism},
    month = {March},
    number = {5919},
    pages = {1293--1294},
    posted-at = {2009-03-07 05:48:04},
    priority = {5},
    title = {SCIENTIFIC INTEGRITY: Responding to Possible Plagiarism},
    url = {http://dx.doi.org/10.1126/science.1167408},
    volume = {323},
    year = {2009}
}

@article{scullyEtAl1994:negProb,
    author = {Scully, Marlan O. and Walther, Herbert and Schleich, Wolfgang},
    citeulike-article-id = {4069043},
    citeulike-linkout-0 = {http://dx.doi.org/10.1103/PhysRevA.49.1562},
    citeulike-linkout-1 = {http://link.aps.org/abstract/PRA/v49/i3/p1562},
    citeulike-linkout-2 = {http://link.aps.org/pdf/PRA/v49/i3/p1562},
    day = {1},
    doi = {10.1103/PhysRevA.49.1562},
    journal = {Physical Review A},
    keywords = {negative\_probability, probability, statistics},
    month = {March},
    number = {3},
    pages = {1562+},
    posted-at = {2009-02-18 23:15:58},
    priority = {2},
    publisher = {American Physical Society},
    title = {Feynman's approach to negative probability in quantum mechanics},
    url = {http://dx.doi.org/10.1103/PhysRevA.49.1562},
    volume = {49},
    year = {1994}
}

@article{savovaEtAl2008:wsdDomains,
    author = {Savova, G. and Coden, A. and Sominsky, I. and Johnson, R. and Ogren, P. and Groen, P. and Chute, C.},
    citeulike-article-id = {3732914},
    citeulike-linkout-0 = {http://portal.acm.org/citation.cfm?id=1460946.1461338},
    citeulike-linkout-1 = {http://dx.doi.org/10.1016/j.jbi.2008.02.003},
    citeulike-linkout-2 = {http://linkinghub.elsevier.com/retrieve/pii/S1532046408000245},
    citeulike-linkout-3 = {http://view.ncbi.nlm.nih.gov/pubmed/18375190},
    citeulike-linkout-4 = {http://www.hubmed.org/display.cgi?uids=18375190},
    doi = {10.1016/j.jbi.2008.02.003},
    issn = {15320464},
    journal = {Journal of Biomedical Informatics},
    keywords = {bioinformatics, nlp, word-sense-dissambiguation},
    month = {December},
    number = {6},
    pages = {1088--1100},
    posted-at = {2009-02-10 00:32:07},
    priority = {2},
    title = {Word sense disambiguation across two domains: Biomedical literature and clinical notes},
    url = {http://dx.doi.org/10.1016/j.jbi.2008.02.003},
    volume = {41},
    year = {2008}
}

@article{xuEtAl2006:wsdBio,
    author = {Xu, Hua and Markatou, Marianthi and Dimova, Rositsa and Liu, Hongfang and Friedman, Carol},
    citeulike-article-id = {744821},
    citeulike-linkout-0 = {http://dx.doi.org/10.1186/1471-2105-7-334},
    citeulike-linkout-1 = {http://view.ncbi.nlm.nih.gov/pubmed/16822321},
    citeulike-linkout-2 = {http://www.hubmed.org/display.cgi?uids=16822321},
    day = {05},
    doi = {10.1186/1471-2105-7-334},
    issn = {1471-2105},
    journal = {BMC Bioinformatics},
    keywords = {bioinformatics, nlp, word-sense-dissambiguation},
    month = {July},
    pages = {334+},
    posted-at = {2009-02-10 00:27:57},
    priority = {2},
    title = {Machine learning and word sense disambiguation in the biomedical domain: design and evaluation issues},
    url = {http://dx.doi.org/10.1186/1471-2105-7-334},
    volume = {7},
    year = {2006}
}

@inproceedings{gaustad2001:pseudoVsReal,
    abstract = {In this paper we investigate whether the task of disambiguating pseudowords (artificial ambiguous words) is comparable to the disambiguation of real ambiguous words. Since the two methods are inherently different, a direct comparison is not possible. An indirect approach is taken where the setup for both systems is as similar as possible, i.e. using the same corpus and settings. The results obtained clearly indicate that the tasks are quite different. We conclude that the current practice of using pseudowords cannot be taken as a substitute for testing with real ambiguous words. 1},
    author = {Gaustad, Tanja and Groningen, Rijksuniversiteit},
    booktitle = {In Companion Volume to the Proc. of the 39th Annual Meeting of the Association for Computational Linguistics (ACL/EACL 2001) – Proc. of the Student Research Workshop},
    citeulike-article-id = {4027199},
    citeulike-linkout-0 = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.20.9203},
    comment = {The results in this article seem suspect, although the idea behind the author's claims seems reasonable.  The data set of 4 "real" words is very small, and no formal analysis of the results was provided to justify the claims of significance.  Furthermore, the authors chose pseudowords based on the distribution of the real words in the test set only.  The distribution of pseudoword senses in the training set does not come close to the distribution of the corresponding real words in the training set. (These distributions are not provided directly, but they can be calculated from data in Table 1.)  This would not be as concerting, however, the pseudowords that exhibit this discrepancy also appear to be the only examples that actually support the author's hypothesis.},
    keywords = {nlp, pseudowords, statistics, word-sense-dissambiguation},
    pages = {61--66},
    posted-at = {2009-02-09 23:58:56},
    priority = {0},
    title = {Statistical corpus-based word sense disambiguation: Pseudowords vs. real ambiguous words},
    url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.20.9203},
    year = {2001}
}

@inproceedings{maedcheStaab2002:ontSim,
    address = {London, UK},
    author = {Maedche, Alexander and Staab, Steffen},
    booktitle = {EKAW '02: Proc. of the 13th Intl. Conf. on Knowledge Engineering and Knowledge Management. Ontologies and the Semantic Web},
    citeulike-article-id = {4018590},
    citeulike-linkout-0 = {http://portal.acm.org/citation.cfm?id=650859},
    isbn = {3-540-44268-5},
    keywords = {ontologies, similarity},
    pages = {251--263},
    posted-at = {2009-02-06 22:20:01},
    priority = {2},
    publisher = {Springer-Verlag},
    title = {Measuring Similarity between Ontologies},
    url = {http://portal.acm.org/citation.cfm?id=650859},
    year = {2002}
}

@inproceedings{stumpfEtAl2008:iui,
    abstract = {The potential for machine learning systems to improve via a
mutually beneficial exchange of information with users has
yet to be explored in much detail. Previously, we found that
users were willing to provide a generous amount of rich
feedback to machine learning systems, and that the types of
some of this rich feedback seem promising for assimilation
by machine learning algorithms. Following up on those
findings, we ran an experiment to assess the viability of
incorporating real-time keyword-based feedback in initial
training phases when data is limited. We found that rich
feedback improved accuracy but an initial unstable period
often caused large fluctuations in classifier behavior. Partic-
ipants were able to give feedback by relying heavily on sys-
tem communication in order to respond to changes. The
results show that in order to benefit from the user's know-
ledge, machine learning systems must be able to absorb
keyword-based rich feedback in a graceful manner and pro-
vide clear explanations of their predictions.},
    author = {Simone},
    booktitle = {ACM Intl. Conf. on Intelligent User Interfaces},
    citeulike-article-id = {3656404},
    keywords = {iui08},
    month = {January},
    posted-at = {2008-11-21 19:34:07},
    priority = {5},
    title = {Integrating Rich User Feedback into Intelligent User Interfaces},
    year = {2008}
}

@inproceedings{skalkaWang2004:trust,
    address = {New York, NY, USA},
    author = {Skalka, Christian and Wang, Sean X.},
    booktitle = {SWS '04: Proc. of the 2004 workshop on Secure web service},
    citeulike-article-id = {611375},
    citeulike-linkout-0 = {http://portal.acm.org/citation.cfm?id=1111354},
    citeulike-linkout-1 = {http://dx.doi.org/10.1145/1111348.1111354},
    doi = {10.1145/1111348.1111354},
    isbn = {158113973X},
    keywords = {security, webservices},
    pages = {47--55},
    posted-at = {2008-08-20 21:00:22},
    priority = {2},
    publisher = {ACM Press},
    title = {Trust but verify: authorization for web services},
    url = {http://dx.doi.org/10.1145/1111348.1111354},
    year = {2004}
}

@article{SkalkaEtAl:riskManagementJCS,
    author = {Skalka, Christian and Wang, Sean X. and Chapin, Peter},
    citeulike-article-id = {3141770},
    day = {To appear},
    journal = {Journal of Computer Security},
    posted-at = {2008-08-20 20:57:21},
    priority = {2},
    title = {Risk Management for Distributed Authorization}
}

@article{yaoEtAl2006:DBprivacyJCS,
    author = {Yao, Chao and Wang, Lingyu and Wang, Sean X. and Jajodia, Sushil},
    citeulike-article-id = {3141767},
    day = {To appear},
    journal = {Journal of Computer Security},
    keywords = {security},
    posted-at = {2008-08-20 20:54:44},
    priority = {2},
    title = {Evaluating privacy threats in released database views by symmetric indistinguishability}
}

@inproceedings{yaoEtAl2006:DBprivacy,
    address = {Seoul, Korea},
    author = {Yao, Chao and Wang, Lingyu and Wang, Sean X. and Jajodia, Sushil},
    booktitle = {Proc. 3rd VLDB Workshop on Secure Data Management (SDM'06)},
    citeulike-article-id = {3141763},
    day = {10},
    editor = {Jonker, Willem and Petkovic, Milan},
    keywords = {security},
    month = {September},
    pages = {1--17},
    posted-at = {2008-08-20 20:51:34},
    priority = {2},
    publisher = {Springer},
    title = {Indistinguishability: the other aspect of privacy},
    year = {2006}
}

@techreport{gomezPerezManzano2003:ontologies,
    author = {G\'{o}mez-P\'{e}rez, Asunci\'{o}n and Manzano-Macho, David},
    citeulike-article-id = {2504966},
    day = {30},
    keywords = {ontologies},
    month = {May},
    organization = {IST Programme of the Commission of the European Communities},
    posted-at = {2008-08-20 18:57:40},
    priority = {2},
    series = {Next Web Generation},
    title = {Deliverable 1.5: A survey of ontology learning methods and
techniques},
    year = {2003}
}

@article{lunt1989:aggregation,
    abstract = {This paper examines inference and aggregation problems that can arise in multilevel relational database systems and points out some fallacies in our thinking about these problems that may hinder real progress from being made toward their solution. Although others have done come initial research toward solving inference problems, aggregation has been treated only superficially in the literature. This paper attempts to lay a firmer foundation for a theory of these problems. Several types of problem are identified and approaches toward their solution suggested.},
    address = {Los Alamitos, CA, USA},
    author = {Lunt, Teresa F.},
    citeulike-article-id = {3134592},
    citeulike-linkout-0 = {http://doi.ieeecomputersociety.org/10.1109/SECPRI.1989.36284},
    citeulike-linkout-1 = {http://dx.doi.org/10.1109/SECPRI.1989.36284},
    doi = {10.1109/SECPRI.1989.36284},
    issn = {1540-7993},
    journal = {sp},
    keywords = {security},
    posted-at = {2008-08-19 01:23:53},
    priority = {2},
    publisher = {IEEE Computer Society},
    title = {Aggregation and Inference: Facts and Fallacies},
    url = {http://dx.doi.org/10.1109/SECPRI.1989.36284},
    volume = {00},
    year = {1989}
}

@proceedings{min-ruiEtAl2007:watermarking,
    abstract = {A novel document watermarking algorithm is proposed. In this algorithm a watermark is embedded into a WORD document by modifying the font color attribute. Experimental results show that this algorithm has good perceptual transparency, large capacity and security. It is robust to format change attacks and the WINRAR compress attack. This algorithm is sensitive to content attacks, such as adding, deleting and supplanting. It can locate tamper position and recover the original content which had been tempered.},
    author = {Min-Rui, Zhang and Yi-Xuan, Wang and Zhan, Yao},
    booktitle = {Anti-counterfeiting, Security, Identification, 2007 IEEE Intl. Workshop on},
    citeulike-article-id = {3134131},
    citeulike-linkout-0 = {http://dx.doi.org/10.1109/IWASID.2007.373731},
    citeulike-linkout-1 = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=4244817},
    doi = {10.1109/IWASID.2007.373731},
    journal = {Anti-counterfeiting, Security, Identification, 2007 IEEE Intl. Workshop on},
    pages = {223--226},
    posted-at = {2008-08-18 20:06:49},
    priority = {2},
    title = {A Novel Semi-Fragile Document Watermarking Algorithm},
    url = {http://dx.doi.org/10.1109/IWASID.2007.373731},
    year = {2007}
}

@inproceedings{chenEtAl2001:watermarks,
    author = {Chen, Minya and Wong, Edward K. and Memon, Nasir and Adams, Scott},
    booktitle = {Proc. of SPIE Vol. 4518 Multimedia Systems and Applications IV},
    citeulike-article-id = {3134123},
    editor = {Tescher, Andrew G. and Vasudev, Bhaskaran and Bove, Michael V.},
    keywords = {data\_hiding, watermarking},
    month = {November},
    pages = {166--176},
    posted-at = {2008-08-18 20:00:36},
    priority = {2},
    title = {Recent Developments in Document Image Watermarking and Data Hiding},
    year = {2001}
}

@article{yangKot2007:watermarks,
    abstract = {In this paper, a novel blind data hiding method for binary images authentication aims at preserving the connectivity of pixels in a local neighborhood is proposed. The "flippability" of a pixel is determined by imposing three transition criteria in a 3 times 3 moving window centered at the pixel. The "embeddability" of a block is invariant in the watermark embedding process, hence the watermark can be extracted without referring to the original image. The "uneven embeddability" of the host image is handled by embedding the watermark only in those "embeddable" blocks. The locations are chosen in such a way that the visual quality of the watermarked image is guaranteed. Different types of blocks are studied and their abilities to increase the capacity are compared. The problem of how to locate the "embeddable" pixels in a block for different block schemes is addressed which facilitates the incorporation of the cryptographic signature as the hard authenticator watermark to ensure integrity and authenticity of the image. Discussions on the security considerations, visual quality against capacity, counter measures against steganalysis and analysis of the computational load are provided. Comparisons with prior methods show superiority of the proposed scheme},
    author = {Yang, H. and Kot, A. C.},
    booktitle = {Multimedia, IEEE Transactions on},
    citeulike-article-id = {2401481},
    citeulike-linkout-0 = {http://dx.doi.org/10.1109/TMM.2006.887990},
    citeulike-linkout-1 = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=4130382},
    doi = {10.1109/TMM.2006.887990},
    journal = {Multimedia, IEEE Transactions on},
    keywords = {data\_hiding, watermarking},
    number = {3},
    pages = {475--486},
    posted-at = {2008-08-18 19:54:09},
    priority = {2},
    title = {Pattern-Based Data Hiding for Binary Image Authentication by Connectivity-Preserving},
    url = {http://dx.doi.org/10.1109/TMM.2006.887990},
    volume = {9},
    year = {2007}
}

@inproceedings{mihalcea:2004:CONLL,
    author = {Mihalcea, Rada},
    booktitle = {HLT-NAACL 2004 Workshop: Eighth Conf. on Computational Natural Language Learning},
    citeulike-article-id = {3134105},
    day = {6},
    editor = {Hwee},
    location = {Boston, Massachusetts, USA},
    month = {May},
    pages = {33--40},
    posted-at = {2008-08-18 19:34:34},
    priority = {2},
    publisher = {Association for Computational Linguistics},
    title = {Co-training and Self-training for Word Sense Disambiguation},
    year = {2004}
}

@inproceedings{suEtAl2004:wsd,
    address = {Morristown, NJ, USA},
    author = {Su, Weifeng and Carpuat, Marine and Wu, Dekai},
    booktitle = {COLING '04: Proc. of the 20th intl. conf. on Computational Linguistics},
    citeulike-article-id = {3134100},
    citeulike-linkout-0 = {http://portal.acm.org/citation.cfm?id=1220545},
    citeulike-linkout-1 = {http://dx.doi.org/10.3115/1220355.1220545},
    doi = {10.3115/1220355.1220545},
    keywords = {svm, word-sense-dissambiguation},
    location = {Geneva, Switzerland},
    posted-at = {2008-08-18 19:29:08},
    priority = {2},
    publisher = {Association for Computational Linguistics},
    title = {Semi-supervised training of a kernel PCA-based model for word sense disambiguation},
    url = {http://dx.doi.org/10.3115/1220355.1220545},
    year = {2004}
}

@inproceedings{wuEtAl2004:wsd,
    address = {Morristown, NJ, USA},
    author = {Wu, Dekai and Su, Weifeng and Carpuat, Marine},
    booktitle = {ACL '04: Proc. of the 42nd Annual Meeting on Association for Computational Linguistics},
    citeulike-article-id = {3134097},
    citeulike-linkout-0 = {http://portal.acm.org/citation.cfm?id=1219036},
    citeulike-linkout-1 = {http://dx.doi.org/10.3115/1218955.1219036},
    doi = {10.3115/1218955.1219036},
    keywords = {svm, word-sense-dissambiguation},
    location = {Barcelona, Spain},
    posted-at = {2008-08-18 19:27:31},
    priority = {2},
    publisher = {Association for Computational Linguistics},
    title = {A kernel PCA method for superior word sense disambiguation},
    url = {http://dx.doi.org/10.3115/1218955.1219036},
    year = {2004}
}

@inproceedings{udeaSaito2002:categorization,
    address = {New York, NY, USA},
    author = {Ueda, Naonori and Saito, Kazumi},
    booktitle = {KDD '02: Proc. of the eighth ACM SIGKDD intl. conf. on Knowledge discovery and data mining},
    citeulike-article-id = {3134093},
    citeulike-linkout-0 = {http://portal.acm.org/citation.cfm?id=775140},
    citeulike-linkout-1 = {http://dx.doi.org/10.1145/775047.775140},
    doi = {10.1145/775047.775140},
    isbn = {158113567X},
    keywords = {document\_clustering, information\_extraction, kdd04, text\_categorization},
    location = {Edmonton, Alberta, Canada},
    pages = {626--631},
    posted-at = {2008-08-18 19:25:56},
    priority = {2},
    publisher = {ACM},
    title = {Single-shot detection of multiple categories of text using parametric mixture models},
    url = {http://dx.doi.org/10.1145/775047.775140},
    year = {2002}
}

@inproceedings{jachims1998:svmsShort,
    address = {London, UK},
    author = {Joachims, Thorsten},
    booktitle = {ECML '98: Proc. of the 10th European Conf. on Machine Learning},
    citeulike-article-id = {3134088},
    citeulike-linkout-0 = {http://portal.acm.org/citation.cfm?id=649721},
    isbn = {3-540-64417-2},
    keywords = {document\_clustering, document\_linking, information\_retrieval, svm},
    pages = {137--142},
    posted-at = {2008-08-18 19:22:17},
    priority = {2},
    publisher = {Springer-Verlag},
    title = {Text Categorization with Suport Vector Machines: Learning with Many Relevant Features},
    url = {http://portal.acm.org/citation.cfm?id=649721},
    year = {1998}
}

@book{mitchell1997:ml,
    author = {Mitchell, Thomas},
    citeulike-article-id = {218147},
    citeulike-linkout-0 = {http://www.amazon.ca/exec/obidos/redirect?tag=citeulike09-20\&amp;path=ASIN/0071154671},
    citeulike-linkout-1 = {http://www.amazon.de/exec/obidos/redirect?tag=citeulike01-21\&amp;path=ASIN/0071154671},
    citeulike-linkout-2 = {http://www.amazon.fr/exec/obidos/redirect?tag=citeulike06-21\&amp;path=ASIN/0071154671},
    citeulike-linkout-3 = {http://www.amazon.jp/exec/obidos/ASIN/0071154671},
    citeulike-linkout-4 = {http://www.amazon.co.uk/exec/obidos/ASIN/0071154671/citeulike00-21},
    citeulike-linkout-5 = {http://www.amazon.com/exec/obidos/redirect?tag=citeulike07-20\&path=ASIN/0071154671},
    citeulike-linkout-6 = {http://www.worldcat.org/isbn/0071154671},
    citeulike-linkout-7 = {http://books.google.com/books?vid=ISBN0071154671},
    citeulike-linkout-8 = {http://www.amazon.com/gp/search?keywords=0071154671\&index=books\&linkCode=qs},
    citeulike-linkout-9 = {http://www.librarything.com/isbn/0071154671},
    day = {01},
    edition = {1st},
    howpublished = {Paperback},
    isbn = {0071154671},
    keywords = {ai, book, machine\_learning},
    month = {October},
    posted-at = {2008-08-18 19:13:39},
    priority = {2},
    publisher = {McGraw Hill Higher Education},
    title = {Machine Learning (Mcgraw-Hill Intl. Edit)},
    url = {http://www.amazon.com/exec/obidos/redirect?tag=citeulike07-20\&path=ASIN/0071154671},
    year = {1997}
}

@proceedings{lin1990:aggregateSecurity,
    abstract = {Proposes a probabilistic type of measure theory which estimates the amount of security relevant information of every subset of a given aggregate. This probabilistic measure theory provides each application a means or mechanism to furnish the system a numerical measure to assist the system security officer for making decisions on releasing or downgrading the internal data of the aggregate. A scenario for solving the aggregation problems during the design phase is proposed. In developing this work, two guiding principles are applied: Minimum Aggregation Principle (MAP) and the Maximum Protection Principle (MPP). MAP keeps both the number of elements in an aggregation and the number of aggregates to a minimum, while MPP keeps the unnecessary risks to a minimum},
    author = {Lin, T. Y.},
    booktitle = {Computer Security Applications Conf., 1990., Proc. of the Sixth Annual},
    citeulike-article-id = {3110238},
    citeulike-linkout-0 = {http://dx.doi.org/10.1109/CSAC.1990.143787},
    citeulike-linkout-1 = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=143787},
    doi = {10.1109/CSAC.1990.143787},
    journal = {Computer Security Applications Conf., 1990., Proc. of the Sixth Annual},
    keywords = {security},
    pages = {286--294},
    posted-at = {2008-08-11 22:54:12},
    priority = {0},
    title = {Probabilistic measure on aggregations [data security]},
    url = {http://dx.doi.org/10.1109/CSAC.1990.143787},
    year = {1990}
}

@inproceedings{creswickEtAl2008:InfoTracker,
    abstract = {The accurate tracking and retrieval of content pedigree is
a quickly growing requirement as our abilities to create information
assets increases exponentially. Plagiarism detection, accurate accred-
itation, and classiﬁcation tasks all rely on the ability to determine
where content is being used and where it originated. We present an
approach to document pedigree tracking that is based on an efﬁcient
disk-based data structure and the use of two contrasting collections of
historical text. These collections enable content of two types (or de-
grees of importance) to be deﬁned and accounted for when locating
documents with overlapping content. This approach is resilient in the
face of substantial ancillary content and paraphrasing, two common
sources of error in existing content tracking techniques.},
    author = {Creswick, Eugene R. and Fujioka, Emi and Goan, Terrance},
    booktitle = {Proc. of the Second workshop on Uncovering Plagiarism, Authorship, and Software Misuse (PAN)},
    citeulike-article-id = {3089574},
    day = {22},
    editor = {Stein, Benno and Stamatatos, Efstathios and Koppel, Moshe},
    keywords = {information\_retrieval, plagiarism, string\_matching},
    location = {Patras, Greece},
    month = {July},
    pages = {21--25},
    posted-at = {2008-08-05 22:11:19},
    priority = {0},
    title = {Pedigree Tracking in the Face of Ancillary Content},
    year = {2008}
}

@article{raykarEtAl2008:fastLearning,
    abstract = {We consider the problem of learning the ranking function that maximizes a generalization of the Wilcoxon-Mann-Whitney statistic on the training data. Relying on an \$\epsilon\$-accurate approximation for the error-function, we reduce the computational complexity of each iteration of a conjugate gradient algorithm for learning ranking functions from \$\mathcal{O}(m^2)\$, to \$\mathcal{O}(m)\$, where \$m\$ is the number of training samples. Experiments on public benchmarks for ordinal regression and collaborative filtering indicate that the proposed algorithm is as accurate as the best available methods in terms of ranking accuracy, when the algorithms are trained on the same data. However, since it is several orders of magnitude faster than the current state-of-the-art approaches, it is able to leverage much larger training datasets.},
    author = {Raykar, Vikas C. and Duraiswami, Ramani and Krishnapuram, Balaji},
    citeulike-article-id = {2897162},
    citeulike-linkout-0 = {http://csdl2.computer.org/persagen/DLAbsToc.jsp?resourcePath=/dl/trans/tp/\&\#38;toc=comp/trans/tp/2008/07/ttp200807toc.xml\&\#38;DOI=10.1109/TPAMI.2007.70776},
    journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
    month = {July},
    number = {7},
    pages = {1158--1170},
    posted-at = {2008-06-15 23:35:10},
    priority = {2},
    title = {A Fast Algorithm for Learning a Ranking Function from Large-Scale Data Sets},
    url = {http://csdl2.computer.org/persagen/DLAbsToc.jsp?resourcePath=/dl/trans/tp/\&\#38;toc=comp/trans/tp/2008/07/ttp200807toc.xml\&\#38;DOI=10.1109/TPAMI.2007.70776},
    volume = {30},
    year = {2008}
}

@article{ukkonen1995:suffix,
    abstract = {An on-line algorithm is presented for constructing the suffix tree for a given string in time linear in the length of the string. The new algorithm has the desirable property of processing the string symbol by symbol from left to right. It has always the suffix tree for the scanned part of the string ready. The method is developed as a linear-time version of a very simple algorithm for (quadratic size) suffix tries. Regardless of its quadratic worst-case this latter algorithm can be a good...},
    author = {Ukkonen, Esko},
    citeulike-article-id = {707616},
    citeulike-linkout-0 = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.10.751},
    journal = {Algorithmica},
    keywords = {string\_matching, suffix\_trees},
    number = {3},
    pages = {249--260},
    posted-at = {2008-06-08 04:09:56},
    priority = {2},
    title = {On-Line Construction of Suffix Trees},
    url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.10.751},
    volume = {14},
    year = {1995}
}

@inproceedings{kratschEtAl2006:certAlg,
    abstract = {A  certifying algorithm  for a decision problem is an algorithm that provides a certificate with each answer that it produces. The certificate is a piece of evidence that proves that the answer has not been compromised by a bug in the implementation. We give linear-time certifying algorithms for recognition of interval graphs and permutation graphs. Previous algorithms fail to provide supporting evidence when they claim that the input graph is not a member of the class. We show that our certificates of non-membership can be authenticated in  O (\&verbar; V \&verbar;) time.},
    address = {Philadelphia, PA, USA},
    author = {Kratsch, Dieter and McConnell, Ross M. and Mehlhorn, Kurt and Spinrad, Jeremy P.},
    booktitle = {SODA '03: Proc. of the fourteenth annual ACM-SIAM symposium on Discrete algorithms},
    citeulike-article-id = {2859542},
    citeulike-linkout-0 = {http://portal.acm.org/citation.cfm?id=644137},
    isbn = {0-89871-538-5},
    keywords = {algorithms, certifying\_algorithms, software\_engineering, testing},
    location = {Baltimore, Maryland},
    pages = {158--167},
    posted-at = {2008-06-03 17:27:53},
    priority = {4},
    publisher = {Society for Industrial and Applied Mathematics},
    title = {Certifying algorithms for recognizing interval graphs and permutation graphs},
    url = {http://portal.acm.org/citation.cfm?id=644137},
    year = {2003}
}

@inproceedings{eissenEtAl2005:suffixTrees,
    abstract = {In text-based information retrieval, which is the predominant retrieval task at present, several document models have been proposed, such as boolean, probabilistic, or (extended) vec- tor models [Baeza-Yates and Ribeiro-Neto 1999]. Interestingly, the sufﬁx tree document model is usually not discussed in the literature on the subject though it comes along with a property that sets it apart from the other models: It encodes information about word order. The sufﬁx tree docu- ment model owes much of its popularity from the Viv\'{\i}simo search engine, which operationalizes on-the-ﬂy categorization of Internet search results. While the classical document models can be considered as vectors of words, the sufﬁx tree doc- ument model as well as the related similarity measures are graph-based. Both types of document models provide an efﬁcient means to compute document similarities, and, according to various publications, both types of document models work well in practice. However, there is no com- parison between both paradigms that explains the concepts of one in terms of the other, or that contrasts their advantages and disadvantages with respect to certain retrieval tasks. In this paper we start to tackle this gap by shading light on the following questions: (1) How does similarity computation work in the sufﬁx tree document model? (2) Based on the insights of Question 1, is it possible to combine concepts of both document model types within classiﬁcation or categoriza- tion tasks? (3) Which of the document model types is more powerful with respect to unsupervised document classiﬁcation?},
    address = {Graz, Austria},
    author = {Eissen, Sven M. and Stein, Benno and Potthast, Martin},
    booktitle = {Proc. of the 5th Intl. Conf. on Knowledge Management (I-KNOW 05)},
    citeulike-article-id = {2845585},
    howpublished = {Journal of Universal Computer Science},
    keywords = {document\_clustering, information\_retrieval, string\_matching, suffix\_trees},
    month = {July},
    pages = {596--603},
    posted-at = {2008-05-29 18:51:32},
    priority = {2},
    publisher = {Know-Center. ISSN 0948-695x.},
    title = {The Suffix Tree Document Model Revisited},
    year = {2005}
}

@inproceedings{chimDeng2007:newTrees,
    abstract = {In this paper, we propose a new similarity measure to compute the pairwise similarity of text-based documents based on suffix tree document model. By applying the new suffix tree similarity measure in Group-average Agglomerative Hierarchical Clustering (GAHC) algorithm, we developed a new suffix tree document clustering algorithm (NSTC). Experimental results on two standard document clustering benchmark corpus OHSUMED and RCV1 indicate that the new clustering algorithm is a very effective document clustering algorithm. Comparing with the results of traditional word term weight tf-idf similarity measure in the same GAHC algorithm, NSTC achieved an improvement of 51\% on the average of F-measure score. Furthermore, we apply the new clustering algorithm in analyzing the Web documents in online forum communities. A topic oriented clustering algorithm is developed to help people in assessing, classifying and searching the the Web documents in a large forum community.},
    address = {New York, NY, USA},
    author = {Chim, Hung and Deng, Xiaotie},
    booktitle = {WWW '07: Proc. of the 16th intl. conf. on World Wide Web},
    citeulike-article-id = {2845421},
    citeulike-linkout-0 = {http://portal.acm.org/citation.cfm?id=1242590},
    citeulike-linkout-1 = {http://dx.doi.org/10.1145/1242572.1242590},
    doi = {10.1145/1242572.1242590},
    isbn = {978-1-59593-654-7},
    keywords = {string\_matching, suffix\_trees},
    location = {Banff, Alberta, Canada},
    pages = {121--130},
    posted-at = {2008-05-29 18:46:55},
    priority = {2},
    publisher = {ACM},
    title = {A new suffix tree similarity measure for document clustering},
    url = {http://dx.doi.org/10.1145/1242572.1242590},
    year = {2007}
}

@article{monostoriEtAl2002:svector,
    abstract = {Suffix trees are versatile data structures that are used for solving
many string-matching problems. One of the main arguments
against widespread usage of the structure is its space
requirement. This paper describes a new structure called suffix
vector, which is not only better in terms of storage space but
also simpler than the most efficient suffix tree representation
known to date. Alternatives of storage representations are
discussed and a linear-time construction algorithm is also
proposed in this paper. Space requirement of the suffix vector
structure is compared to the space requirement of alternative
suffix tree representations. We also make a theoretical
comparison on the number of operations required to run
algorithms on the suffix vector.},
    address = {Los Alamitos, CA, USA},
    author = {Monostori, Kriszti\'{a}n and Zaslavsky, Arkady and Schmidt, Heinz},
    citeulike-article-id = {2845335},
    citeulike-linkout-0 = {http://portal.acm.org/citation.cfm?id=563857.563820},
    citeulike-linkout-1 = {http://dx.doi.org/10.1145/563857.563820},
    doi = {10.1145/563857.563820},
    journal = {Aust. Comput. Sci. Commun.},
    keywords = {string\_matching},
    number = {1},
    pages = {157--165},
    posted-at = {2008-05-29 18:29:44},
    priority = {2},
    publisher = {IEEE Computer Society Press},
    title = {Suffix vector: space- and time-efficient alternative to suffix trees},
    url = {http://dx.doi.org/10.1145/563857.563820},
    volume = {24},
    year = {2002}
}

@inproceedings{stein2005:fuzzyPrints,
    abstract = {Abstract: This paper introduces a particular form of fuzzy-ﬁngerprints—their construction, their interpretation, and their use in the ﬁeld of information retrieval. Though the concept of ﬁnger- printing in general is not new, the way of using them within a similarity search as described here is: Instead of computing the similarity between two ﬁngerprints in order to access the similarity between the associated objects, simply the event of a ﬁngerprint collision is used for a similarity assessment. The main impact of this approach is the small number of comparisons necessary to conduct a similarity search.},
    address = {Graz, Austria},
    author = {Stein, Benno},
    booktitle = {Proc. of the 5th Intl. Conf. on Knowledge Management (I-KNOW 05)},
    citeulike-article-id = {2845270},
    howpublished = {Journal of Universal Computer Science},
    keywords = {document\_clustering, information\_retrieval, string\_matching},
    month = {July},
    pages = {572--579},
    posted-at = {2008-05-29 18:25:48},
    priority = {2},
    publisher = {Know-Center. ISSN 0948-695x.},
    title = {Fuzzy-Fingerprints for Text-Based Information Retrieval},
    year = {2005}
}

@inproceedings{okanSchilit2008:miningLinks,
    abstract = {Scanning books, magazines, and newspapers has become
a widespread activity because people believe that much of
the worlds information still resides oﬀ-line. In general after
works are scanned they are indexed for search and processed
to add links. This paper describes a new approach to au-
tomatically add links by mining popularly quoted passages.
Our technique connects elements that are semantically rich,
so strong relations are made. Moreover, link targets point
within a work, facilitating navigation. This paper makes
three contributions. We describe a scalable algorithm for
mining repeated word sequences from extremely large text
corpora. Second, we present techniques that ﬁlter and rank
the repeated sequences for quotations. Third, we present a
new user interface for navigating across and within works in
the collection using quotation links. Our system has been
run on a digital library of over 1 million books and has been
used by thousands of people.},
    address = {Pittsburg, Pennsylvania},
    author = {Kolak, Okan and Schilit, Bill N.},
    booktitle = {Proc. of the 19th conf. on Hypertext and hypermedia},
    citeulike-article-id = {2838127},
    day = {19},
    keywords = {data\_mining, document\_linking, ht08, information\_extraction},
    month = {June},
    posted-at = {2008-05-27 20:31:02},
    priority = {2},
    title = {Generating Links by Mining Quotations},
    year = {2008}
}

@article{cohnEtAl1992:activeLearning,
    abstract = {Active learning differs from  ” learning from examples” in that the learning algorithm assumes at least some control over what part of the input domain it receives information about. In some situations, active learning is provably more powerful than learning from examples alone, giving better generalization for a fixed number of training examples.

In this article, we consider the problem of learning a binary concept in the absence of noise. We describe a formalism for active concept learning called selective sampling and show how it may be approximately implemented by a neural network. In selective sampling, a learner receives distribution information from the environment and queries an oracle on parts of the domain it considers  ” useful.” We test our implementation, called an SG-network, on three domains and observe significant improvement in generalization.},
    address = {Hingham, MA, USA},
    author = {Cohn, David and Atlas, Les and Ladner, Richard},
    citeulike-article-id = {1727805},
    citeulike-linkout-0 = {http://portal.acm.org/citation.cfm?id=189489},
    citeulike-linkout-1 = {http://dx.doi.org/10.1023/A:1022673506211},
    doi = {10.1023/A:1022673506211},
    issn = {0885-6125},
    journal = {Mach. Learn.},
    keywords = {active\_learning, ai, pbd},
    month = {May},
    number = {2},
    pages = {201--221},
    posted-at = {2008-05-23 00:20:43},
    priority = {2},
    publisher = {Kluwer Academic Publishers},
    title = {Improving Generalization with Active Learning},
    url = {http://dx.doi.org/10.1023/A:1022673506211},
    volume = {15},
    year = {1994}
}

@article{kidsim1994,
    address = {New York, NY, USA},
    author = {Smith, David C. and Cypher, Allen and Spohrer, Jim},
    citeulike-article-id = {1985889},
    doi = {10.1145/176789.176795},
    journal = {Communications of the ACM},
    keywords = {programming, rules, visual\_programming\_languages},
    number = {7},
    pages = {54--67},
    posted-at = {2008-05-22 19:57:37},
    priority = {2},
    publisher = {ACM Press},
    title = {KidSim: programming agents without a programming language},
    volume = {37},
    year = {1994}
}

@inproceedings{koMyers2006:Barista,
    abstract = {Recent advances in programming environments have focused on improving programmer productivity by utilizing the inherent structure in computer programs. However, because these environments represent code as plain text, it is difficult and sometimes impossible to embed interactive tools, annotations, and alternative views in the code itself. Barista is an implementation framework that enables the creation of such user interfaces by simplifying the implementation of editors that represent code internally as an abstract syntax tree and maintain a corresponding, fully structured visual representation on-screen. Barista also provides designers of editors with a standard text-editing interaction technique that closely mimics that of conventional text editors, overcoming a central usability issue of previous structured code editors.},
    address = {New York, NY, USA},
    author = {Ko, Andrew J. and Myers, Brad A.},
    booktitle = {CHI '06: Proc. of the SIGCHI conf. on Human Factors in computing systems},
    citeulike-article-id = {691586},
    citeulike-linkout-0 = {http://portal.acm.org/citation.cfm?id=1124772.1124831},
    citeulike-linkout-1 = {http://dx.doi.org/10.1145/1124772.1124831},
    doi = {10.1145/1124772.1124831},
    isbn = {1-59593-372-7},
    keywords = {editors, programming, tools},
    location = {Montr\'{e}al, Qu\'{e}bec, Canada},
    pages = {387--396},
    posted-at = {2008-05-22 19:03:56},
    priority = {2},
    publisher = {ACM},
    title = {Barista: An implementation framework for enabling new tools, interaction techniques and views in code editors},
    url = {http://dx.doi.org/10.1145/1124772.1124831},
    year = {2006}
}

@inproceedings{koEtAl2005:editing,
    abstract = {A detailed study of Java programmers' text editing found that the full flexibility of unstructured text was not utilized for the vast majority of programmers' character-level edits. Rather, programmers used a small set of editing patterns to achieve their modifications, which accounted for all of the edits observed in the study. About two-thirds of the edits were of name and list structures and most edits preserved structure except for temporary omissions of delimiters. These findings inform the design of a new class of more flexible structured program editors that may avoid well-known usability problems of traditional structured editors, while providing more sophisticated support such as more universal code completion and smarter copy and paste.},
    address = {New York, NY, USA},
    author = {Ko, Andrew J. and Aung, Htet H. and Myers, Brad A.},
    booktitle = {CHI '05: CHI '05 extended abstracts on Human factors in computing systems},
    citeulike-article-id = {984681},
    citeulike-linkout-0 = {http://portal.acm.org/citation.cfm?id=1056808.1056965},
    citeulike-linkout-1 = {http://dx.doi.org/10.1145/1056808.1056965},
    doi = {10.1145/1056808.1056965},
    isbn = {1-59593-002-7},
    keywords = {debugging, programming},
    location = {Portland, OR, USA},
    pages = {1557--1560},
    posted-at = {2008-05-22 19:01:59},
    priority = {2},
    publisher = {ACM},
    title = {Design requirements for more flexible structured editors from a study of programmers' text editing},
    url = {http://dx.doi.org/10.1145/1056808.1056965},
    year = {2005}
}

@inproceedings{koMyers2003:errormodel,
    address = {Washington, DC, USA},
    author = {Ko, A. J. and Myers, B. A.},
    booktitle = {HCC '03: Proc. of the 2003 IEEE Symposium on Human Centric Computing Languages and Environments},
    citeulike-article-id = {2823519},
    citeulike-linkout-0 = {http://portal.acm.org/citation.cfm?id=1153917.1153960},
    isbn = {0780382250},
    keywords = {debugging, programming},
    pages = {7--14},
    posted-at = {2008-05-22 18:59:01},
    priority = {2},
    publisher = {IEEE Computer Society},
    title = {Development and evaluation of a model of programming errors},
    url = {http://portal.acm.org/citation.cfm?id=1153917.1153960},
    year = {2003}
}

@book{cypherEtAl1993:watch,
    address = {Cambridge, MA, USA},
    citeulike-article-id = {2753332},
    citeulike-linkout-0 = {http://portal.acm.org/citation.cfm?id=168080},
    editor = {Cypher, Allen and Halbert, Daniel C. and Kurlander, David and Lieberman, Henry and Maulsby, David and Myers, Brad A. and Turransky, Alan},
    isbn = {0-262-03213-9},
    keywords = {book, pbd},
    posted-at = {2008-05-22 00:53:01},
    priority = {2},
    publisher = {MIT Press},
    title = {Watch what I do: programming by demonstration},
    url = {http://portal.acm.org/citation.cfm?id=168080},
    year = {1993}
}

@inproceedings{perroneRepenning1998:rules,
    address = {Washington, DC, USA},
    author = {Perrone, Corrina and Repenning, Alexander},
    booktitle = {VL '98: Proc. of the IEEE Symposium on Visual Languages},
    citeulike-article-id = {2821522},
    citeulike-linkout-0 = {http://portal.acm.org/citation.cfm?id=834478},
    isbn = {0818687126},
    keywords = {rules, visual\_programming\_languages},
    posted-at = {2008-05-22 00:49:03},
    priority = {2},
    publisher = {IEEE Computer Society},
    title = {Graphical Rewrite Rule Analogies: Avoiding the Inherit or Copy \& Paste Reuse Dilemma},
    url = {http://portal.acm.org/citation.cfm?id=834478},
    year = {1998}
}

@incollection{grice1975:logic,
    author = {Grice, H. P.},
    booktitle = {Syntax and semantics},
    citeulike-article-id = {2359703},
    editor = {Cole, Peter and Morgan, Jerry L.},
    keywords = {conversation, logic},
    posted-at = {2008-05-16 19:23:03},
    priority = {2},
    publisher = {New York: Academic Press},
    title = {Logic and conversation},
    volume = {3},
    year = {1975}
}

@article{altermanEtAl1998:pragmaticAction,
    author = {Alterman, Richard and Zito-Wolf, Roland and Carpenter, Tamitha},
    citeulike-article-id = {2805674},
    journal = {Cognitive Science},
    keywords = {shai},
    number = {1},
    pages = {53--106},
    posted-at = {2008-05-16 19:13:45},
    priority = {2},
    title = {Pragmatic Action},
    volume = {22},
    year = {1998}
}

@phdthesis{carpenter1999:phd,
    author = {Carpenter, Tamitha},
    citeulike-article-id = {2805670},
    institution = {Brandeis University},
    keywords = {nlp, shai},
    posted-at = {2008-05-16 19:12:27},
    priority = {2},
    title = {Reading to Plan; Planning to Read},
    year = {1999}
}

@inbook{carpenter2000:r2d2,
    author = {Carpenter, Tamitha},
    booktitle = {Working Notes of the AAAI Spring Symposium titled  ” My Dinner with R2D2: Natural Dialogs with Practical Robotic Devices"},
    citeulike-article-id = {2805667},
    keywords = {nlp, shai},
    posted-at = {2008-05-16 19:10:03},
    priority = {2},
    title = {Letting R2D2 Read Instructions and Ask for Help},
    year = {2000}
}

@inproceedings{carpenterEtAl2000:dl,
    author = {Carpenter, Tamitha and Fu, Daniel and Michalak, Phillip and Spencer, Laurie and Iorizzo, Luciano},
    booktitle = {In Proc. of the Industry/Interservice, Training, Simulation \& Education Conf.},
    citeulike-article-id = {2805660},
    keywords = {learning, shai},
    posted-at = {2008-05-16 19:08:32},
    priority = {2},
    title = {A
Constructivist Approach to Distance Learning for Counterterrorist
Intelligence Analysis},
    year = {2000}
}

@inproceedings{jonesCarpenter2002:net,
    address = {Las Vegas},
    author = {Jones, Lynn W. and Carpenter, Tamitha},
    booktitle = {Proc. of the 2002 Intl. Conf. on Security and Management},
    citeulike-article-id = {2805656},
    keywords = {security, shai},
    posted-at = {2008-05-16 19:05:41},
    priority = {2},
    title = {Towards Decentralized Network Management
and Reliability},
    year = {2002}
}

@inproceedings{woods2000:indexing,
    author = {Woods, William A.},
    booktitle = {Proc. of AAAI},
    citeulike-article-id = {2802772},
    keywords = {nlp},
    pages = {1180--1185},
    posted-at = {2008-05-15 23:26:27},
    priority = {2},
    publisher = {MIT Press},
    title = {Conceptual Indexing: Practical Large-Scale AI for Efficient Information Extraction},
    year = {2000}
}

@inproceedings{roth1998:learning,
    address = {Menlo Park, CA, USA},
    author = {Roth, Dan},
    booktitle = {AAAI '98/IAAI '98: Proc. of the fifteenth national/tenth conf. on Artificial intelligence/Innovative applications of artificial intelligence},
    citeulike-article-id = {2802763},
    citeulike-linkout-0 = {http://portal.acm.org/citation.cfm?id=295894},
    isbn = {0262510987},
    keywords = {disambiguation, nlp},
    pages = {806--813},
    posted-at = {2008-05-15 23:20:31},
    priority = {2},
    publisher = {American Association for Artificial Intelligence},
    title = {Learning to resolve natural language ambiguities: a unified approach},
    url = {http://portal.acm.org/citation.cfm?id=295894},
    year = {1998}
}

@inproceedings{yatesEtzioni:resolver,
    address = {Rochester, NY},
    author = {Yates, Alexander and Etzioni, Oren},
    booktitle = {Proc. of NAACL HLT},
    citeulike-article-id = {2802661},
    month = {April},
    pages = {121--130},
    posted-at = {2008-05-15 21:48:17},
    priority = {2},
    title = {Unsupervised Resolution of Objects and Relations on the Web},
    year = {2007}
}

@inproceedings{agichtein2005:scaling,
    abstract = {Information extraction and text mining applications are just beginning to tap the immense amounts of
valuable textual information available online. In order to extract information from millions, and in some
cases, billions of documents, different solutions to scalability emerged. We review key approaches for
scaling up information extraction, including using general-purpose search engines as well as indexing
techniques specialized for information extraction applications. Scalable information extraction is an
active area of research, and we highlight some of the opportunities and challenges in this area that are
relevant to the database community.},
    author = {Agichtein, Eugene},
    booktitle = {IEEE Data Engineering Bulletin},
    citeulike-article-id = {2789987},
    keywords = {information\_extraction, nlp, scalability},
    posted-at = {2008-05-12 17:30:45},
    priority = {2},
    title = {Scaling Information Extraction to Large Document Collections},
    year = {2005}
}

@inproceedings{bankoEtAl2007:OIE,
    abstract = {Traditionally, Information Extraction (IE) has fo- cused on satisfying precise, narrow, pre-speciﬁed requests from small homogeneous corpora (e.g., extract the location and time of seminars from a set of announcements). Shifting to a new domain requires the user to name the target relations and to manually create new extraction rules or hand-tag new training examples. This manual labor scales linearly with the number of target relations. This paper introduces Open IE (OIE), a new ex- traction paradigm where the system makes a single data-driven pass over its corpus and extracts a large set of relational tuples without requiring any human input. The paper also introduces T EXT RUNNER, a fully implemented, highly scalable OIE system where the tuples are assigned a probability and indexed to support efﬁcient extraction and explo- ration via user queries. We report on experiments over a 9,000,000 Web page corpus that compare T EXT RUNNER with K NOW I TA LL, a state-of-the-art Web IE system. T EXT RUNNER achieves an error reduction of 33\% on a comparable set of extractions. Furthermore, in the amount of time it takes K NOW I TA LL to per- form extraction for a handful of pre-speciﬁed re- lations, T EXT RUNNER extracts a far broader set of facts reﬂecting orders of magnitude more rela- tions, discovered on the ﬂy. We report statistics on T EXT RUNNER's 11,000,000 highest probability tuples, and show that they contain over 1,000,000 concrete facts and over 6,500,000 more abstract as- sertions.},
    author = {Banko, Michele and Cafarella, Michael J. and Soderland, Stephen and Broadhead, Matt and Etzioni, Oren},
    booktitle = {In the Proc. of the 20th Intl. Joint Conf. on Artificial Intelligence},
    citeulike-article-id = {2773682},
    keywords = {information\_retrieval, nlp},
    month = {January},
    pages = {2670--2676},
    posted-at = {2008-05-08 23:04:09},
    priority = {2},
    title = {Open Information Extraction from the Web},
    year = {2007}
}

@inproceedings{kudoMatsumoto2003:kernel,
    address = {Morristown, NJ, USA},
    author = {Kudo, Taku and Matsumoto, Yuji},
    booktitle = {ACL '03: Proc. of the 41st Annual Meeting on Association for Computational Linguistics},
    citeulike-article-id = {2767200},
    citeulike-linkout-0 = {http://portal.acm.org/citation.cfm?id=1075096.1075100},
    citeulike-linkout-1 = {http://dx.doi.org/10.3115/1075096.1075100},
    doi = {10.3115/1075096.1075100},
    keywords = {nlp},
    pages = {24--31},
    posted-at = {2008-05-07 20:31:33},
    priority = {2},
    publisher = {Association for Computational Linguistics},
    title = {Fast methods for kernel-based text analysis},
    url = {http://dx.doi.org/10.3115/1075096.1075100},
    year = {2003}
}

@inproceedings{isozakiKazawa2002:NamedEntitySVM,
    abstract = {Named Entity (NE) recognition is a task in which proper nouns and numerical information are extracted from documents and are classified into categories such as person, organization, and date. It is a key technology of Information Extraction and Open-Domain Question Answering. First, we show that an NE recognizer based on Support Vector Machines (SVMs) gives better scores than conventional systems. However, off-the-shelf SVM classifiers are too inefficient for this task. Therefore, we present a method that makes the system substantially faster. This approach can also be applied to other similar tasks such as chunking and part-of-speech tagging. We also present an SVM-based feature selection method and an efficient training method.},
    address = {Morristown, NJ, USA},
    author = {Isozaki, Hideki and Kazawa, Hideto},
    booktitle = {Proc. of the 19th intl. conf. on Computational linguistics},
    citeulike-article-id = {2767196},
    citeulike-linkout-0 = {http://portal.acm.org/citation.cfm?id=1072228.1072282},
    citeulike-linkout-1 = {http://dx.doi.org/10.3115/1072228.1072282},
    doi = {10.3115/1072228.1072282},
    keywords = {nlp},
    location = {Taipei, Taiwan},
    pages = {1--7},
    posted-at = {2008-05-07 20:28:59},
    priority = {2},
    publisher = {Association for Computational Linguistics},
    title = {Efficient support vector classifiers for named entity recognition},
    url = {http://dx.doi.org/10.3115/1072228.1072282},
    year = {2002}
}

@inproceedings{gildeaJurafsky2000:semanticRoles,
    address = {Morristown, NJ, USA},
    author = {Gildea, Daniel and Jurafsky, Daniel},
    booktitle = {ACL '00: Proc. of the 38th Annual Meeting on Association for Computational Linguistics},
    citeulike-article-id = {2767191},
    citeulike-linkout-0 = {http://portal.acm.org/citation.cfm?id=1075218.1075283},
    citeulike-linkout-1 = {http://dx.doi.org/10.3115/1075218.1075283},
    doi = {10.3115/1075218.1075283},
    keywords = {nlp},
    pages = {512--520},
    posted-at = {2008-05-07 20:25:58},
    priority = {2},
    publisher = {Association for Computational Linguistics},
    title = {Automatic labeling of semantic roles},
    url = {http://dx.doi.org/10.3115/1075218.1075283},
    year = {2000}
}

@inproceedings{bakerEtAl1998:frameNet,
    address = {Morristown, NJ, USA},
    author = {Baker, Collin F. and Fillmore, Charles J. and Lowe, John B.},
    booktitle = {Proc. of the 17th intl. conf. on Computational linguistics},
    citeulike-article-id = {2767182},
    citeulike-linkout-0 = {http://portal.acm.org/citation.cfm?id=980860},
    citeulike-linkout-1 = {http://dx.doi.org/10.3115/980451.980860},
    doi = {10.3115/980451.980860},
    keywords = {nlp},
    pages = {86--90},
    posted-at = {2008-05-07 20:23:45},
    priority = {2},
    publisher = {Association for Computational Linguistics},
    title = {The Berkeley FrameNet Project},
    url = {http://dx.doi.org/10.3115/980451.980860},
    year = {1998}
}

@inproceedings{zhai1997:nouns,
    address = {San Francisco, CA, USA},
    author = {Zhai, Chengxiang},
    booktitle = {Proc. of the fifth conf. on Applied natural language processing},
    citeulike-article-id = {2767180},
    citeulike-linkout-0 = {http://portal.acm.org/citation.cfm?id=974603},
    citeulike-linkout-1 = {http://dx.doi.org/10.3115/974557.974603},
    doi = {10.3115/974557.974603},
    keywords = {nlp},
    pages = {312--319},
    posted-at = {2008-05-07 20:16:49},
    priority = {2},
    publisher = {Morgan Kaufmann Publishers Inc.},
    title = {Fast statistical parsing of noun phrases for document indexing},
    url = {http://dx.doi.org/10.3115/974557.974603},
    year = {1997}
}

@article{goanBroadhead2004:skm,
    abstract = {The insider threat has proved a tough nut to crack. Previous work in this area has been dominated by efforts to model normal user behavior through statistical measures and then detect substantial anomalies. Unfortunately, while these methods have shown some ability in the detection of masqueraders, broader applications have proved ineffectual due to extremely high false alarm rates. In this paper, the authors describe an alternative approach, Stochastic Long-String Analysis with Feedback (SL-SAFE), that can achieve high levels of accuracy in detecting the unauthorized access and distribution of sensitive/proprietary information by insiders -- the single most costly type of computer crime. SL-SAFE succeeds in this task by means of a stochastic sampling of bottlenecks through which information must flow to be useful to the malicious insider. Further, it achieves a low (and shrinking) false alarm rate by validating its suspicions through public information sources and eliciting feedback from the information owner.},
    author = {Goan, Terrance and Broadhead, Matthew},
    booktitle = {Worshop on Secure Knowledge Management (SKM)},
    citeulike-article-id = {2730766},
    citeulike-linkout-0 = {http://stinet.dtic.mil/oai/oai?verb=getRecord\&\#38;metadataPrefix=html\&\#38;identifier=ADA454839},
    keywords = {security, shai},
    posted-at = {2008-04-28 19:47:59},
    priority = {2},
    title = {Detecting the Misappropriation of Sensitive Information through Bottleneck Monitoring},
    url = {http://stinet.dtic.mil/oai/oai?verb=getRecord\&\#38;metadataPrefix=html\&\#38;identifier=ADA454839},
    year = {2004}
}

@inproceedings{goanAndMayk2005,
    abstract = {Command and Control for coordinated response to domestic terrorist attack will require the ability of Federal, state, and local agencies to maintain awareness of the status, capabilities, requirements, response plans, and C2 procedures, etc. of the other collaborating organizations. While progress is being made in improving information sharing, the TOPOFF exercises have demonstrated that organizations still lack any substantial ability to coordinate responses to large scale events that involve dozens of local, state, and Federal organizations. In this paper we describe progress made in the development of new information access services that provide for improved situation awareness. We have strived to develop a solution that enables User Defined Operational Picture (UDOP) functionality while respecting the unique information management practices of the collaborating Homeland Security organizations. Our system concept, Vista, employs an adaptive machine learning paradigm that supports a new form of context-sensitive information access, monitoring, and alerting that fills substantial gaps in existing Crisis Information Management System technologies. Experimental results demonstrate very substantial improvements in information access efficiency and provide strong evidence for the feasibility of the overall concept.},
    author = {Goan, Terrance and Mayk, Israel},
    booktitle = {Proc. of the 10th Intl.  Command and Control Research and Technology Symposium (ICCRTS)},
    citeulike-article-id = {2730750},
    keywords = {security, shai},
    posted-at = {2008-04-28 19:44:55},
    priority = {2},
    title = {Improving Information Exchange and Coordination Amongst Homeland Security Organizations},
    year = {2005}
}

@incollection{goanEtAl2006:provenance,
    abstract = {In recent years, it has become clear that our ability to create vast information assets far outstrips our ability to exploit and protect them. The Intelligence Community faces particularly significant information management challenges as they seek to: improve information awareness amongst analysts; improve the reliability of intelligence; safely share information with warfighters and allies; and root out malicious insiders. One means to mitigating these challenges is to provide reliable knowledge of the provenance (i.e., lineage) of documents. This knowledge would allow, for instance, analysts to identify source information underpinning an intelligence report.},
    author = {Goan, Terrance and Fujioka, Emi and Kaneshiro, Ryan and Gasch, Lynn},
    citeulike-article-id = {2730742},
    citeulike-linkout-0 = {http://dx.doi.org/10.1007/11760146\_93},
    doi = {10.1007/11760146\_93},
    journal = {Intelligence and Security Informatics},
    keywords = {document\_linking, shai},
    pages = {692--693},
    posted-at = {2008-04-28 19:40:57},
    priority = {2},
    title = {Identifying Information Provenance in Support of Intelligence Analysis, Sharing, and Protection},
    url = {http://dx.doi.org/10.1007/11760146\_93},
    year = {2006}
}

@inproceedings{monteith2001:TIE,
    address = {Washington, DC, USA},
    author = {Monteith, E.},
    booktitle = {ACSAC '01: Proc. of the 17th Annual Computer Security Applications Conf.},
    citeulike-article-id = {2730725},
    citeulike-linkout-0 = {http://portal.acm.org/citation.cfm?id=872016.872142},
    isbn = {0769514057},
    keywords = {security},
    posted-at = {2008-04-28 19:36:00},
    priority = {2},
    publisher = {IEEE Computer Society},
    title = {Genoa TIE, Advanced Boundary Controller Experiment},
    url = {http://portal.acm.org/citation.cfm?id=872016.872142},
    year = {2001}
}

@incollection{bettiniEtAl2004,
    abstract = {In a database system, authorization-based access-control is generally the first line of defense, preventing unauthorized accesses to secret or sensitive data. However, this mechanism is susceptible to security breaches due to improper authorization (e.g., the general public is mistakenly granted access to a copy of sensitive data) and cannot block insider attacks (an authorized user accidentally or intentionally discloses secrets to outsiders). Supplementary to access-control, the release-control mechanism is to check all the outgoing documents for any leak of secret or sensitive information. This paper reports preliminary results on a specific release-control task, namely, how to deal with sensitive associations that need to be restricted from releasing. A sensitive association refers to a pair of values whose connection involves some secrets. The disclosure of such a pair may reveal the secretive connection and therefore should be controlled. The release control of sensitive associations is a very challenging and long term research problem. This paper introduces techniques to identify and represent sensitive associations hidden in a database.},
    author = {Bettini, Claudio and Wang, Xiaoyang S. and Jajodia, Sushil},
    citeulike-article-id = {2730692},
    citeulike-linkout-0 = {http://www.springerlink.com/content/qdv2un1c8kjjuxu0},
    journal = {Secure Data Management},
    keywords = {security},
    pages = {187--201},
    posted-at = {2008-04-28 19:24:20},
    priority = {2},
    title = {Identifying Sensitive Associations in Databases for Release Control},
    url = {http://www.springerlink.com/content/qdv2un1c8kjjuxu0},
    year = {2004}
}

@inproceedings{bettiniEtAl2003,
    abstract = {Controlled release of information from an organization is becoming important from various considerations: privacy, competitive information protection, strategic data control, and more. In most organizations, data protection is afforded only by using access control. However, it can be argued that access control suffers from at least two problems. First, effective access control assumes a perfect categorization of information ("who can access what"), which is increasingly difficult in a complex...},
    address = {Lausanne, Switzerland},
    author = {Bettini, Claudio and Wang, Sean X. and Jajodia, Sushil},
    booktitle = {Sixth IFIP  TC-11 WG 11.5 Working Conf. on Integrity and Internal Control in Information Systems},
    citeulike-article-id = {2730671},
    citeulike-linkout-0 = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.2.5363},
    keywords = {security},
    month = {November},
    posted-at = {2008-04-28 19:14:40},
    priority = {2},
    title = {A Learning-Based Approach to Information Release Control},
    url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.2.5363},
    year = {2003}
}

@techreport{dwyer2005,
    address = {http://www.mitre.org/news/digest/defense\_intelligence/08\_05/di\_xbis.html},
    author = {Dwyer, Shari},
    citeulike-article-id = {2730661},
    howpublished = {MITRE Digest},
    keywords = {security},
    posted-at = {2008-04-28 19:11:11},
    priority = {2},
    title = {Need-To-Know vs. Need-To-Share},
    year = {2005}
}

@techreport{reedEtAl2005,
    abstract = {This report documents research efforts for the FY03 and FY04 Mission Oriented Investigation and Experimentation (MOIE) project "Security Guards for the Future Web." Traditionally, computer security guards have been used to control what information flows between security domains. Guard technology needs to keep pace with the evolving Web environment. The authors conjectured that a family of security guard services would be needed to provide the full range of functionality necessary to support the future Web. They structured their research into three segments: the browser-based environment, the Web Services environment and the Semantic Web. Their objectives were to investigate how the content and format of data would change in the various Web environments and how mechanisms designed to enforce cross-domain information sharing rules would have to evolve to match that changing content. Their research for the browser-based environment included a requirements analysis and an analysis of alternative architectures for cross-domain information exchanges. Their research for the Web Services and Semantic Web environments focused on two aspects of guards: the type of data that the guard can transfer and the method used to check the content of items. They decided to focus their Web Services and Semantic Web research on highly structured XML data and guards that would automatically check the content using machine-interpretable information sharing rules. They chose to explore guard designs that supported a wide range of security policies and the ability to easily replace or update the information sharing rules. Their approach entailed three steps: (1) creating an operational scenario for use in their experiment, (2) building a prototype for experimentation, and (3) conducting tests with the prototype using the generated scenario and capturing the lessons learned.},
    address = {http://www.mitre-corporation.org/work/tech\_papers/tech\_papers\_05/05\_0166/index.html},
    author = {Reed, Nancy and Bryson, Dave and Garriss, James and Gosnell, Steve and Heaton, Brook and Huber, Gary and Jacobs, David and Pulvermacher, Mary and Semy, Salim and Smith, Chad and Standard, John},
    citeulike-article-id = {2730653},
    howpublished = {MITRE Technical Report},
    keywords = {security},
    posted-at = {2008-04-28 19:08:29},
    priority = {2},
    title = {Security Guards for the Future Web},
    year = {2005}
}

@electronic{purifile,
    citeulike-article-id = {2724648},
    howpublished = {Website: http://www.purifile.com},
    keywords = {security},
    posted-at = {2008-04-27 21:17:41},
    priority = {2},
    title = {{PuriFile}},
    year = {2008}
}

@misc{wilks97:grammar,
    abstract = {This paper describes two experiments: one exploring the amount of information relevant to
sense disambiguation contained in the part-of-speech field of entries in a Machine Readable
Dictionary (MRD); the other, more practical, experiment attempts sense disambiguation
of all content words in a text assigning MRD homographs as sense tags using only partof
-speech information. We have implemented a simple sense tagger which successfully tags
94\% of words using this method. A plan to extend this...},
    author = {Wilks, Y. and Stevenson, M.},
    citeulike-article-id = {2719359},
    citeulike-linkout-0 = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.52.223},
    keywords = {word-sense-dissambiguation},
    posted-at = {2008-04-25 20:54:43},
    priority = {2},
    title = {The grammar of sense: using part-of-speech tags as a first step in semantic disambiguation},
    url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.52.223},
    year = {1997}
}

@article{porter1980:stemmer,
    abstract = {http://tartarus.org/\~{}martin/PorterStemmer/def.txt},
    address = {San Francisco, CA, USA},
    author = {Porter, M. F.},
    citeulike-article-id = {220516},
    citeulike-linkout-0 = {http://portal.acm.org/citation.cfm?id=275705},
    comment = {Full text:
http://tartarus.org/\~{}martin/PorterStemmer/def.txt},
    isbn = {1-55860-454-5},
    journal = {Program},
    keywords = {string\_matching},
    month = {July},
    number = {3},
    pages = {130--137},
    posted-at = {2008-04-25 20:10:44},
    priority = {2},
    publisher = {Morgan Kaufmann Publishers Inc.},
    title = {An algorithm for suffix stripping},
    url = {http://portal.acm.org/citation.cfm?id=275705},
    volume = {14},
    year = {1980}
}

@book{manningSchutze:nlp,
    abstract = {"Statistical natural-language processing is, in my estimation, one of the most
fast-moving and exciting areas of computer science these days. Anyone who
wants to learn this field would be well advised to get this book. For that
matter, the same goes for anyone who is already in the field. I know that it
is going to be one of the most well-thumbed books on my bookshelf." -- Eugene
Charniak, Department of Computer Science, Brown University

Statistical approaches to processing natural language text have become
dominant in recent years. This foundational text is the first comprehensive
introduction to statistical natural language processing (NLP) to appear. The
book contains all the theory and algorithms needed for building NLP tools. It
provides broad but rigorous coverage of mathematical and linguistic
foundations, as well as detailed discussion of statistical methods, allowing
students and researchers to construct their own implementations. The book
covers collocation finding, word sense disambiguation, probabilistic parsing,
information retrieval, and other applications.

More on this book},
    author = {Manning, Christopher D. and Schuetze, Hinrich},
    citeulike-article-id = {105906},
    citeulike-linkout-0 = {http://www.amazon.ca/exec/obidos/redirect?tag=citeulike09-20\&amp;path=ASIN/0262133601},
    citeulike-linkout-1 = {http://www.amazon.de/exec/obidos/redirect?tag=citeulike01-21\&amp;path=ASIN/0262133601},
    citeulike-linkout-2 = {http://www.amazon.fr/exec/obidos/redirect?tag=citeulike06-21\&amp;path=ASIN/0262133601},
    citeulike-linkout-3 = {http://www.amazon.jp/exec/obidos/ASIN/0262133601},
    citeulike-linkout-4 = {http://www.amazon.co.uk/exec/obidos/ASIN/0262133601/citeulike00-21},
    citeulike-linkout-5 = {http://www.amazon.com/exec/obidos/redirect?tag=citeulike07-20\&path=ASIN/0262133601},
    citeulike-linkout-6 = {http://www.worldcat.org/isbn/0262133601},
    citeulike-linkout-7 = {http://books.google.com/books?vid=ISBN0262133601},
    citeulike-linkout-8 = {http://www.amazon.com/gp/search?keywords=0262133601\&index=books\&linkCode=qs},
    citeulike-linkout-9 = {http://www.librarything.com/isbn/0262133601},
    day = {18},
    edition = {1},
    howpublished = {Hardcover},
    isbn = {0262133601},
    keywords = {book, nlp},
    month = {June},
    posted-at = {2008-04-25 19:52:17},
    priority = {2},
    publisher = {The MIT Press},
    title = {Foundations of Statistical Natural Language Processing},
    url = {http://www.amazon.com/exec/obidos/redirect?tag=citeulike07-20\&path=ASIN/0262133601},
    year = {1999}
}

@article{grubbs,
    author = {Grubbs, Frank E.},
    citeulike-article-id = {2693766},
    journal = {Technometrics},
    keywords = {statistics},
    month = {February},
    number = {1},
    pages = {1--21},
    posted-at = {2008-04-20 23:54:37},
    priority = {2},
    title = {Procedures for Detecting Outlying Observations in Samples},
    volume = {11},
    year = {1969}
}

@article{wilburLipman:rapidSequence,
    abstract = {With the development of large data banks of protein and nucleic acid sequences, the need for efficient methods of searching such banks for sequences similar to a given sequence has become evident. We present an algorithm for the global comparison of sequences based on matching k-tuples of sequence elements for a fixed k. The method results in substantial reduction in the time required to search a data bank when compared with prior techniques of similarity analysis, with minimal loss in sensitivity. The algorithm has also been adapted, in a separate implementation, to produce rigorous sequence alignments. Currently, using the DEC KL-10 system, we can compare all sequences in the entire Protein Data Bank of the National Biomedical Research Foundation with a 350-residue query sequence in less than 3 min and carry out a similar analysis with a 500-base query sequence against all eukaryotic sequences in the Los Alamos Nucleic Acid Data Base in less than 2 min. 10.1073/pnas.80.3.726},
    author = {Wilbur, W. J. and Lipman, David J.},
    citeulike-article-id = {1574178},
    citeulike-linkout-0 = {http://dx.doi.org/10.1073/pnas.80.3.726},
    citeulike-linkout-1 = {http://www.pnas.org/cgi/content/abstract/80/3/726},
    citeulike-linkout-2 = {http://view.ncbi.nlm.nih.gov/pubmed/6572363},
    citeulike-linkout-3 = {http://www.hubmed.org/display.cgi?uids=6572363},
    day = {1},
    doi = {10.1073/pnas.80.3.726},
    journal = {PNAS},
    keywords = {string\_matching},
    month = {February},
    number = {3},
    pages = {726--730},
    posted-at = {2008-04-20 22:19:13},
    priority = {2},
    title = {Rapid Similarity Searches of Nucleic Acid and Protein Data Banks},
    url = {http://dx.doi.org/10.1073/pnas.80.3.726},
    volume = {80},
    year = {1983}
}

@article{Maier1978:seqComplexity,
    address = {New York, NY, USA},
    author = {Maier, David},
    citeulike-article-id = {2693625},
    citeulike-linkout-0 = {http://portal.acm.org/citation.cfm?id=322075},
    citeulike-linkout-1 = {http://dx.doi.org/10.1145/322063.322075},
    doi = {10.1145/322063.322075},
    issn = {0004-5411},
    journal = {J. ACM},
    keywords = {complexity},
    month = {April},
    number = {2},
    pages = {322--336},
    posted-at = {2008-04-20 22:00:28},
    priority = {2},
    publisher = {ACM},
    title = {The Complexity of Some Problems on Subsequences and Supersequences},
    url = {http://dx.doi.org/10.1145/322063.322075},
    volume = {25},
    year = {1978}
}

@book{gareyJohnson:intract,
    abstract = {{This book's introduction features a humorous story of a man with a line of people behind him, who explains to his boss, "I can't find an efficient algorithm, but neither can all these famous people." This man illustrates an important quality of a class of problems, namely, the NP-complete problems: if you can prove that a problem is in this class, then it has no known polynomial-time solution that is guaranteed to work in general. This quality implies that the problem is difficult to deal with in practice.<p> The focus of this book is to teach the reader how to identify, deal with, and understand the essence of NP-complete problems; <I>Computers and Intractability</I> does all of those things effectively. In a readable yet mathematically rigorous manner, the book covers topics such as how to prove that a given problem is NP-complete and how to cope with NP-complete problems. (There is even a chapter on advanced topics, with numerous references.) <I>Computers and Intractability</I> also contains a list of more than 300 problems--most of which are known to be NP-complete--with comments and references.}},
    author = {Garey, M. R. and Johnson, D. S.},
    citeulike-article-id = {574128},
    citeulike-linkout-0 = {http://www.amazon.ca/exec/obidos/redirect?tag=citeulike09-20\&amp;path=ASIN/0716710455},
    citeulike-linkout-1 = {http://www.amazon.de/exec/obidos/redirect?tag=citeulike01-21\&amp;path=ASIN/0716710455},
    citeulike-linkout-2 = {http://www.amazon.fr/exec/obidos/redirect?tag=citeulike06-21\&amp;path=ASIN/0716710455},
    citeulike-linkout-3 = {http://www.amazon.jp/exec/obidos/ASIN/0716710455},
    citeulike-linkout-4 = {http://www.amazon.co.uk/exec/obidos/ASIN/0716710455/citeulike00-21},
    citeulike-linkout-5 = {http://www.amazon.com/exec/obidos/redirect?tag=citeulike07-20\&path=ASIN/0716710455},
    citeulike-linkout-6 = {http://www.worldcat.org/isbn/0716710455},
    citeulike-linkout-7 = {http://books.google.com/books?vid=ISBN0716710455},
    citeulike-linkout-8 = {http://www.amazon.com/gp/search?keywords=0716710455\&index=books\&linkCode=qs},
    citeulike-linkout-9 = {http://www.librarything.com/isbn/0716710455},
    day = {15},
    howpublished = {Paperback},
    isbn = {0716710455},
    keywords = {book, complexity, pvnp},
    month = {January},
    posted-at = {2008-04-20 21:51:25},
    priority = {0},
    publisher = {W. H. Freeman},
    title = {Computers and Intractability: A Guide to the Theory of NP-Completeness (Series of Books in the Mathematical Sciences)},
    url = {http://www.amazon.com/exec/obidos/redirect?tag=citeulike07-20\&path=ASIN/0716710455},
    year = {1979}
}

@inproceedings{eppsteinEtAl1991:efficientSeq,
    abstract = {: We consider new algorithms for the solution of many dynamic programming recurrences for sequence comparison and for RNA secondary structure prediction. The techniques upon which the algorithms are based e\#ectively exploit the physical constraints of the problem to derive more e\#cient methods for sequence analysis. 1. INTRODUCTION In this paper we consider algorithms for two problems in sequence analysis. The first problem is sequence alignment, and the second is the prediction of RNA...},
    author = {Eppstein, David and Galil, Zvi and Giancarlo, Raffaele and Italiano, Guiseppe F.},
    booktitle = {SEQS: Sequences '91},
    citeulike-article-id = {2693615},
    citeulike-linkout-0 = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.39.8503},
    keywords = {string\_matching},
    posted-at = {2008-04-20 21:46:58},
    priority = {2},
    title = {Efficient Algorithms for Sequence Analysis},
    url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.39.8503},
    year = {1991}
}

@inproceedings{jagadishEtAl1995:sim,
    address = {New York, NY, USA},
    author = {Jagadish, H. V. and Mendelzon, Alberto O. and Milo, Tova},
    booktitle = {PODS '95: Proc. of the fourteenth ACM SIGACT-SIGMOD-SIGART symposium on Principles of database systems},
    citeulike-article-id = {2693593},
    citeulike-linkout-0 = {http://portal.acm.org/citation.cfm?id=212444},
    citeulike-linkout-1 = {http://dx.doi.org/10.1145/212433.212444},
    doi = {10.1145/212433.212444},
    isbn = {0897917308},
    keywords = {string\_matching},
    pages = {36--45},
    posted-at = {2008-04-20 21:28:30},
    priority = {2},
    publisher = {ACM},
    title = {Similarity-based queries},
    url = {http://dx.doi.org/10.1145/212433.212444},
    year = {1995}
}

@article{bensteinZobel2004:coderived,
    abstract = {Documents are co-derivative if they share content: for two documents to be co-derived, some portion of one must be derived from the other or some portion of both must be derived from a third document. The current technique for concurrently detecting all co-derivatives in a collection is document fingerprinting, which matches documents based on the hash values of selected document subsequences, or chunks. Fingerprinting is currently hampered by an inability to accurately isolate information that is useful in identifying co-derivatives. In this paper we present , a novel hash-based algorithm for extracting duplicated chunks from a document collection. We discuss how information about shared chunks can be used for efficiently and reliably identifying co-derivative clusters, and describe , a prototype system that makes use of . Our experiments with several document collections demonstrate the effectiveness of the approach.},
    author = {Bernstein, Yaniv and Zobel, Justin},
    citeulike-article-id = {2693518},
    citeulike-linkout-0 = {http://www.springerlink.com/content/ej6qjg9gnt91hq29},
    journal = {String Processing and Information Retrieval},
    keywords = {document\_linking},
    pages = {55--67},
    posted-at = {2008-04-20 20:32:30},
    priority = {2},
    title = {A Scalable System for Identifying Co-derivative Documents},
    url = {http://www.springerlink.com/content/ej6qjg9gnt91hq29},
    year = {2004}
}

@inproceedings{metzlerEtAl2005:simMeasures,
    abstract = {Text similarity spans a spectrum, with broad topical similarity near one extreme and document identity at the other. Intermediate levels of similarity -- resulting from summarization, paraphrasing, copying, and stronger forms of topical relevance -- are useful for applications such as information flow analysis and question-answering tasks. In this paper, we explore mechanisms for measuring such intermediate kinds of similarity, focusing on the task of identifying where a particular piece of information originated. We consider both sentence-to-sentence and document-to-document comparison, and have incorporated these algorithms into \&lt;small\&gt;RECAP\&lt;/small\&gt;, a prototype information flow analysis tool. Our experimental results with \&lt;small\&gt;RECAP\&lt;/small\&gt; indicate that new mechanisms such as those we propose are likely to be more appropriate than existing methods for identifying the intermediate forms of similarity.},
    address = {New York, NY, USA},
    author = {Metzler, Donald and Bernstein, Yaniv and Croft, W. Bruce and Moffat, Alistair and Zobel, Justin},
    booktitle = {CIKM '05: Proc. of the 14th ACM intl. conf. on Information and knowledge management},
    citeulike-article-id = {2501412},
    citeulike-linkout-0 = {http://portal.acm.org/citation.cfm?id=1099554.1099695},
    citeulike-linkout-1 = {http://dx.doi.org/10.1145/1099554.1099695},
    doi = {10.1145/1099554.1099695},
    isbn = {1-59593-140-6},
    keywords = {string\_matching},
    location = {Bremen, Germany},
    pages = {517--524},
    posted-at = {2008-04-17 01:37:21},
    priority = {2},
    publisher = {ACM},
    title = {Similarity measures for tracking information flow},
    url = {http://dx.doi.org/10.1145/1099554.1099695},
    year = {2005}
}

@article{levenshtein1966,
    abstract = {Not Available},
    author = {{Levenshtein}, V. I.},
    citeulike-article-id = {1667643},
    citeulike-linkout-0 = {http://adsabs.harvard.edu/cgi-bin/nph-bib\_query?bibcode=1966SPhD...10..707L},
    journal = {Soviet Physics Doklady},
    keywords = {string\_matching},
    month = {February},
    pages = {707+},
    posted-at = {2008-04-17 00:30:08},
    priority = {2},
    title = {Binary Codes Capable of Correcting Deletions, Insertions and Reversals},
    url = {http://adsabs.harvard.edu/cgi-bin/nph-bib\_query?bibcode=1966SPhD...10..707L},
    volume = {10},
    year = {1966}
}

@article{smithWaterman1981:subseq,
    author = {Smith, T. F. and Waterman, M. S.},
    citeulike-article-id = {668527},
    citeulike-linkout-0 = {http://view.ncbi.nlm.nih.gov/pubmed/7265238},
    citeulike-linkout-1 = {http://www.hubmed.org/display.cgi?uids=7265238},
    day = {25},
    issn = {0022-2836},
    journal = {Journal of molecular biology},
    keywords = {string\_matching},
    month = {March},
    number = {1},
    pages = {195--197},
    posted-at = {2008-04-17 00:25:19},
    priority = {2},
    title = {Identification of common molecular subsequences.},
    url = {http://view.ncbi.nlm.nih.gov/pubmed/7265238},
    volume = {147},
    year = {1981}
}

@book{gusfield1997,
    abstract = {{Traditionally an area of study in computer science, string algorithms have, in recent years, become an increasingly important part of biology, particularly genetics.  This volume is a comprehensive look at computer algorithms for string processing. In addition to pure computer science, Gusfield adds extensive discussions on biological problems that are cast as string problems and on methods developed to solve them. This text emphasizes the fundamental ideas and techniques central to today's applications.  New approaches to this complex material simplify methods that up to now have been for the specialist alone.  With over 400 exercises to reinforce the material and develop additional topics, the book is suitable as a text for graduate or advanced undergraduate students in computer science, computational biology, or bio-informatics.}},
    author = {Gusfield, Dan},
    citeulike-article-id = {163533},
    citeulike-linkout-0 = {http://www.amazon.ca/exec/obidos/redirect?tag=citeulike09-20\&amp;path=ASIN/0521585198},
    citeulike-linkout-1 = {http://www.amazon.de/exec/obidos/redirect?tag=citeulike01-21\&amp;path=ASIN/0521585198},
    citeulike-linkout-10 = {http://www.worldcat.org/oclc/255631314},
    citeulike-linkout-2 = {http://www.amazon.fr/exec/obidos/redirect?tag=citeulike06-21\&amp;path=ASIN/0521585198},
    citeulike-linkout-3 = {http://www.amazon.jp/exec/obidos/ASIN/0521585198},
    citeulike-linkout-4 = {http://www.amazon.co.uk/exec/obidos/ASIN/0521585198/citeulike00-21},
    citeulike-linkout-5 = {http://www.amazon.com/exec/obidos/redirect?tag=citeulike07-20\&path=ASIN/0521585198},
    citeulike-linkout-6 = {http://www.worldcat.org/isbn/0521585198},
    citeulike-linkout-7 = {http://books.google.com/books?vid=ISBN0521585198},
    citeulike-linkout-8 = {http://www.amazon.com/gp/search?keywords=0521585198\&index=books\&linkCode=qs},
    citeulike-linkout-9 = {http://www.librarything.com/isbn/0521585198},
    day = {15},
    howpublished = {Hardcover},
    isbn = {0521585198},
    keywords = {book, datastructures},
    month = {January},
    posted-at = {2008-04-16 23:27:57},
    priority = {1},
    publisher = {Cambridge Univ. Press},
    title = {Algorithms on strings, trees, and sequences : computer science and computational biology},
    url = {http://www.amazon.com/exec/obidos/redirect?tag=citeulike07-20\&path=ASIN/0521585198},
    year = {2007}
}

@article{ferraginaGrossi1999:StringBtree,
    abstract = {We introduce a new text-indexing data structure, the  String B-Tree , that can be seen as a link between some traditional external-memory and string-matching data structures. In a short phrase, it is a combination of B-trees and Patricia tries for internal-node indices that is made more effective by adding extra pointers to speed up search and update operations. Consequently, the String B-Tree overcomes the theoretical limitations of inverted files, B-trees, prefix B-trees, suffix arrays, compacted tries and suffix trees. String B-trees have the same worst-case performance as B-trees but they manage unbounded-length strings and perform much more powerful search operations such as the ones supported by suffix trees. String B-trees are also effective in main memory (RAM  model) because they improve the online suffix tree search on a dynamic set of strings. They also can be successfully applied to database indexing and software duplication.},
    address = {New York, NY, USA},
    author = {Ferragina, Paolo and Grossi, Roberto},
    citeulike-article-id = {1228075},
    citeulike-linkout-0 = {http://portal.acm.org/citation.cfm?id=301970.301973},
    citeulike-linkout-1 = {http://dx.doi.org/10.1145/301970.301973},
    doi = {10.1145/301970.301973},
    issn = {0004-5411},
    journal = {J. ACM},
    keywords = {datastructures},
    month = {March},
    number = {2},
    pages = {236--280},
    posted-at = {2008-04-16 23:25:46},
    priority = {2},
    publisher = {ACM},
    title = {The string B-tree: a new data structure for string search in external memory and its applications},
    url = {http://dx.doi.org/10.1145/301970.301973},
    volume = {46},
    year = {1999}
}

@article{hoadZobel2003:plagiarism,
    abstract = {The widespread use of on-line publishing of text promotes storage of multiple versions of documents and mirroring of documents in multiple locations, and greatly simplifies the task of plagiarizing the work of others. We evaluate two families of methods for searching a collection to find documents that are coderivative, that is, are versions or plagiarisms of each other. The first, the ranking family, uses information retrieval techniques; extending this family, we propose the identity measure, which is specifically designed for identification of coderivative documents. The second, the fingerprinting family, uses hashing to generate a compact document description, which can then be compared to the fingerprints of the documents in the collection. We introduce a new method for evaluating the effectiveness of these techniques, and demonstrate it in practice. Using experiments on two collections, we demonstrate that the identity measure and the best fingerprinting technique are both able to accurately identify coderivative documents. However, for fingerprinting parameters must be carefully chosen, and even so the identity measure is clearly superior.},
    address = {School of Computer Science and Information Technology, RMIT University GPO Box 2476V, Melbourne 3001, Australia},
    author = {Hoad, Timothy C. and Zobel, Justin},
    citeulike-article-id = {2526649},
    citeulike-linkout-0 = {http://portal.acm.org/citation.cfm?id=766444},
    citeulike-linkout-1 = {http://dx.doi.org/10.1002/asi.10170},
    citeulike-linkout-2 = {http://www3.interscience.wiley.com/cgi-bin/abstract/102525285/ABSTRACT},
    doi = {10.1002/asi.10170},
    journal = {Journal of the American Society for Information Science and Technology},
    keywords = {document\_linking, plagiarism},
    number = {3},
    pages = {203--215},
    posted-at = {2008-04-16 23:11:23},
    priority = {2},
    title = {Methods for identifying versioned and plagiarized documents},
    url = {http://dx.doi.org/10.1002/asi.10170},
    volume = {54},
    year = {2003}
}

@article{citeulike:276360,
    address = {New York, NY, USA},
    author = {Iverson, Kenneth E.},
    citeulike-article-id = {276360},
    citeulike-linkout-0 = {http://portal.acm.org/citation.cfm?id=358899},
    citeulike-linkout-1 = {http://dx.doi.org/10.1145/358896.358899},
    doi = {10.1145/358896.358899},
    issn = {0001-0782},
    journal = {Commun. ACM},
    keywords = {language, sapirworf},
    month = {August},
    number = {8},
    pages = {444--465},
    posted-at = {2008-04-15 06:30:13},
    priority = {2},
    publisher = {ACM Press},
    title = {Notation as a tool of thought},
    url = {http://dx.doi.org/10.1145/358896.358899},
    volume = {23},
    year = {1980}
}

@electronic{opennlp,
    citeulike-article-id = {2670980},
    howpublished = {Website: http://opennlp.sourceforge.net},
    keywords = {nlp},
    posted-at = {2008-04-15 02:19:52},
    priority = {2},
    title = {{Open NLP}},
    year = {2008}
}

@inbook{singh-barto-chentanez:nips,
    abstract = {Psychologists call behavior intrinsically motivated when it is engaged in
for its own sake rather than as a step toward solving a speciﬁc problem
of clear practical value. But what we learn during intrinsically motivated
behavior is essential for our development as competent autonomous en-
tities able to efﬁciently solve a wide range of practical problems as they
arise. In this paper we present initial results from a computational study
of intrinsically motivated reinforcement learning aimed at allowing arti-
ﬁcial agents to construct and extend hierarchies of reusable skills that are
needed for competent autonomy.},
    address = {Cambridge, MA},
    author = {Singh, Satinder and Barto, Andrew G. and Chentanez, Nuttapong},
    booktitle = {Advances in Neural Information Processing Systems 17},
    citeulike-article-id = {2604148},
    editor = {Saul, Lawrence K. and Weiss, Yair and Bottou, {l\'{e}on}},
    keywords = {intrinsic\_motivation, reinforcement\_learning},
    pages = {1281--1288},
    posted-at = {2008-03-27 22:58:19},
    priority = {2},
    publisher = {MIT Press},
    title = {Intrinsically Motivated Reinforcement Learning},
    year = {2005}
}

@book{liVap1997:Kolmogorov,
    address = {Secaucus, NJ, USA},
    author = {Li, Ming and Vit\'{a}nyi, Paul},
    citeulike-article-id = {2604143},
    keywords = {book, kormogorov},
    posted-at = {2008-03-27 22:54:22},
    priority = {2},
    publisher = {Springer-Verlag New York, Inc.},
    title = {An introduction to Kolmogorov complexity and its applications (2nd ed.)},
    year = {1997}
}

@inbook{BengioLecun2007:scaling,
    abstract = {One long-term goal of machine learning research is to produce methods that
are applicable to highly complex tasks, such as perception (vision, audition), rea-
soning, intelligent control, and other artiﬁcially intelligent behaviors. We argue
that in order to progress toward this goal, the Machine Learning community must
endeavor to discover algorithms that can learn highly complex functions, with min-
imal need for prior knowledge, and with minimal human intervention. We present
mathematical and empirical evidence suggesting that many popular approaches
to non-parametric learning, particularly kernel methods, are fundamentally lim-
ited in their ability to learn complex high-dimensional functions. Our analysis
focuses on two problems. First, kernel machines are shallow architectures, in
which one large layer of simple template matchers is followed by a single layer
of trainable coefﬁcients. We argue that shallow architectures can be very inefﬁ-
cient in terms of required number of computational elements and examples. Sec-
ond, we analyze a limitation of kernel machines with a local kernel, linked to the
curse of dimensionality, that applies to supervised, unsupervised (manifold learn-
ing) and semi-supervised kernel machines. Using empirical results on invariant
image recognition tasks, kernel methods are compared with deep architectures, in
which lower-level features or concepts are progressively combined into more ab-
stract and higher-level representations. We argue that deep architectures have the
potential to generalize in non-local ways, i.e., beyond immediate neighbors, and
that this is crucial in order to make progress on the kind of complex tasks required
for artiﬁcial intelligence.},
    author = {Bengio, Yoshua and Lecun, Yann},
    booktitle = {Large-Scale Kernel Machines},
    citeulike-article-id = {2604139},
    editor = {Bottou, L. and Chapelle, O. and Decoste, D. and Weston, J.},
    keywords = {kernelmachines, learning},
    posted-at = {2008-03-27 22:50:43},
    priority = {2},
    publisher = {MIT Press},
    title = {Scaling Learning Algorithms towards AI},
    year = {2007}
}

@proceedings{ranzatoEtAl2007:Image,
    abstract = {We present an unsupervised method for learning a hierarchy of sparse feature detectors that are invariant to small shifts and distortions. The resulting feature extractor consists of multiple convolution filters, followed by a feature-pooling layer that computes the max of each filter output within adjacent windows, and a point-wise sigmoid non-linearity. A second level of larger and more invariant features is obtained by training the same algorithm on patches of features from the first level. Training a supervised classifier on these features yields 0.64\% error on MNIST, and 54\% average recognition rate on Caltech 101 with 30 training samples per category. While the resulting architecture is similar to convolutional networks, the layer-wise unsupervised training procedure alleviates the over-parameterization problems that plague purely supervised learning procedures, and yields good performance with very few labeled training samples.},
    author = {Ranzato, M. A. and Huang, Fu J. and Boureau, Y. L. and Lecun, Yann},
    booktitle = {Computer Vision and Pattern Recognition, 2007. CVPR '07. IEEE Conf. on},
    citeulike-article-id = {2604128},
    citeulike-linkout-0 = {http://dx.doi.org/10.1109/CVPR.2007.383157},
    citeulike-linkout-1 = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=4270182},
    doi = {10.1109/CVPR.2007.383157},
    journal = {Computer Vision and Pattern Recognition, 2007. CVPR '07. IEEE Conf. on},
    keywords = {learning, unsupervised\_learning},
    pages = {1--8},
    posted-at = {2008-03-27 22:45:21},
    priority = {2},
    title = {Unsupervised Learning of Invariant Feature Hierarchies with Applications to Object Recognition},
    url = {http://dx.doi.org/10.1109/CVPR.2007.383157},
    year = {2007}
}

@inproceedings{lecunEtAl1997:checks,
    address = {Washington, DC, USA},
    author = {Lecun, Y. and Bottou, L. and Bengio, Y.},
    booktitle = {ICASSP '97: Proc. of the 1997 IEEE Intl. Conf. on Acoustics, Speech, and Signal Processing (ICASSP '97) -Volume 1},
    citeulike-article-id = {2604121},
    citeulike-linkout-0 = {http://portal.acm.org/citation.cfm?id=844378.844449},
    isbn = {0818679190},
    keywords = {learning},
    posted-at = {2008-03-27 22:43:17},
    priority = {2},
    publisher = {IEEE Computer Society},
    title = {Reading Checks with Multilayer Graph Transformer Networks},
    url = {http://portal.acm.org/citation.cfm?id=844378.844449},
    year = {1997}
}

@article{natarajan:ilp2007,
    address = {Berlin},
    author = {Natarajan, Sriraam and Tadepalli, Prasad and Fern, Alan},
    booktitle = {Proc. of the Seventeenth Conf. on Inductive Logic Programming},
    citeulike-article-id = {2604117},
    editor = {Blockeel},
    keywords = {learning},
    posted-at = {2008-03-27 22:41:18},
    priority = {2},
    publisher = {Springer-Verlag},
    title = {A Relational Hierarchical Model for Decision Theoretic Assistance},
    year = {2007}
}

@inproceedings{simsek-barto:icml2006,
    abstract = {How should a reinforcement learning agent act if its sole purpose is to efficiently learn an optimal policy for later use? In other words, how should it explore, to be able to exploit later? We formulate this problem as a Markov Decision Process by explicitly modeling the internal state of the agent and propose a principled heuristic for its solution. We present experimental results in a number of domains, also exploring the algorithm's use for learning a policy for a skill given its reward function---an important but neglected component of skill discovery.},
    address = {New York, NY, USA},
    author = {\c{s}im\c{s}ek, \"{O}zg\"{u}r and Barto, Andrew G.},
    booktitle = {ICML '06: Proc. of the 23rd intl. conf. on Machine learning},
    citeulike-article-id = {2604098},
    citeulike-linkout-0 = {http://portal.acm.org/citation.cfm?id=1143949},
    citeulike-linkout-1 = {http://dx.doi.org/10.1145/1143844.1143949},
    doi = {10.1145/1143844.1143949},
    isbn = {1595933832},
    keywords = {icml06, reinforcement\_learning},
    pages = {833--840},
    posted-at = {2008-03-27 22:33:44},
    priority = {2},
    publisher = {ACM},
    title = {An intrinsic reward mechanism for efficient exploration},
    url = {http://dx.doi.org/10.1145/1143844.1143949},
    year = {2006}
}

@inproceedings{hintonsemantic,
    abstract = {We show how to learn a deep graphical model of the word-count
vectors obtained from a large set of documents. The values of the
latent variables in the deepest layer are easy to infer and give a
much better representation of each document than Latent Semantic
Analysis. When the deepest layer is forced to use a small number of
                       ¿¾
binary variables (e.g. ), the graphical model performs  ” semantic
hashing”: Documents are mapped to memory addresses in such a
way that semantically similar documents are located at nearby ad-
dresses. Documents similar to a query document can then be found
by simply accessing all the addresses that differ by only a few bits
from the address of the query document. This way of extending the
efﬁciency of hash-coding to approximate matching is much faster
than locality sensitive hashing, which is the fastest current method.
By using semantic hashing to ﬁlter the documents given to TF-IDF,
we achieve higher accuracy than applying TF-IDF to the entire doc-
ument set.},
    address = {Amsterdam},
    author = {Salakhutdinov, Ruslan R. and Hinton, Geoffrey E.},
    booktitle = {Proc. of the SIGIR Workshop on Information Retrieval and Applications of Graphical Models},
    citeulike-article-id = {2604093},
    keywords = {semantics, sigir07, tf-idf},
    posted-at = {2008-03-27 22:31:28},
    priority = {4},
    publisher = {Elsevier},
    title = {Semantic Hashing},
    year = {2007}
}

@inproceedings{suttonEtal1998:options,
    address = {San Francisco, CA, USA},
    author = {Sutton, Richard S. and Precup, Doina and Singh, Satinder P.},
    booktitle = {ICML '98: Proc. of the Fifteenth Intl. Conf. on Machine Learning},
    citeulike-article-id = {2604086},
    citeulike-linkout-0 = {http://portal.acm.org/citation.cfm?id=657453},
    isbn = {1558605568},
    keywords = {learning},
    pages = {556--564},
    posted-at = {2008-03-27 22:26:04},
    priority = {2},
    publisher = {Morgan Kaufmann Publishers Inc.},
    title = {Intra-Option Learning about Temporally Abstract Actions},
    url = {http://portal.acm.org/citation.cfm?id=657453},
    year = {1998}
}

@inproceedings{leCunEtAl:endToEnd,
    abstract = {We describe a vision-based obstacle avoidance system for off-road mo-
bile robots. The system is trained from end to end to map raw input
images to steering angles. It is trained in supervised mode to predict the
steering angles provided by a human driver during training runs collected
in a wide variety of terrains, weather conditions, lighting conditions, and
obstacle types. The robot is a 50cm off-road truck, with two forward-
pointing wireless color cameras. A remote computer processes the video
and controls the robot via radio. The learning system is a large 6-layer
convolutional network whose input is a single left/right pair of unpro-
cessed low-resolution images. The robot exhibits an excellent ability to
detect obstacles and navigate around them in real time at speeds of 2 m/s.},
    address = {Cambridge, MA},
    author = {Lecun, Yann and Muller, Urs and Ben, Jan and Cosatto, Eric and Flepp, Beat},
    booktitle = {Advances in Neural Information Processing Systems 18},
    citeulike-article-id = {2604078},
    citeulike-linkout-0 = {http://www.bibsonomy.org/bibtex/2e0784582db73525889a96595be298d94/idsia},
    editor = {Weiss, Y. and Sch\"{o}lkopf, B. and Platt, J.},
    keywords = {learning, nips},
    pages = {739--746},
    posted-at = {2008-03-27 22:24:03},
    priority = {2},
    publisher = {MIT Press},
    title = {Off-Road Obstacle Avoidance through End-to-End Learning},
    url = {http://www.bibsonomy.org/bibtex/2e0784582db73525889a96595be298d94/idsia},
    year = {2006}
}

@book{vapnik1999,
    abstract = {{The aim of this book is to discuss the fundamental ideas which lie behind the statistical theory of learning and generalization. It considers learning as a general problem of function estimation based on empirical data. Omitting proofs and technical details, the author concentrates on discussing the main results of learning theory and their connections to fundamental problems in statistics. These include: * the setting of learning problems based on the model of minimizing the risk functional from empirical data * a comprehensive analysis of the empirical risk minimization principle including necessary and sufficient conditions for its consistency * non-asymptotic bounds for the risk achieved using the empirical risk minimization principle * principles for controlling the generalization ability of learning machines using small sample sizes based on these bounds * the Support Vector methods that control the generalization ability when estimating function using small sample size. The second edition of the book contains three new chapters devoted to further development of the learning theory and SVM techniques. These include: * the theory of direct method of learning based on solving multidimensional integral equations for density, conditional probability, and conditional density estimation * a new inductive principle of learning. Written in a readable and concise style, the book is intended for statisticians, mathematicians, physicists, and computer scientists. Vladimir N. Vapnik is Technology Leader AT\&T Labs-Research and Professor of London University. He is one of the founders of statistical learning theory, and the author of seven books published in English, Russian, German, and Chinese.}},
    author = {Vapnik, Vladimir},
    citeulike-article-id = {115106},
    citeulike-linkout-0 = {http://www.cs.orst.edu/\~{}bulatov/papers/Vapnik, V. (2000) The Nature of Stastical Learning Theory, 167p.pdf},
    citeulike-linkout-1 = {http://www.amazon.ca/exec/obidos/redirect?tag=citeulike09-20\&amp;path=ASIN/0387987800},
    citeulike-linkout-10 = {http://www.librarything.com/isbn/0387987800},
    citeulike-linkout-2 = {http://www.amazon.de/exec/obidos/redirect?tag=citeulike01-21\&amp;path=ASIN/0387987800},
    citeulike-linkout-3 = {http://www.amazon.fr/exec/obidos/redirect?tag=citeulike06-21\&amp;path=ASIN/0387987800},
    citeulike-linkout-4 = {http://www.amazon.jp/exec/obidos/ASIN/0387987800},
    citeulike-linkout-5 = {http://www.amazon.co.uk/exec/obidos/ASIN/0387987800/citeulike00-21},
    citeulike-linkout-6 = {http://www.amazon.com/exec/obidos/redirect?tag=citeulike07-20\&path=ASIN/0387987800},
    citeulike-linkout-7 = {http://www.worldcat.org/isbn/0387987800},
    citeulike-linkout-8 = {http://books.google.com/books?vid=ISBN0387987800},
    citeulike-linkout-9 = {http://www.amazon.com/gp/search?keywords=0387987800\&index=books\&linkCode=qs},
    day = {19},
    edition = {2nd},
    howpublished = {Hardcover},
    isbn = {0387987800},
    keywords = {book, learning},
    month = {November},
    posted-at = {2008-03-27 22:16:35},
    priority = {2},
    publisher = {Springer},
    title = {The Nature of Statistical Learning Theory (Information Science and Statistics)},
    url = {http://www.cs.orst.edu/\~{}bulatov/papers/Vapnik, V. (2000) The Nature of Stastical Learning Theory, 167p.pdf},
    year = {1999}
}

@article{hinton2007:learninglayers,
    abstract = {To achieve its' impressive performance at tasks such as speech or
object recognition, the brain extracts multiple levels of representation
from the sensory input. Backpropagation was the ﬁrst computation-
ally eﬃcient model of how neural networks could learn multiple layers
of representation, but it required labeled training data and it did not
work well in deep networks. The limitations of backpropagation learn-
ing can now be overcome by using multi-layer neural networks that
contain top-down connections and training them to generate sensory
data rather than to classify it. Learning multilayer generative models
appears to be diﬃcult, but a recent discovery makes it easy to learn
non-linear, distributed representations one layer at a time. The mul-
tiple layers of representation learned in this way can subsequently be
ﬁne-tuned to produce generative or discriminative models that work
much better than previous approaches.},
    author = {Hinton, Geoffrey E.},
    citeulike-article-id = {2599867},
    journal = {Trends in Cognitive Sciences},
    keywords = {learning},
    number = {10},
    posted-at = {2008-03-26 22:08:00},
    priority = {2},
    title = {Learning multiple layers of representation},
    volume = {11},
    year = {2007}
}

@inproceedings{hinton2007:shapes,
    abstract = {The uniformity of the cortical architecture and the ability of functions to move to
diﬀerent areas of cortex following early damage strongly suggests that there is a single
basic learning algorithm for extracting underlying structure from richly-structured, high-
dimensional sensory data. There have been many attempts to design such an algorithm,
but until recently they all suﬀered from serious computational weaknesses. This chapter
describes several of the proposed algorithms and shows how they can be combined to
produce hybrid methods that work eﬃciently in networks with many layers and millions
of adaptive connections.},
    author = {Hinton, Geoffrey E.},
    booktitle = {Computational Neuroscience: Theoretical Insights into Brain Function},
    citeulike-article-id = {2599858},
    keywords = {learning},
    posted-at = {2008-03-26 22:02:42},
    priority = {2},
    publisher = {Elsevier},
    title = {To Recognize Shapes First Learn to Generate Images},
    year = {2007}
}

@inproceedings{salakhutdinovEtAl2007:rmbsFiltering,
    abstract = {Most of the existing approaches to collaborative filtering cannot handle very large data sets. In this paper we show how a class of two-layer undirected graphical models, called Restricted Boltzmann Machines (RBM's), can be used to model tabular data, such as user's ratings of movies. We present efficient learning and inference procedures for this class of models and demonstrate that RBM's can be successfully applied to the Netflix data set, containing over 100 million user/movie ratings. We also show that RBM's slightly outperform carefully-tuned SVD models. When the predictions of multiple RBM models and multiple SVD models are linearly combined, we achieve an error rate that is well over 6\% better than the score of Netflix's own system.},
    address = {New York, NY, USA},
    author = {Salakhutdinov, Ruslan and Mnih, Andriy and Hinton, Geoffrey},
    booktitle = {ICML '07: Proc. of the 24th intl. conf. on Machine learning},
    citeulike-article-id = {1639680},
    citeulike-linkout-0 = {http://portal.acm.org/citation.cfm?id=1273496.1273596},
    citeulike-linkout-1 = {http://dx.doi.org/10.1145/1273496.1273596},
    doi = {10.1145/1273496.1273596},
    isbn = {978-1-59593-793-3},
    keywords = {rbms},
    location = {Corvalis, Oregon},
    pages = {791--798},
    posted-at = {2008-03-26 21:54:24},
    priority = {2},
    publisher = {ACM},
    title = {Restricted Boltzmann machines for collaborative filtering},
    url = {http://dx.doi.org/10.1145/1273496.1273596},
    year = {2007}
}

@inproceedings{mehtaEtAl2005:transferVRHRL,
    abstract = {We consider the problem of transferring learned knowledge among
Markov Decision Processes (MDPs) that share the same transition dy-
namics but different reward functions. In particular, we assume that
reward functions are described as linear combinations of reward fea-
tures, and that only the feature weights vary among MDPs. We intro-
duce Variable-Reward Hierarchical Reinforcement Learning (VRHRL),
which leverages previously learned policies to speed-up learning in this
setting. With suitable design of the hierarchy, VRHRL can achieve better
transfer than its non-hierarchical counterpart.},
    author = {Mehta, Neville and Natarajan, Sriraam and Tadepalli, Prasad and Fern, Alan},
    booktitle = {Workshop on Transfer Learning at Neural Information Processing Systems},
    citeulike-article-id = {2599841},
    keywords = {learning, reinforcement\_learning},
    posted-at = {2008-03-26 21:49:43},
    priority = {2},
    title = {Transfer in Variable-reward Hierarchical Reinforcement Learning},
    year = {2005}
}

@phdthesis{Andre2003:alispThesis,
    author = {Andre, David},
    citeulike-article-id = {2599835},
    keywords = {learning, reinforcement\_learning},
    posted-at = {2008-03-26 21:44:12},
    priority = {2},
    school = {University of California, Berkeley},
    title = {Programmable reinforcement learning agents},
    year = {2003}
}

@inproceedings{dietterich1998:maxq,
    address = {San Francisco, CA, USA},
    author = {Dietterich, Thomas G.},
    booktitle = {ICML '98: Proc. of the Fifteenth Intl. Conf. on Machine Learning},
    citeulike-article-id = {2599825},
    citeulike-linkout-0 = {http://portal.acm.org/citation.cfm?id=645527.657449},
    isbn = {1558605568},
    keywords = {heirarchical\_learning, hierarchical\_learning, learning, q-learning, reinforcement\_learning},
    pages = {118--126},
    posted-at = {2008-03-26 21:39:13},
    priority = {2},
    publisher = {Morgan Kaufmann Publishers Inc.},
    title = {The MAXQ Method for Hierarchical Reinforcement Learning},
    url = {http://portal.acm.org/citation.cfm?id=645527.657449},
    year = {1998}
}

@inproceedings{banerjee07,
    abstract = {We present a reinforcement learning game player that can interact with a General Game Playing system and transfer knowledge learned in one game to expedite learning in many other games. We use the technique of value-function transfer where general features are extracted from the state space of a previous game and matched with the completely different state space of a new game. To capture the underlying similarity of vastly disparate state spaces arising from different games, we use a game-tree lookahead structure for features. We show that such feature-based value function transfer learns superior policies faster than a reinforcement learning agent that does not use knowledge transfer. Furthermore, knowledge transfer using lookahead features can capture opponent-specific value-functions, i.e. can exploit an opponent's weaknesses to learn faster than a reinforcement learner that uses lookahead with minimax (pessimistic) search against the same opponent. 1},
    address = {Hyderabad, India},
    author = {Banerjee, Bikramjit and Stone, Peter},
    booktitle = {Proc. of the 20th Intl. Joint Conf. on Artificial Intelligence (IJCAI)},
    citeulike-article-id = {2599820},
    citeulike-linkout-0 = {http://citeseerx.ist.psu.edu/viewdoc/summary;jsessionid=55728FE96E5758F92EB011CA1F2A6BC0?cid=3715927},
    keywords = {learning, reinforcement\_learning},
    posted-at = {2008-03-26 21:36:01},
    priority = {2},
    title = {General game learning using knowledge transfer},
    url = {http://citeseerx.ist.psu.edu/viewdoc/summary;jsessionid=55728FE96E5758F92EB011CA1F2A6BC0?cid=3715927},
    year = {2007}
}

@book{suttonBarto1998:RLanIntro,
    abstract = {Reinforcement learning, one of the most active research areas in artificial intelligence, is a computational approach to learning whereby an agent tries to maximize the total amount of reward it receives when interacting with a complex, uncertain environment. In <i>Reinforcement Learning</i>, Richard Sutton and Andrew Barto provide a clear and simple account of the key ideas and algorithms of reinforcement learning. Their discussion ranges from the history of the field's intellectual foundations to the most recent developments and applications. The only necessary mathematical background is familiarity with elementary concepts of probability.<br /> <br /> The book is divided into three parts. Part I defines the reinforcement learning problem in terms of Markov decision processes. Part II provides basic solution methods: dynamic programming, Monte Carlo methods, and temporal-difference learning. Part III presents a unified view of the solution methods and incorporates artificial neural networks, eligibility traces, and planning; the two final chapters present case studies and consider the future of reinforcement learning.},
    author = {Sutton, Richard S. and Barto, Andrew G.},
    citeulike-article-id = {112017},
    citeulike-linkout-0 = {http://www.amazon.ca/exec/obidos/redirect?tag=citeulike09-20\&amp;path=ASIN/0262193981},
    citeulike-linkout-1 = {http://www.amazon.de/exec/obidos/redirect?tag=citeulike01-21\&amp;path=ASIN/0262193981},
    citeulike-linkout-2 = {http://www.amazon.fr/exec/obidos/redirect?tag=citeulike06-21\&amp;path=ASIN/0262193981},
    citeulike-linkout-3 = {http://www.amazon.jp/exec/obidos/ASIN/0262193981},
    citeulike-linkout-4 = {http://www.amazon.co.uk/exec/obidos/ASIN/0262193981/citeulike00-21},
    citeulike-linkout-5 = {http://www.amazon.com/exec/obidos/redirect?tag=citeulike07-20\&path=ASIN/0262193981},
    citeulike-linkout-6 = {http://www.worldcat.org/isbn/0262193981},
    citeulike-linkout-7 = {http://books.google.com/books?vid=ISBN0262193981},
    citeulike-linkout-8 = {http://www.amazon.com/gp/search?keywords=0262193981\&index=books\&linkCode=qs},
    citeulike-linkout-9 = {http://www.librarything.com/isbn/0262193981},
    day = {01},
    howpublished = {Hardcover},
    isbn = {0262193981},
    keywords = {book, reinforcement\_learning},
    month = {March},
    posted-at = {2008-03-26 21:30:42},
    priority = {2},
    publisher = {The MIT Press},
    title = {Reinforcement Learning: An Introduction},
    url = {http://www.amazon.com/exec/obidos/redirect?tag=citeulike07-20\&path=ASIN/0262193981},
    year = {1998}
}

@article{tesauro1992:TD-learning,
    address = {Hingham, MA, USA},
    author = {Tesauro, Gerald},
    citeulike-article-id = {465792},
    citeulike-linkout-0 = {http://portal.acm.org/citation.cfm?id=139611.139616},
    citeulike-linkout-1 = {http://dx.doi.org/10.1007/BF00992697},
    doi = {10.1007/BF00992697},
    issn = {0885-6125},
    journal = {Mach. Learn.},
    keywords = {td\_learning},
    number = {3-4},
    pages = {257--277},
    posted-at = {2008-03-26 21:26:27},
    priority = {2},
    publisher = {Kluwer Academic Publishers},
    title = {Practical Issues in Temporal Difference Learning},
    url = {http://dx.doi.org/10.1007/BF00992697},
    volume = {8},
    year = {1992}
}

@article{tadepalli-ok:modelBasedRL,
    address = {Essex, UK},
    author = {Tadepalli, Prasad and Ok, Dokyeong},
    citeulike-article-id = {2599797},
    citeulike-linkout-0 = {http://portal.acm.org/citation.cfm?id=279711},
    issn = {0004-3702},
    journal = {Artif. Intell.},
    keywords = {reinforcement\_learning},
    number = {1-2},
    pages = {177--224},
    posted-at = {2008-03-26 21:24:31},
    priority = {2},
    publisher = {Elsevier Science Publishers Ltd.},
    title = {Model-based average reward reinforcement learning},
    url = {http://portal.acm.org/citation.cfm?id=279711},
    volume = {100},
    year = {1998}
}

@article{comon:ICA,
    abstract = {The independent component analysis (ICA) of a random vector consists of searching for a linear transformation that minimizes the statistical dependence between its components. In order to define suitable search criteria, the expansion of mutual information is utilized as a function of cumulants of increasing orders. An efficient algorithm is proposed, which allows the computation of the ICA of a data matrix within a polynomial time. The concept of ICA may actually be seen as an extension of the principal component analysis (PCA), which can only impose independence up to the second order and, consequently, defines directions that are orthogonal. Potential applications of ICA include data analysis and compression, Bayesian detection, localization of sources, and blind identification and deconvolution.},
    address = {Amsterdam, The Netherlands, The Netherlands},
    author = {Comon, Pierre},
    citeulike-article-id = {2599791},
    citeulike-linkout-0 = {http://portal.acm.org/citation.cfm?id=195312},
    citeulike-linkout-1 = {http://dx.doi.org/10.1016/0165-1684(94)90029-9},
    citeulike-linkout-2 = {http://linkinghub.elsevier.com/retrieve/pii/0165168494900299},
    doi = {10.1016/0165-1684(94)90029-9},
    issn = {0165-1684},
    journal = {Signal Process.},
    keywords = {signal\_processing},
    month = {April},
    number = {3},
    pages = {287--314},
    posted-at = {2008-03-26 21:21:48},
    priority = {2},
    publisher = {Elsevier North-Holland, Inc.},
    title = {Independent component analysis, a new concept?},
    url = {http://dx.doi.org/10.1016/0165-1684(94)90029-9},
    volume = {36},
    year = {1994}
}

@book{jolliffe:PCA,
    address = {New York, NY, USA},
    author = {Jolliffe, I. T.},
    citeulike-article-id = {2599787},
    keywords = {book, pca},
    posted-at = {2008-03-26 21:20:22},
    priority = {2},
    publisher = {Springer},
    title = {Principal Component Analysis},
    year = {2002}
}

@book{bertsekas-ndp,
    author = {Bertsekas, Dimitri P. and Tsitsiklis, John N.},
    citeulike-article-id = {2599781},
    citeulike-linkout-0 = {http://portal.acm.org/citation.cfm?id=560669},
    isbn = {1886529108},
    keywords = {book},
    posted-at = {2008-03-26 21:17:22},
    priority = {2},
    publisher = {Athena Scientific},
    title = {Neuro-Dynamic Programming},
    url = {http://portal.acm.org/citation.cfm?id=560669},
    year = {1996}
}

@article{tadepalliRussel1998:trees,
    address = {Hingham, MA, USA},
    author = {Tadepalli, Prasad and Russell, Stuart},
    citeulike-article-id = {2544340},
    citeulike-linkout-0 = {http://portal.acm.org/citation.cfm?id=298772},
    issn = {0885-6125},
    journal = {Mach. Learn.},
    month = {September},
    number = {3},
    pages = {245--295},
    posted-at = {2008-03-17 06:17:13},
    priority = {2},
    publisher = {Kluwer Academic Publishers},
    title = {Learning from Examples and Membership Queries with Structured Determinations},
    url = {http://portal.acm.org/citation.cfm?id=298772},
    volume = {32},
    year = {1998}
}

@inproceedings{seriAndTadepalli2002:harl,
    address = {San Francisco, CA, USA},
    author = {Seri, Sandeep and Tadepalli, Prasad},
    booktitle = {ICML '02: Proc. of the Nineteenth Intl. Conf. on Machine Learning},
    citeulike-article-id = {2217111},
    citeulike-linkout-0 = {http://portal.acm.org/citation.cfm?id=645531.656009},
    isbn = {1558608737},
    pages = {562--569},
    posted-at = {2008-03-17 06:11:44},
    priority = {2},
    publisher = {Morgan Kaufmann Publishers Inc.},
    title = {Model-based Hierarchical Average-reward Reinforcement Learning},
    url = {http://portal.acm.org/citation.cfm?id=645531.656009},
    year = {2002}
}

@article{rieber1996:seriousgames,
    abstract = {http://it.coe.uga.edu/\~{}lrieber/play.html},
    author = {Rieber, Lloyd P.},
    citeulike-article-id = {908315},
    citeulike-linkout-0 = {http://dx.doi.org/10.1007/BF02300540},
    citeulike-linkout-1 = {http://www.springerlink.com/content/g85t307215n48202},
    day = {18},
    doi = {10.1007/BF02300540},
    journal = {Educational Technology Research and Development},
    keywords = {intelligent\_tutoring, serious\_games},
    month = {June},
    number = {2},
    pages = {43--58},
    posted-at = {2008-03-16 05:44:30},
    priority = {2},
    title = {Seriously considering play: Designing interactive learning environments based on the blending of microworlds, simulations, and games},
    url = {http://dx.doi.org/10.1007/BF02300540},
    volume = {44},
    year = {1996}
}

@inproceedings{houletteEtAl2001:aiToolkit,
    abstract = {We discuss our work to create an AI engine and editing
toolkit, suitable for both AI researchers and game develop-
ers. We describe the run-time engine and its interface, as
well as a simple behavior editor that allows non-
programmers to author behaviors. Initial work was con-
ducted using the Half-Life SDK.},
    author = {Houlette, Ryan and Fu, Daniel and Ross, David},
    booktitle = {AAAI Spring Symposium on AI and Interactive Entertainment},
    citeulike-article-id = {2534647},
    keywords = {ai, intelligent\_tutoring},
    posted-at = {2008-03-14 18:04:40},
    priority = {2},
    title = {Towards an AI Behavior Toolkit for Games},
    year = {2001}
}

@inproceedings{wilsonetal2007:multitaskHRL,
    address = {New York, NY, USA},
    author = {Wilson, Aaron and Fern, Alan and Ray, Soumya and Tadepalli, Prasad},
    booktitle = {ICML '07: Proc. of the 24th intl. conf. on Machine learning},
    citeulike-article-id = {2521733},
    citeulike-linkout-0 = {http://portal.acm.org/citation.cfm?id=1273624},
    citeulike-linkout-1 = {http://dx.doi.org/10.1145/1273496.1273624},
    doi = {10.1145/1273496.1273624},
    isbn = {9781595937933},
    keywords = {bayesian, hierarchical\_learning, learning},
    pages = {1015--1022},
    posted-at = {2008-03-12 16:33:39},
    priority = {0},
    publisher = {ACM},
    title = {Multi-task reinforcement learning: a hierarchical Bayesian approach},
    url = {http://dx.doi.org/10.1145/1273496.1273624},
    year = {2007}
}

@article{citeulike:1693134,
    abstract = {Reinforcement learning is bedeviled by the curse of dimensionality: the number of parameters to be learned grows exponentially with the size of any compact encoding of a state. Recent attempts to combat the curse of dimensionality have turned to principled ways of exploiting temporal abstraction, where decisions are not required at each step, but rather invoke the execution of temporally-extended activities which follow their own policies until termination. This leads naturally to hierarchical control architectures and associated learning algorithms. We review several approaches to temporal abstraction and hierarchical organization that machine learning researchers have recently developed. Common to these approaches is a reliance on the theory of semi-Markov decision processes, which we emphasize in our review. We then discuss extensions of these ideas to concurrent activities, multiagent coordination, and hierarchical memory for addressing partial observability. Concluding remarks address open challenges facing the further development of reinforcement learning in a hierarchical setting.},
    author = {Barto, Andrew G. and Mahadevan, Sridhar},
    citeulike-article-id = {1693134},
    citeulike-linkout-0 = {http://portal.acm.org/citation.cfm?id=941102},
    citeulike-linkout-1 = {http://dx.doi.org/10.1023/A:1025696116075},
    citeulike-linkout-2 = {http://www.springerlink.com/content/tl1n705w7q452066},
    day = {1},
    doi = {10.1023/A:1025696116075},
    journal = {Discrete Event Dynamic Systems},
    keywords = {hierarchical\_learning, learning, reinforcement\_learning},
    month = {October},
    number = {4},
    pages = {341--379},
    posted-at = {2008-03-08 00:50:22},
    priority = {2},
    title = {Recent Advances in Hierarchical Reinforcement Learning},
    url = {http://dx.doi.org/10.1023/A:1025696116075},
    volume = {13},
    year = {2003}
}

@misc{marthi:ijcai05,
    abstract = {We consider applying hierarchical reinforcement learning techniques to problems in which an agent has several effectors to control simultaneously. We argue that the kind of prior knowledge one typically has about such problems is best expressed using a multithreaded partial program, and present concurrent ALisp, a language for specifying such partial programs. We describe algorithms for learning and acting with concurrent ALisp that can be efficient even when there are exponentially many joint...},
    author = {Marthi, Bhaskara and Latham, David and Russell, Stuart and Guestrin, Carlos},
    citeulike-article-id = {2487616},
    citeulike-linkout-0 = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.61.2343},
    keywords = {heirarchical\_learning, learning},
    posted-at = {2008-03-08 00:49:08},
    priority = {2},
    title = {Concurrent Hierarchical Reinforcement Learning},
    url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.61.2343}
}

@phdthesis{citeulike:2487613,
    abstract = {Adviser-Stuart Russell},
    address = {Berkeley, CA, USA},
    author = {Marthi, Bhaskara M.},
    citeulike-article-id = {2487613},
    citeulike-linkout-0 = {http://portal.acm.org/citation.cfm?id=1293150},
    keywords = {heirarchical\_learning, learning},
    posted-at = {2008-03-08 00:46:05},
    priority = {2},
    publisher = {University of California at Berkeley},
    title = {Concurrent hierarchical reinforcement learning},
    url = {http://portal.acm.org/citation.cfm?id=1293150},
    year = {2006}
}

@article{hintonEtAl2006:fastlearning,
    abstract = {We show how to use "complementary priors" to eliminate the explaining-away effects that make inference difficult in densely connected belief nets that have many hidden layers. Using complementary priors, we derive a fast, greedy algorithm that can learn deep, directed belief networks one layer at a time, provided the top two layers form an undirected associative memory. The fast, greedy algorithm is used to initialize a slower learning procedure that fine-tunes the weights using a contrastive version of the wake-sleep algorithm. After fine-tuning, a network with three hidden layers forms a very good generative model of the joint distribution of handwritten digit images and their labels. This generative model gives better digit classification than the best discriminative learning algorithms. The low-dimensional manifolds on which the digits lie are modeled by long ravines in the free-energy landscape of the top-level associative memory, and it is easy to explore these ravines by using the directed connections to display what the associative memory has in mind.},
    address = {Cambridge, MA, USA},
    author = {Hinton, Geoffrey E. and Osindero, Simon and Teh, Yee W.},
    citeulike-article-id = {2458377},
    citeulike-linkout-0 = {http://portal.acm.org/citation.cfm?id=1161603.1161605},
    citeulike-linkout-1 = {http://dx.doi.org/10.1162/neco.2006.18.7.1527},
    doi = {10.1162/neco.2006.18.7.1527},
    issn = {0899-7667},
    journal = {Neural Comput.},
    keywords = {heirarchical\_learning, learning},
    month = {July},
    number = {7},
    pages = {1527--1554},
    posted-at = {2008-03-08 00:44:17},
    priority = {2},
    publisher = {MIT Press},
    title = {A fast learning algorithm for deep belief nets},
    url = {http://dx.doi.org/10.1162/neco.2006.18.7.1527},
    volume = {18},
    year = {2006}
}

@inproceedings{dietterich2000:maxq,
    address = {London, UK},
    author = {Dietterich, Thomas G.},
    booktitle = {SARA '02: Proc. of the 4th Intl. Symposium on Abstraction, Reformulation, and Approximation},
    citeulike-article-id = {2468404},
    citeulike-linkout-0 = {http://portal.acm.org/citation.cfm?id=668653},
    isbn = {3540678395},
    keywords = {heirarchical\_learning, qlearning, reinforcement\_learning},
    pages = {26--44},
    posted-at = {2008-03-05 00:40:42},
    priority = {0},
    publisher = {Springer-Verlag},
    title = {An Overview of MAXQ Hierarchical Reinforcement Learning},
    url = {http://portal.acm.org/citation.cfm?id=668653},
    year = {2000}
}

@inproceedings{mehtaEtAl2007:HerirarchyInduction,
    author = {Mehta, Neville and Wynkoop, Mike and Ray, Soumya and Tadepalli, Prasad and Dietterich, Tom},
    booktitle = {NIPS},
    citeulike-article-id = {2363975},
    howpublished = {Workshop},
    keywords = {heirarchical\_learning, maxq, nips, q-learning},
    month = {December},
    posted-at = {2008-02-11 22:08:06},
    priority = {0},
    title = {Automatic Induction of MAXQ Hierarchies},
    year = {2007}
}

@article{oreilly2006:bioBasedCognition,
    abstract = {Computer models based on the detailed biology of the brain can help us understand the myriad complexities of human cognition and intelligence. Here, we review models of the higher level aspects of human intelligence, which depend critically on the prefrontal cortex and associated subcortical areas. The picture emerging from a convergence of detailed mechanistic models and more abstract functional models represents a synthesis between analog and digital forms of computation. Specifically, the need for robust active maintenance and rapid updating of information in the prefrontal cortex appears to be satisfied by bistable activation states and dynamic gating mechanisms. These mechanisms are fundamental to digital computers and may be critical for the distinctive aspects of human intelligence.},
    address = {Department of Psychology, University of Colorado Boulder, Boulder, CO 80309, USA. oreilly@psych.colorado.edu},
    author = {O'Reilly, R. C.},
    citeulike-article-id = {890979},
    citeulike-linkout-0 = {http://dx.doi.org/10.1126/science.1127242},
    citeulike-linkout-1 = {http://www.sciencemag.org/cgi/content/abstract/314/5796/91},
    citeulike-linkout-2 = {http://view.ncbi.nlm.nih.gov/pubmed/17023651},
    citeulike-linkout-3 = {http://www.hubmed.org/display.cgi?uids=17023651},
    day = {6},
    doi = {10.1126/science.1127242},
    issn = {1095-9203},
    journal = {Science},
    keywords = {cognition, computational\_cognition},
    month = {October},
    number = {5796},
    pages = {91--94},
    posted-at = {2008-02-04 21:47:11},
    priority = {2},
    title = {Biologically based computational models of high-level cognition.},
    url = {http://dx.doi.org/10.1126/science.1127242},
    volume = {314},
    year = {2006}
}

@phdthesis{coen2006:multimodalDynamics,
    author = {Coen, Michael H.},
    citeulike-article-id = {2331001},
    keywords = {cognition, computational\_cognition, selfsupervised\_learning},
    posted-at = {2008-02-04 21:40:28},
    priority = {2},
    school = {Massachusetts Institute of Technology},
    title = {Multimodal Dynamics: Self-Supervised Learning in Perceptual and Motor Systems.},
    year = {2006}
}

@article{granger2005:brainEngines,
    address = {Menlo Park, CA, USA},
    author = {Granger, Richard},
    citeulike-article-id = {2330987},
    citeulike-linkout-0 = {http://portal.acm.org/citation.cfm?id=1167633},
    issn = {0738-4602},
    journal = {AI Mag.},
    keywords = {cognition, computational\_cognition},
    month = {July},
    number = {2},
    pages = {15--32},
    posted-at = {2008-02-04 21:35:38},
    priority = {2},
    publisher = {American Association for Artificial Intelligence},
    title = {Engines of the brain: the computational instruction set of human cognition},
    url = {http://portal.acm.org/citation.cfm?id=1167633},
    volume = {27},
    year = {2006}
}

@article{stevensonWilks2001:WSD,
    abstract = {Word sense disambiguation (WSD) is a computational linguistics task likely to benefit from the tradition of combining different knowledge sources in artificial in telligence research. An important step in the exploration of this hypothesis is to determine which linguistic knowledge sources are most useful and whether their combination leads to improved results. We present a sense tagger which uses several knowledge sources. Tested accuracy exceeds 94\% on our evaluation corpus.Our system attempts to disambiguate all content words in running text rather than limiting itself to treating a restricted vocabulary of words. It is argued that this approach is more likely to assist the creation of practical systems},
    address = {Cambridge, MA, USA},
    author = {Stevenson, Mark and Wilks, Yorick},
    citeulike-article-id = {1374557},
    citeulike-linkout-0 = {http://portal.acm.org/citation.cfm?id=972656},
    citeulike-linkout-1 = {http://dx.doi.org/10.1162/089120101317066104},
    doi = {10.1162/089120101317066104},
    issn = {0891-2017},
    journal = {Comput. Linguist.},
    keywords = {word-sense-dissambiguation},
    month = {September},
    number = {3},
    pages = {321--349},
    posted-at = {2007-12-29 00:10:37},
    priority = {2},
    publisher = {MIT Press},
    title = {The interaction of knowledge sources in word sense disambiguation},
    url = {http://dx.doi.org/10.1162/089120101317066104},
    volume = {27},
    year = {2001}
}

@book{citeulike:1104130,
    author = {Altintas, Ergin and Karsligil, Elif and Coskun, Vedat},
    citeulike-article-id = {1104130},
    citeulike-linkout-0 = {http://dx.doi.org/10.1007/11569596\_65},
    doi = {10.1007/11569596\_65},
    journal = {: Computer and Information Sciences - ISCIS 2005},
    keywords = {word-sense-dissambiguation},
    pages = {626--635},
    posted-at = {2007-12-29 00:05:04},
    priority = {2},
    title = {The Effect of Windowing in Word Sense Disambiguation},
    url = {http://dx.doi.org/10.1007/11569596\_65},
    year = {2005}
}

@misc{pedersenEtAl2005:WSD,
    abstract = {This article presents a method of word sense disambiguation that assigns a target word the sense that is most related to the senses of its neighboring words. We explore the use of measures of similarity and relatedness that are based on finding paths in a concept network, information content derived from a large corpus, and word sense glosses. We observe that measures of relatedness are useful sources of information for disambiguation, and in particular we find that two gloss based measures...},
    author = {Pedersen, Ted and Banerjee, Satanjeev and Patwardhan, Siddharth},
    citeulike-article-id = {160044},
    citeulike-linkout-0 = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.6.2112},
    keywords = {word-sense-dissambiguation},
    posted-at = {2007-12-28 23:16:19},
    priority = {2},
    title = {Maximizing Semantic Relatedness to Perform Word Sense Disambiguation},
    url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.6.2112}
}

@inproceedings{jonesEtAl2002:tarantula,
    abstract = {One of the most expensive and time-consuming components of the debugging process is locating the errors or faults. To locate faults, developers must identify statements involved in failures and select suspicious statements that might contain faults. This paper presents a new technique that uses visualization to assist with these tasks. The technique uses color to visually map the participation of each program statement in the outcome of the execution of the program with a test suite, consisting of both passed and failed test cases. Based on this visual mapping, a user can inspect the statements in the program, identify statements involved in failures, and locate potentially faulty statements. The paper also describes a prototype tool that implements our technique along with a set of empirical studies that use the tool for evaluation of the technique. The empirical studies show that, for the subject we studied, the technique can be effective in helping a user locate faults in a program.},
    address = {New York, NY, USA},
    author = {Jones, James A. and Harrold, Mary J. and Stasko, John},
    booktitle = {ICSE '02: Proc. of the 24th Intl. Conf. on Software Engineering},
    citeulike-article-id = {472852},
    citeulike-linkout-0 = {http://portal.acm.org/citation.cfm?id=581397},
    citeulike-linkout-1 = {http://dx.doi.org/10.1145/581339.581397},
    doi = {10.1145/581339.581397},
    isbn = {1-58113-472-X},
    keywords = {fault\_localization, icse02, visualization},
    location = {Orlando, Florida},
    pages = {467--477},
    posted-at = {2007-12-14 22:02:34},
    priority = {2},
    publisher = {ACM},
    title = {Visualization of test information to assist fault localization},
    url = {http://dx.doi.org/10.1145/581339.581397},
    year = {2002}
}

@article{ruthruffEtAl2006:fl,
    abstract = {End-user programmers develop more software than any other group of programmers, using software authoring devices such as multimedia simulation builders, e-mail filtering editors, by-demonstration macro builders, and spreadsheet environments. Despite this, there has been only a little research on finding ways to help these programmers with the dependability of the software they create. We have been working to address this problem in several ways, one of which includes supporting end-user debugging activities through interactive fault localization techniques. This paper investigates fault localization techniques in the spreadsheet domain, the most common type of end-user programming environment. We investigate a technique previously described in the research literature and two new techniques. We present the results of an empirical study to examine the impact of two individual factors on the effectiveness of fault localization techniques. Our results reveal several insights into the contributions such techniques can make to the end-user debugging process and highlight key issues of interest to researchers and practitioners who may design and evaluate future fault localization techniques.},
    author = {Ruthruff, Joseph R. and Burnett, Margaret and Rothermel, Gregg},
    booktitle = {Transactions on Software Engineering},
    citeulike-article-id = {2120814},
    citeulike-linkout-0 = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=1628969},
    journal = {Transactions on Software Engineering},
    keywords = {debugging, end\_user\_programmers, fault\_localization, visualization},
    number = {4},
    pages = {213--239},
    posted-at = {2007-12-14 21:57:18},
    priority = {1},
    title = {Interactive fault localization techniques in a spreadsheet environment},
    url = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=1628969},
    volume = {32},
    year = {2006}
}

@techreport{ScaffidiEtAl2005:90mil,
    author = {Scaffidi, Christopher and Shaw, Mary and Myers, Brad},
    citeulike-article-id = {2109962},
    institution = {Carnegie Mellon University},
    journal = {CMU-HCII-05-100},
    keywords = {end\_user\_programmers},
    posted-at = {2007-12-14 02:28:03},
    priority = {2},
    title = {The ``55M End-User Programmers'' Estimate Revisited},
    year = {2005}
}

@article{leeAndSee2004:trust,
    abstract = {Automation is often problematic because people fail to rely upon it appropriately. Because people respond to technology socially, trust influences reliance on automation. In particular, trust guides reliance when complexity and unanticipated situations make a complete understanding of the automation impractical. This review considers trust from the organizational, sociological, interpersonal, psychological, and neurological perspectives. It considers how the context, automation characteristics, and cognitive processes affect the appropriateness of trust. The context in which the automation is used influences automation performance and provides a goal-oriented perspective to assess automation characteristics along a dimension of attributional abstraction. These characteristics can influence trust through analytic, analogical, and affective processes. The challenges of extrapolating the concept of trust in people to trust in automation are discussed. A conceptual model integrates research regarding trust in automation and describes the dynamics of trust, the role of context, and the influence of display characteristics. Actual or potential applications of this research include improved designs of systems that require people to manage imperfect automation.},
    address = {Department of Mechanical and Industrial Engineering, University of Iowa, Iowa City, IA 52245, USA. jdlee@engineering.uiowa.edu},
    author = {Lee, John D. and See, Katrina A.},
    citeulike-article-id = {1313699},
    citeulike-linkout-0 = {http://view.ncbi.nlm.nih.gov/pubmed/15151155},
    citeulike-linkout-1 = {http://www.hubmed.org/display.cgi?uids=15151155},
    issn = {0018-7208},
    journal = {Human Factors},
    keywords = {trust},
    number = {1},
    pages = {50--80},
    posted-at = {2007-12-13 23:53:39},
    priority = {2},
    title = {Trust in automation: designing for appropriate reliance.},
    url = {http://view.ncbi.nlm.nih.gov/pubmed/15151155},
    volume = {46},
    year = {2004}
}

@inproceedings{citeulike:2096117,
    address = {New York, NY, USA},
    author = {Brodie, Carolyn A. and Karat, Clare-Marie and Karat, John},
    booktitle = {SOUPS '06: Proc. of the second symposium on Usable privacy and security},
    citeulike-article-id = {2096117},
    citeulike-linkout-0 = {http://portal.acm.org/citation.cfm?id=1143120.1143123},
    citeulike-linkout-1 = {http://dx.doi.org/10.1145/1143120.1143123},
    doi = {10.1145/1143120.1143123},
    isbn = {1595934480},
    keywords = {nlp, policy\_rules},
    pages = {8--19},
    posted-at = {2007-12-12 00:41:41},
    priority = {2},
    publisher = {ACM},
    title = {An empirical study of natural language parsing of privacy policy rules using the SPARCLE policy workbench},
    url = {http://dx.doi.org/10.1145/1143120.1143123},
    year = {2006}
}

@inproceedings{goanEtAl1996:recognizeWeb,
    author = {Goan, Terrance and Benson, Nels and Etzioni, Oren},
    booktitle = {Proc. of the AAAI Spring Symposium on Machine Learning in Information Access},
    citeulike-article-id = {2091049},
    comment = {http://citeseer.ist.psu.edu/40209.html},
    keywords = {information\_retrieval},
    posted-at = {2007-12-11 18:46:42},
    priority = {2},
    title = {A grammar inference algorithm for the World Wide Web},
    year = {1996}
}

@inproceedings{goan1997:supUser,
    address = {London, UK},
    author = {Goan, Terrance},
    booktitle = {Selected Papers from the Symposium on Conceptual Modeling, Current Issues and Future Directions},
    citeulike-article-id = {2091033},
    citeulike-linkout-0 = {http://portal.acm.org/citation.cfm?id=726254},
    isbn = {3540659269},
    keywords = {information\_retrieval, knowledge\_discovery},
    pages = {100--104},
    posted-at = {2007-12-11 18:40:03},
    priority = {2},
    publisher = {Springer-Verlag},
    title = {Supporting the User: Conceptual Modeling \& Knowledge Discovery},
    url = {http://portal.acm.org/citation.cfm?id=726254},
    year = {1999}
}

@article{citeulike:2091031,
    address = {Menlo Park, CA, USA},
    author = {Charniak, Eugene},
    citeulike-article-id = {2091031},
    citeulike-linkout-0 = {http://portal.acm.org/citation.cfm?id=122716},
    issn = {0738-4602},
    journal = {AI Mag.},
    keywords = {bayesian},
    number = {4},
    pages = {50--63},
    posted-at = {2007-12-11 18:39:19},
    priority = {2},
    publisher = {American Association for Artificial Intelligence},
    title = {Bayesian networks without tears: making Bayesian networks more accessible to the probabilistically unsophisticated},
    url = {http://portal.acm.org/citation.cfm?id=122716},
    volume = {12},
    year = {1991}
}

@article{citeulike:1263946,
    address = {New York, NY, USA},
    author = {Goan, Terrance},
    citeulike-article-id = {1263946},
    citeulike-linkout-0 = {http://portal.acm.org/citation.cfm?id=306569},
    citeulike-linkout-1 = {http://dx.doi.org/10.1145/306549.306569},
    doi = {10.1145/306549.306569},
    issn = {0001-0782},
    journal = {Commun. ACM},
    month = {July},
    number = {7},
    pages = {46--52},
    posted-at = {2007-12-11 18:38:24},
    priority = {2},
    publisher = {ACM Press},
    title = {A cop on the beat: collecting and appraising intrusion evidence},
    url = {http://dx.doi.org/10.1145/306549.306569},
    volume = {42},
    year = {1999}
}

@inproceedings{citeulike:2091019,
    address = {New York, NY, USA},
    author = {Ko, Calvin and Frincke, Deborah A. and Goan, Terrance and Heberlein, Todd and Levitt, Karl and Mukherjee, Biswanath and Wee, Christopher},
    booktitle = {CCS '93: Proc. of the 1st ACM conf. on Computer and communications security},
    citeulike-article-id = {2091019},
    citeulike-linkout-0 = {http://portal.acm.org/citation.cfm?id=168608},
    citeulike-linkout-1 = {http://dx.doi.org/10.1145/168588.168608},
    doi = {10.1145/168588.168608},
    isbn = {0897916298},
    pages = {154--164},
    posted-at = {2007-12-11 18:37:56},
    priority = {2},
    publisher = {ACM},
    title = {Analysis of an algorithm for distributed recognition and accountability},
    url = {http://dx.doi.org/10.1145/168588.168608},
    year = {1993}
}

@inproceedings{goan1998:intelDiscovery,
    author = {Goan, Terrance},
    booktitle = {Proc. of the 6th INFORMS Computer Science Technical Section Conf. on Computer Science and Operations Research: Recent Advances in the Interface},
    citeulike-article-id = {2090997},
    keywords = {information\_retrieval, knowledge\_discovery},
    posted-at = {2007-12-11 18:29:00},
    priority = {2},
    title = {An Active and Intelligent Tool for Knowledge Discovery},
    year = {1998}
}

@inproceedings{goanAndSpencer1999:discoveryViz,
    author = {Goan, Terrance and Spencer, Laurie},
    booktitle = {Proc. of the 2nd Intl. Conf. on Information Fusion},
    citeulike-article-id = {2090993},
    keywords = {visualization},
    posted-at = {2007-12-11 18:26:32},
    priority = {2},
    title = {Supporting Thorough Knowledge Discovery with Data-Aware Visualizations},
    year = {1999}
}

@inproceedings{bachrachAndPlayford2001:jse,
    author = {Bachrach, Jonthan and Playford, Keith},
    booktitle = {Proc. of the 16th ACM SIGPLAN conf. on Object oriented programming, systems, languages, and applications},
    citeulike-article-id = {1154},
    citeulike-linkout-0 = {http://portal.acm.org/citation.cfm?id=504285},
    citeulike-linkout-1 = {http://dx.doi.org/10.1145/504282.504285},
    doi = {10.1145/504282.504285},
    issn = {0362-1340},
    keywords = {java, macros, software\_engineering},
    month = {November},
    number = {11},
    pages = {31--42},
    posted-at = {2007-12-11 00:02:36},
    priority = {0},
    publisher = {ACM Press},
    title = {The Java syntactic extender (JSE)},
    url = {http://dx.doi.org/10.1145/504282.504285},
    volume = {36},
    year = {2001}
}

@article{eick1998:parallel,
    address = {Cambridge, MA},
    author = {Eick, S.},
    citeulike-article-id = {2075749},
    editor = {Stasko, J. and Dominigue, J. and Brown, M. and Price, B.},
    keywords = {software\_engineering, visualization},
    pages = {315--328},
    posted-at = {2007-12-08 01:52:06},
    priority = {2},
    publisher = {MIT Press},
    title = {Maintenance of large systems},
    year = {1998}
}

@article{heath1998:parallel,
    address = {Cambridge, MA},
    author = {Heath, M. and Malony, A. and Rover, D.},
    booktitle = {Software Visualization--Programming as a Multimedia Experience},
    citeulike-article-id = {2075744},
    editor = {Stasko, J. and Dominigue, J. and Brown, M. and Price, B.},
    keywords = {visualization},
    pages = {347--365},
    posted-at = {2007-12-08 01:49:05},
    priority = {2},
    publisher = {MIT Press},
    title = {Visualization for parallel performance evaluation and optimization},
    year = {1998}
}

@book{kimelmanEtAl1998:realWorld,
    address = {Cambridge, MA},
    author = {Kimelman, Doug and Rosenburg, Bryan and Roth, Tova},
    booktitle = {Software Visualization--Programming as a Multimedia Experience},
    citeulike-article-id = {2075737},
    editor = {Stasko, John T. and Domingue, John B. and Brown, Marc H. and Price, Blaine A.},
    keywords = {visualization},
    pages = {293--314},
    posted-at = {2007-12-08 01:45:46},
    priority = {2},
    publisher = {MIT Press},
    title = {Visualization of Dynamics in Real World Software Systems},
    year = {1998}
}

@article{burnett2001:Forms3,
    address = {New York, NY, USA},
    author = {Burnett, Margaret and Atwood, John and Djang, Rebecca W. and Reichwein, James and Gottfried, Herkimer and Yang, Sherry},
    citeulike-article-id = {2075727},
    citeulike-linkout-0 = {http://portal.acm.org/citation.cfm?id=968487},
    issn = {0956-7968},
    journal = {J. Funct. Program.},
    keywords = {end\_user\_programmers, software\_engineering, visualization},
    month = {March},
    number = {2},
    pages = {155--206},
    posted-at = {2007-12-08 01:41:55},
    priority = {2},
    publisher = {Cambridge University Press},
    title = {Forms/3: A first-order visual language to explore the boundaries of the spreadsheet paradigm},
    url = {http://portal.acm.org/citation.cfm?id=968487},
    volume = {11},
    year = {2001}
}

@article{hegerEtAl1998:cocoa,
    abstract = {Cocoa is a visual programming language for children. Cocoa attended the Visual Programming Challenge '97 for two reasons: to show that a fairly complex task could be implemented in Cocoa, and to test Cocoa's third-party extensions mechanism. The challenge was to navigate a Lego robot over an arbitrary track of Lego street tiles, and to create a map of the entire layout. Cocoa not only provided a solution but also won the competition. We introduce Cocoa and present our solution to the VPC '97.},
    author = {Heger, Nikolaus and Cypher, Allen and Smith, David C.},
    citeulike-article-id = {2075717},
    citeulike-linkout-0 = {http://dx.doi.org/10.1006/jvlc.1998.0079},
    citeulike-linkout-1 = {http://www.sciencedirect.com/science/article/B6WMM-45J5BN3-X/2/26fe70abe3938975adec65f51320c9df},
    doi = {10.1006/jvlc.1998.0079},
    journal = {Journal of Visual Languages \& Computing},
    keywords = {visualization},
    month = {April},
    number = {2},
    pages = {151--169},
    posted-at = {2007-12-08 01:38:57},
    priority = {2},
    title = {Cocoa at the Visual Programming Challenge},
    url = {http://dx.doi.org/10.1006/jvlc.1998.0079},
    volume = {9},
    year = {1998}
}

@book{liebermanAndFry1998:ZStep95,
    address = {Cambridge, MA--London},
    author = {Lieberman, Henry and Fry, Christoper},
    booktitle = {Software Visualization--Programming as a Multimedia Experience},
    citeulike-article-id = {2075714},
    editor = {Stasko, John and Domingue, John and Brown, Marc H. and Price, Blaine A.},
    keywords = {visualization},
    pages = {277--292},
    posted-at = {2007-12-08 01:36:36},
    priority = {2},
    publisher = {MIT Press},
    title = {ZStep 95: A reversible,
animated source code stepper},
    year = {1998}
}

@article{notkin1985:gandalf,
    author = {Notkin, D. and Ellison, R. and Kaiser, G. and Kant, E. and Habermann, A. and Ambriola, V. and Montanegero, C.},
    citeulike-article-id = {2075699},
    journal = {Journal of Systems and Software},
    month = {May},
    number = {2},
    posted-at = {2007-12-08 01:32:47},
    priority = {2},
    title = {Special issue on the GANDALF project},
    volume = {5},
    year = {1985}
}

@book{reiss1998:visualization,
    address = {Cambridge, MA},
    author = {Reiss, S.},
    booktitle = {Software Visualization: Programming as a Multimedia Experience},
    citeulike-article-id = {2075681},
    editor = {Stasko, J. and Domingue, J. and Brown, M. and Price, B.},
    keywords = {visualization},
    pages = {259--276},
    posted-at = {2007-12-08 01:27:02},
    priority = {2},
    publisher = {MIT Press},
    title = {Visualization for software engineering--programming environments,},
    year = {1998}
}

@article{Teitelbaum1981:cps,
    address = {New York, NY, USA},
    author = {Teitelbaum, Tim and Reps, Thomas},
    citeulike-article-id = {2075692},
    citeulike-linkout-0 = {http://portal.acm.org/citation.cfm?id=358755},
    citeulike-linkout-1 = {http://dx.doi.org/10.1145/358746.358755},
    doi = {10.1145/358746.358755},
    issn = {0001-0782},
    journal = {Commun. ACM},
    keywords = {visualization},
    month = {September},
    number = {9},
    pages = {563--573},
    posted-at = {2007-12-08 01:25:21},
    priority = {2},
    publisher = {ACM},
    title = {The Cornell program synthesizer: a syntax-directed programming environment},
    url = {http://dx.doi.org/10.1145/358746.358755},
    volume = {24},
    year = {1981}
}

@article{reiss1984:pecan,
    abstract = {This paper describes the user's view of the PECAN family of program development systems. PECAN is a program development system generator for algebraic programming languages. The program development systems it produces support multiple views of the user's program, its semantics, and its execution. The program views include a syntax-directed editor, a declaration editor, and a structured flow graph editor. The semantic views include expression trees, data type diagrams, flow graphs, and the symbol table. Execution views show the program in action and the stack contents as the program executes. PECAN is designed to make effective use of powerful personal machines with high-resolution graphics displays and is currently implemented on APOLLO workstations.},
    address = {New York, NY, USA},
    author = {Reiss, Steven P.},
    booktitle = {SDE 1: Proc. of the first ACM SIGSOFT/SIGPLAN software engineering symposium on Practical software development environments},
    citeulike-article-id = {2075683},
    citeulike-linkout-0 = {http://portal.acm.org/citation.cfm?id=808246},
    citeulike-linkout-1 = {http://dx.doi.org/10.1145/800020.808246},
    doi = {10.1145/800020.808246},
    issn = {0362-1340},
    journal = {SIGPLAN Not.},
    keywords = {software\_design, software\_engineering, software\_maintenance, visualization},
    number = {5},
    pages = {30--41},
    posted-at = {2007-12-08 01:21:05},
    priority = {2},
    publisher = {ACM},
    title = {Graphical program development with PECAN program development systems},
    url = {http://dx.doi.org/10.1145/800020.808246},
    volume = {19},
    year = {1984}
}

@inproceedings{reiss2003:visualization,
    address = {New York, NY, USA},
    author = {Reiss, Steven P.},
    booktitle = {SoftVis '03: Proc. of the 2003 ACM symposium on Software visualization},
    citeulike-article-id = {2075673},
    citeulike-linkout-0 = {http://portal.acm.org/citation.cfm?id=774842},
    citeulike-linkout-1 = {http://dx.doi.org/10.1145/774833.774842},
    doi = {10.1145/774833.774842},
    isbn = {1581136420},
    keywords = {visualization},
    pages = {57--ff},
    posted-at = {2007-12-08 01:13:22},
    priority = {2},
    publisher = {ACM},
    title = {Visualizing Java in action},
    url = {http://dx.doi.org/10.1145/774833.774842},
    year = {2003}
}

@inproceedings{atwoodEtAl1996:steering,
    address = {Washington, DC, USA},
    author = {Atwood, J. W. and Burnett, M. M. and Walpole, R. A. and Wilcox, E. M. and Yang, S.},
    booktitle = {VL '96: Proc. of the 1996 IEEE Symposium on Visual Languages},
    citeulike-article-id = {2075669},
    citeulike-linkout-0 = {http://portal.acm.org/citation.cfm?id=834339},
    isbn = {081867508X},
    keywords = {end\_user\_programmers, software\_engineering, visualization},
    posted-at = {2007-12-08 01:11:08},
    priority = {2},
    publisher = {IEEE Computer Society},
    title = {Steering Programs Via Time Travel},
    url = {http://portal.acm.org/citation.cfm?id=834339},
    year = {1996}
}

@techreport{tip1995:slicing,
    address = {Amsterdam, The Netherlands, The Netherlands},
    author = {Tip, Frank},
    citeulike-article-id = {504390},
    citeulike-linkout-0 = {http://portal.acm.org/citation.cfm?id=869354},
    keywords = {debugging, software\_engineering, software\_maintenance},
    posted-at = {2007-12-05 22:10:32},
    priority = {2},
    publisher = {CWI (Centre for Mathematics and Computer Science)},
    title = {A Survey of Program Slicing Techniques.},
    url = {http://portal.acm.org/citation.cfm?id=869354},
    year = {1994}
}

@article{chenAndCheung1997:onDicing,
    address = {New York, NY, USA},
    author = {Chen, T. Y. and Cheung, Y. Y.},
    citeulike-article-id = {2064158},
    citeulike-linkout-0 = {http://portal.acm.org/citation.cfm?id=250048},
    citeulike-linkout-1 = {http://dx.doi.org/10.1002/(SICI)1096-908X(199701)9:1\%3C33::AID-SMR143\%3E3.3.CO;2-W},
    doi = {10.1002/(SICI)1096-908X(199701)9:1\%3C33::AID-SMR143\%3E3.3.CO;2-W},
    journal = {Journal of Software Maintenance},
    keywords = {debugging, software\_engineering, software\_maintenance},
    number = {1},
    pages = {33--46},
    posted-at = {2007-12-05 22:03:26},
    priority = {2},
    publisher = {John Wiley \& Sons, Inc.},
    title = {On program dicing},
    url = {http://dx.doi.org/10.1002/(SICI)1096-908X(199701)9:1\%3C33::AID-SMR143\%3E3.3.CO;2-W},
    volume = {9},
    year = {1997}
}

@proceedings{blackwell2002:attention,
    abstract = {Research into the cognitive aspects of programming originated in the study of professional programmers (whether experts or students). Even "end-user" programmers in previous studies have often worked in organizations where programming is recognized to be demanding professional work-the term "power-user" recognizes this technical kudos. But as personal computers become widespread, and most new domestic appliances incorporate microprocessors, many people are engaging in programming-like activities in domestic or nonprofessional contexts. Such users often have less motivation and more obstacles to programming, meaning that they may be unlikely even to take the first steps. This paper analyses the generic nature of those first steps, and identifies the cognitive demands that characterize them. On the basis of this analysis the paper proposes the attention investment model, a cognitive model of programming that offers a consistent account of all programming behaviour, from professionals to end-users.},
    author = {Blackwell, A. F.},
    citeulike-article-id = {909080},
    citeulike-linkout-0 = {http://dx.doi.org/10.1109/HCC.2002.1046334},
    citeulike-linkout-1 = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=1046334},
    doi = {10.1109/HCC.2002.1046334},
    journal = {Human Centric Computing Languages and Environments, 2002. Proceedings. IEEE 2002 Symposia on},
    keywords = {attention\_investment, cognitive\_dimensions},
    pages = {2--10},
    posted-at = {2007-12-04 20:32:02},
    priority = {2},
    title = {First steps in programming: a rationale for attention investment models},
    url = {http://dx.doi.org/10.1109/HCC.2002.1046334},
    year = {2002}
}

@article{pirolli1999:foraging,
    abstract = {Information Foraging Theory is an approach to understanding how strategies and technologies for information seeking, gathering and consumption are adapted to the flux of information in the environment. The theory assumes that people, when possible, will modify their strategies or the structure of the environment to maximize their rate of gaining valuable information. Field studies inform the theory by illustrating that people do freely structure their environments and their strategies to yield high gains in information foraging. The theory is developed by (a) adaptation (rational) analysis of information foraging problems and (b) a detailed process model (ACT-IF). The adaptation analysis develops (a) information patch models, which deal with time allocation and information filtering and enrichment activities in environments in which information is encountered in clusters (e.g. bibliographic collections), (b) information scent models which address the identification of information value from proximal cues, and (c) information diet models which address decisions about the selection and pursuit of information items. ACT-IF is developed to instantiate these rational models and to fit the moment-by-moment behavior of people interacting with complex information technology. ACT-IF is a production system in which the information scent of bibliographic stimuli is calculated by spreading activation mechanisms. Time allocation and item selection heuristics make use of information sent to select production rules in ways that maximize information foraging activities.},
    author = {Pirolli, Peter and Card, Stuart},
    citeulike-article-id = {448},
    citeulike-linkout-0 = {http://citeseer.ist.psu.edu/pirolli99information.html},
    citeulike-linkout-1 = {http://citeseer.lcs.mit.edu/pirolli99information.html},
    citeulike-linkout-2 = {http://citeseer.ifi.unizh.ch/pirolli99information.html},
    citeulike-linkout-3 = {http://citeseer.comp.nus.edu.sg/pirolli99information.html},
    journal = {Psychological Review},
    keywords = {information\_foraging},
    number = {4},
    pages = {634--675},
    posted-at = {2007-12-04 20:21:22},
    priority = {2},
    title = {Information foraging},
    url = {http://citeseer.ist.psu.edu/pirolli99information.html},
    volume = {106},
    year = {1999}
}

@inproceedings{pirolli1997:computationalScent,
    address = {New York, NY, USA},
    author = {Pirolli, Peter},
    booktitle = {CHI '97: Proc. of the SIGCHI conf. on Human factors in computing systems},
    citeulike-article-id = {584277},
    citeulike-linkout-0 = {http://portal.acm.org/citation.cfm?id=258558},
    citeulike-linkout-1 = {http://dx.doi.org/10.1145/258549.258558},
    doi = {10.1145/258549.258558},
    isbn = {0897918029},
    keywords = {information\_foraging},
    pages = {3--10},
    posted-at = {2007-12-04 20:19:34},
    priority = {2},
    publisher = {ACM Press},
    title = {Computational models of information scent-following in a very large browsable text collection},
    url = {http://dx.doi.org/10.1145/258549.258558},
    year = {1997}
}

@book{pirolli2007:foraging,
    abstract = {{Although much of the hubris and hyperbole surrounding the 1990's Internet has softened to a reasonable level, the inexorable momentum of information growth continues unabated. This wealth of information provides resources for adapting to the problems posed by our increasingly complex world,<br>but the simple availability of more information does not guarantee its successful transformation into valuable knowledge that shapes, guides, and improves our activity. When faced with something like the analysis of sense-making behavior on the web, traditional research models tell us a lot about<br>learning and performance with browser operations, but very little about how people will actively navigate and search through information structures, what information they will choose to consume, and what conceptual models they will induce about the landscape of cyberspace.<br><br>Thus, it is fortunate that a new field of research, Adaptive Information Interaction (AII), is becoming possible. AII centers on the problems of understanding and improving human-information interaction. It is about how people will best shape themselves to their information environments, and how<br>information environments can best be shaped to people. Its roots lie in human-computer interaction (HCI), information retrieval, and the behavioral and social sciences.<br><br>This book is about Information Foraging Theory (IFT), a new theory in Adaptive Information Interaction that is one example of a recent flourish of theories in adaptationist psychology that draw upon evolutionary-ecological theory in biology. IFT assumes that people (indeed, all organisms) are<br>ecologically rational, and that human information-seeking mechanisms and strategies adapt the structure of the information environments in which they operate. Its main aim is to create technology that is better shaped to users. Information Foraging Theory will be of interest to student and<br>professional researchers in HCI and cognitive psychology.}},
    author = {Pirolli, Peter L. T.},
    citeulike-article-id = {1505954},
    citeulike-linkout-0 = {http://www.amazon.ca/exec/obidos/redirect?tag=citeulike09-20\&amp;path=ASIN/0195173325},
    citeulike-linkout-1 = {http://www.amazon.de/exec/obidos/redirect?tag=citeulike01-21\&amp;path=ASIN/0195173325},
    citeulike-linkout-2 = {http://www.amazon.fr/exec/obidos/redirect?tag=citeulike06-21\&amp;path=ASIN/0195173325},
    citeulike-linkout-3 = {http://www.amazon.co.uk/exec/obidos/ASIN/0195173325/citeulike00-21},
    citeulike-linkout-4 = {http://www.amazon.com/exec/obidos/redirect?tag=citeulike07-20\&path=ASIN/0195173325},
    citeulike-linkout-5 = {http://www.worldcat.org/isbn/0195173325},
    citeulike-linkout-6 = {http://books.google.com/books?vid=ISBN0195173325},
    citeulike-linkout-7 = {http://www.amazon.com/gp/search?keywords=0195173325\&index=books\&linkCode=qs},
    citeulike-linkout-8 = {http://www.librarything.com/isbn/0195173325},
    day = {16},
    howpublished = {Hardcover},
    isbn = {0195173325},
    keywords = {information\_foraging, information\_retrieval},
    month = {March},
    posted-at = {2007-12-04 20:17:33},
    priority = {2},
    publisher = {{Oxford University Press, USA}},
    title = {Information Foraging Theory: Adaptive Interaction with Information (Oxford Series in Human-Technology Interaction)},
    url = {http://www.amazon.com/exec/obidos/redirect?tag=citeulike07-20\&path=ASIN/0195173325},
    year = {2007}
}

@inproceedings{goan1994:errors,
    address = {Menlo Park, CA, USA},
    author = {Goan, Terrance and Etzioni, Oren},
    booktitle = {AAAI'94: Proc. of the twelfth national conf. on Artificial intelligence (vol. 2)},
    citeulike-article-id = {2057894},
    citeulike-linkout-0 = {http://portal.acm.org/citation.cfm?id=199480.199759},
    isbn = {0262611023},
    keywords = {debugging, software\_maintenance},
    posted-at = {2007-12-04 20:02:06},
    priority = {2},
    publisher = {American Association for Artificial Intelligence},
    title = {Learning about software errors via systematic experimentation},
    url = {http://portal.acm.org/citation.cfm?id=199480.199759},
    year = {1994}
}

@article{koEtAl2006:seeking,
    abstract = {Abstract—Much of software developers' time is spent understanding unfamiliar code. To better understand how developers gain this
understanding and how software development environments might be involved, a study was performed in which developers were given
an unfamiliar program and asked to work on two debugging tasks and three enhancement tasks for 70 minutes. The study found that
developers interleaved three activities. They began by searching for relevant code both manually and using search tools; however,
they based their searches on limited and misrepresentative cues in the code, environment, and executing program, often leading to
failed searches. When developers found relevant code, they followed its incoming and outgoing dependencies, often returning to it and
navigating its other dependencies; while doing so, however, Eclipse's navigational tools caused significant overhead. Developers
collected code and other information that they believed would be necessary to edit, duplicate, or otherwise refer to later by encoding it
in the interactive state of Eclipse's package explorer, file tabs, and scroll bars. However, developers lost track of relevant code as these
interfaces were used for other tasks, and developers were forced to find it again. These issues caused developers to spend, on
average, 35 percent of their time performing the mechanics of navigation within and between source files. These observations suggest
a new model of program understanding grounded in theories of information foraging and suggest ideas for tools that help developers
seek, relate, and collect information in a more effective and explicit manner.},
    address = {Piscataway, NJ, USA},
    author = {Ko, Andrew J. and Coblenz, Michael J. and Aung, Htet H.},
    booktitle = {Software Engineering, IEEE Transactions on},
    citeulike-article-id = {2057824},
    citeulike-linkout-0 = {http://portal.acm.org/citation.cfm?id=1248780},
    citeulike-linkout-1 = {http://dx.doi.org/10.1109/TSE.2006.116},
    citeulike-linkout-2 = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=4016573},
    doi = {10.1109/TSE.2006.116},
    issn = {0098-5589},
    journal = {IEEE Transactions on Software Engineering},
    keywords = {code\_navigation, information\_foraging},
    month = {December},
    number = {12},
    pages = {971--987},
    posted-at = {2007-12-04 19:20:38},
    priority = {2},
    publisher = {IEEE Press},
    title = {An Exploratory Study of How Developers Seek, Relate, and Collect Relevant Information during Software Maintenance Tasks},
    url = {http://dx.doi.org/10.1109/TSE.2006.116},
    volume = {32},
    year = {2006}
}

@inproceedings{myersEtAl2006:whylineUIqs,
    address = {New York, NY, USA},
    author = {Myers, Brad A. and Weitzman, David A. and Ko, Andrew J. and Chau, Duen H.},
    booktitle = {CHI '06: Proc. of the SIGCHI conf. on Human Factors in computing systems},
    citeulike-article-id = {977456},
    citeulike-linkout-0 = {http://portal.acm.org/citation.cfm?id=1124832},
    citeulike-linkout-1 = {http://dx.doi.org/10.1145/1124772.1124832},
    doi = {10.1145/1124772.1124832},
    isbn = {1595933727},
    keywords = {debugging},
    pages = {397--406},
    posted-at = {2007-12-04 18:46:08},
    priority = {2},
    publisher = {ACM Press},
    title = {Answering why and why not questions in user interfaces},
    url = {http://dx.doi.org/10.1145/1124772.1124832},
    year = {2006}
}

@article{burnettEtAl2002:wysiwyt,
    address = {Piscataway, NJ, USA},
    author = {Burnett, Margaret and Sheretov, Andrei and Ren, Bing and Rothermel, Gregg},
    citeulike-article-id = {2057736},
    citeulike-linkout-0 = {http://portal.acm.org/citation.cfm?id=570531},
    citeulike-linkout-1 = {http://dx.doi.org/10.1109/TSE.2002.1010060},
    doi = {10.1109/TSE.2002.1010060},
    issn = {0098-5589},
    journal = {IEEE Trans. Softw. Eng.},
    keywords = {end\_user\_programmers, software\_engineering, testing},
    month = {June},
    number = {6},
    pages = {576--594},
    posted-at = {2007-12-04 18:29:05},
    priority = {0},
    publisher = {IEEE Press},
    title = {Testing Homogeneous Spreadsheet Grids with the "What You See Is What You Test" Methodology},
    url = {http://dx.doi.org/10.1109/TSE.2002.1010060},
    volume = {28},
    year = {2002}
}

@inproceedings{subrahmaniyan2007:explainDbgtoUsers,
    address = {Washington, DC, USA},
    author = {Subrahmaniyan, Neeraja and Kissinger, Cory and Rector, Kyle and Inman, Derek and Kaplan, Jared and Beckwith, Laura and Burnett, Margaret},
    booktitle = {VLHCC '07: Proc. of the IEEE Symposium on Visual Languages and Human-Centric Computing (VL/HCC 2007)},
    citeulike-article-id = {2057723},
    citeulike-linkout-0 = {http://portal.acm.org/citation.cfm?id=1308174.1308311},
    citeulike-linkout-1 = {http://dx.doi.org/10.1109/VLHCC.2007.21},
    doi = {10.1109/VLHCC.2007.21},
    isbn = {0769529879},
    keywords = {debugging, software\_engineering},
    pages = {127--136},
    posted-at = {2007-12-04 18:21:26},
    priority = {3},
    publisher = {IEEE Computer Society},
    title = {Explaining Debugging Strategies to End-User Programmers},
    url = {http://dx.doi.org/10.1109/VLHCC.2007.21},
    year = {2007}
}

@proceedings{lawrance2007:foraging,
    abstract = {During maintenance, professional developers generate and test many hypotheses about program behavior, but they also spend much of their time navigating among classes and methods. Little is known, however, about how professional developers navigate source code and the extent to which their hypotheses relate to their navigation. A lack of understanding of these issues is a barrier to tools aiming to reduce the large fraction of time developers spend navigating source code. In this paper, we report on a study that makes use of information foraging theory to investigate how professional developers navigate source code during maintenance. Our results showed that information foraging theory was a significant predictor of the developers' maintenance behavior, and suggest how tools used during maintenance can build upon this result, simply by adding word analysis to their reasoning systems.},
    author = {Lawrance, Joseph and Bellamy, Rachel and Burnett, Margaret},
    booktitle = {Visual Languages and Human-Centric Computing, 2007. VL/HCC 2007. IEEE Symposium on},
    citeulike-article-id = {2057713},
    citeulike-linkout-0 = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=4351321},
    journal = {Visual Languages and Human-Centric Computing, 2007. VL/HCC 2007. IEEE Symposium on},
    keywords = {software\_engineering, software\_maintenance},
    pages = {15--22},
    posted-at = {2007-12-04 18:17:11},
    priority = {4},
    title = {Scents in Programs:Does Information Foraging Theory Apply to Program Maintenance?},
    url = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=4351321},
    year = {2007}
}

@inproceedings{patel-schneider2004:owl,
    abstract = {OWL is the new ontology language produced by the W3C Web Ontology Working Group. OWL is thus poised to be a major formalism for the design and dissemination of ontology information, particularly in the Semantic Web. OWL has inﬂuences from several communities, including the RDF community, the Description Logic community, and the frame community. These inﬂuences resulted in a wide variety of re- quirements on OWL, some of which appear to be conﬂicting. OWL contains innovative solutions to several of these appar- ent conﬂicts, but it has not been possible to completely satisfy all the desired requirements for OWL. The talk will describe the development and design of OWL, concentrating on what makes OWL important, the relation- ship of OWL to other formalisms, the place of OWL in the Semantic Web, the innovative solutions that were required in its design, and the impact of the conﬂicting requirements on OWL. I will propose a different foundation for the Semantic Web, one that I think would allow for easier and better devel- opment of new formalisms for the Semantic Web.},
    address = {Whistler, CA},
    author = {Patel-Schneider, Peter F.},
    booktitle = {Proc. of the Ninth Intl. Conf. on the Principles of Knowledge Representation and Reasoning},
    citeulike-article-id = {2053667},
    day = {2-5},
    keywords = {ontologies},
    month = {June},
    posted-at = {2007-12-04 01:48:03},
    priority = {2},
    title = {What is Owl (and why should I care)?},
    year = {2004}
}

@book{russelNorvig2003:aima,
    abstract = {\_Artificial Intelligence: A Modern Approach\_ introduces basic ideas in
artificial intelligence from the perspective of building intelligent agents,
which the authors define as "anything that can be viewed as perceiving its
environment through sensors and acting upon the environment through
effectors." This textbook is up-to-date and is organized using the latest
principles of good textbook design. It includes historical notes at the end of
every chapter, exercises, margin notes, a bibliography, and a competent index.
\_Artificial Intelligence: A Modern Approach\_ covers a wide array of material,
including first-order logic, game playing, knowledge representation, planning,
and reinforcement learning.},
    author = {Russell, Stuart and Norvig, Peter},
    citeulike-article-id = {113848},
    citeulike-linkout-0 = {http://www.amazon.ca/exec/obidos/redirect?tag=citeulike09-20\&amp;path=ASIN/0137903952},
    citeulike-linkout-1 = {http://www.amazon.de/exec/obidos/redirect?tag=citeulike01-21\&amp;path=ASIN/0137903952},
    citeulike-linkout-2 = {http://www.amazon.fr/exec/obidos/redirect?tag=citeulike06-21\&amp;path=ASIN/0137903952},
    citeulike-linkout-3 = {http://www.amazon.jp/exec/obidos/ASIN/0137903952},
    citeulike-linkout-4 = {http://www.amazon.co.uk/exec/obidos/ASIN/0137903952/citeulike00-21},
    citeulike-linkout-5 = {http://www.amazon.com/exec/obidos/redirect?tag=citeulike07-20\&path=ASIN/0137903952},
    citeulike-linkout-6 = {http://www.worldcat.org/isbn/0137903952},
    citeulike-linkout-7 = {http://books.google.com/books?vid=ISBN0137903952},
    citeulike-linkout-8 = {http://www.amazon.com/gp/search?keywords=0137903952\&index=books\&linkCode=qs},
    citeulike-linkout-9 = {http://www.librarything.com/isbn/0137903952},
    day = {30},
    edition = {2},
    howpublished = {Hardcover},
    isbn = {0137903952},
    keywords = {ai, book},
    month = {December},
    posted-at = {2007-12-04 01:39:10},
    priority = {0},
    publisher = {Prentice Hall},
    title = {Artificial Intelligence: A Modern Approach (2nd Edition)},
    url = {http://www.amazon.com/exec/obidos/redirect?tag=citeulike07-20\&path=ASIN/0137903952},
    year = {2002}
}

@inproceedings{sarawagi2002:activeLearning,
    abstract = {Deduplication is a key operation in integrating data from multiple sources. The main challenge in this task is designing a function that can resolve when a pair of records refer to the same entity in spite of various data inconsistencies. Most existing systems use hand-coded functions. One way to overcome the tedium of hand-coding is to train a classifier to distinguish between duplicates and non-duplicates. The success of this method critically hinges on being able to provide a  covering and challenging  set of training pairs that bring out the subtlety of deduplication function. This is non-trivial because it requires manually searching for various data inconsistencies between any two records spread apart in large lists.We present our design of a learning-based deduplication system that uses a novel method of interactively discovering challenging training pairs using  active learning.  Our experiments on real-life datasets show that active learning significantly reduces the number of instances needed to achieve high accuracy. We investigate various design issues that arise in building a system to provide interactive response, fast convergence, and interpretable output.},
    address = {New York, NY, USA},
    author = {Sarawagi, Sunita and Bhamidipaty, Anuradha},
    booktitle = {KDD '02: Proc. of the eighth ACM SIGKDD intl. conf. on Knowledge discovery and data mining},
    citeulike-article-id = {2053628},
    citeulike-linkout-0 = {http://portal.acm.org/citation.cfm?id=775087},
    citeulike-linkout-1 = {http://dx.doi.org/10.1145/775047.775087},
    doi = {10.1145/775047.775087},
    isbn = {1-58113-567-X},
    keywords = {active\_learning},
    location = {Edmonton, Alberta, Canada},
    pages = {269--278},
    posted-at = {2007-12-04 01:35:06},
    priority = {2},
    publisher = {ACM},
    title = {Interactive deduplication using active learning},
    url = {http://dx.doi.org/10.1145/775047.775087},
    year = {2002}
}

@inproceedings{reiserEtAl1985:lispITS,
    address = {Los Angeles, CA},
    author = {Reiser, B. J. and Anderson, J. R. and Farrell, R. G.},
    booktitle = {Proc. of the Ninth Intl. Joint Conf. on Artificial Intelligence},
    citeulike-article-id = {2053165},
    keywords = {intelligent\_tutoring},
    posted-at = {2007-12-03 23:14:20},
    priority = {2},
    title = {Dynamic student modeling in an intelligent tutor for Lisp programming},
    year = {1985}
}

@article{johnson1987:understanding,
    address = {Essex, UK},
    author = {Johnson, W. L.},
    citeulike-article-id = {2053138},
    citeulike-linkout-0 = {http://portal.acm.org/citation.cfm?id=78804},
    citeulike-linkout-1 = {http://dx.doi.org/10.1016/0004-3702(90)90094-G},
    doi = {10.1016/0004-3702(90)90094-G},
    issn = {0004-3702},
    journal = {Artif. Intell.},
    keywords = {intelligent\_tutoring},
    month = {February},
    number = {1},
    pages = {51--97},
    posted-at = {2007-12-03 23:06:10},
    priority = {2},
    publisher = {Elsevier Science Publishers Ltd.},
    title = {Understanding and debugging novice programs},
    url = {http://dx.doi.org/10.1016/0004-3702(90)90094-G},
    volume = {42},
    year = {1990}
}

@article{yangEtAl1997:rdbs,
    abstract = {A weakness of many interactive visual programming languages (VPLs) is their static representations. Lack of an adequate static representation places a heavy cognitive burden on a VPL s programmers, because they must remember potentially long dynamic sequences of screen displays in order to understand a previously written program. However, although this problem is widely acknowledged, research on how to design better static representations for interactive VPLs is still in its infancy. Building...},
    author = {Yang, S. and Burnett, Margaret M. and Dekoven, E. and Zloof, Moshe M.},
    citeulike-article-id = {2052195},
    citeulike-linkout-0 = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.37.4979},
    journal = {Journal of Visual Languages and Computing},
    keywords = {cognitive\_dimensions, jvlc97, software\_design, software\_engineering, visualization},
    number = {5-6},
    pages = {563--599},
    posted-at = {2007-12-03 19:46:01},
    priority = {0},
    title = {Representation Design Benchmarks: A Design-Time Aid for VPL Navigable Static Representations},
    url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.37.4979},
    volume = {8},
    year = {1997}
}

@book{blackwellEtAl2001:cdsOfNotations,
    author = {Blackwell, A. F. and Britton, C. and Cox, A. and Green, T. R. G. and Gurr, C. and Kadoda, G. and Kutar, M. S. and Loomes, M. and Nehaniv, C. L. and Petre, M. and Roast, C. and Roe, C. and Wong, A. and Young, R. M.},
    citeulike-article-id = {1168048},
    citeulike-linkout-0 = {http://www.springerlink.com/content/hrj8lgkhaq96v904},
    journal = {: Cognitive Technology: Instruments of Mind : 4th Intl. Conf., CI 2001, Warwick, UK, August 6-9, 2001, Proceedings},
    keywords = {cognitive\_dimensions},
    pages = {325+},
    posted-at = {2007-12-03 19:38:45},
    priority = {2},
    title = {Cognitive Dimensions of Notations: Design Tools for Cognitive Technology},
    url = {http://www.springerlink.com/content/hrj8lgkhaq96v904},
    year = {2001}
}

@article{green1996:cogDims,
    abstract = {[None]},
    author = {Green, T. R. G. and Petre, Marian},
    citeulike-article-id = {230123},
    citeulike-linkout-0 = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.54.3584},
    journal = {Journal of Visual Languages and Computing},
    keywords = {cognitive\_dimensions, software\_engineering, visualization},
    number = {2},
    pages = {131--174},
    posted-at = {2007-12-03 19:32:25},
    priority = {0},
    title = {Usability Analysis of Visual Programming Environments: A 'Cognitive Dimensions' Framework},
    url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.54.3584},
    volume = {7},
    year = {1996}
}

@inproceedings{greene1989:cogdims,
    address = {New York, NY, USA},
    author = {Green, T. R. G.},
    booktitle = {Proc. of the fifth conf. of the British Computer Society, Human-Computer Interaction Specialist Group on People and computers V},
    citeulike-article-id = {1168047},
    citeulike-linkout-0 = {http://portal.acm.org/citation.cfm?id=92968.93015},
    isbn = {0-521-38430-3},
    keywords = {cognitive\_dimensions, software\_design, software\_engineering, visualization},
    location = {Univ. of Nottingham},
    pages = {443--460},
    posted-at = {2007-12-03 19:31:12},
    priority = {2},
    publisher = {Cambridge University Press},
    title = {Cognitive dimensions of notations},
    url = {http://portal.acm.org/citation.cfm?id=92968.93015},
    year = {1989}
}

@inproceedings{ko2004:whyline,
    abstract = {Debugging is still among the most common and costly of programming activities. One reason is that current debugging tools do not directly support the inquisitive nature of the activity.  Interrogative Debugging  is a new debugging paradigm in which programmers can ask  why did  and even  why didn't  questions directly about their program's runtime failures. The  Whyline  is a prototype Interrogative Debugging interface for the Alice programming environment that visualizes answers in terms of runtime events directly relevant to a programmer's question. Comparisons of identical debugging scenarios from user tests with and without the Whyline showed that the Whyline reduced debugging time by nearly a factor of 8, and helped programmers complete 40\% more tasks.},
    address = {New York, NY, USA},
    author = {Ko, Andrew J. and Myers, Brad A.},
    booktitle = {CHI '04: Proc. of the SIGCHI conf. on Human factors in computing systems},
    citeulike-article-id = {671755},
    citeulike-linkout-0 = {http://portal.acm.org/citation.cfm?id=985712},
    citeulike-linkout-1 = {http://dx.doi.org/10.1145/985692.985712},
    doi = {10.1145/985692.985712},
    isbn = {1-58113-702-8},
    keywords = {chi04, debugging, end\_user\_programmers, fault\_localization, software\_engineering, visualization},
    location = {Vienna, Austria},
    pages = {151--158},
    posted-at = {2007-12-03 19:15:17},
    priority = {0},
    publisher = {ACM},
    title = {Designing the whyline: a debugging interface for asking questions about program behavior},
    url = {http://dx.doi.org/10.1145/985692.985712},
    year = {2004}
}

@inproceedings{walker1998:vizModels,
    address = {New York, NY, USA},
    author = {Walker, Robert J. and Murphy, Gail C. and Freeman-Benson, Bjorn and Wright, Darin and Swanson, Darin and Isaak, Jeremy},
    booktitle = {OOPSLA '98: Proc. of the 13th ACM SIGPLAN conf. on Object-oriented programming, systems, languages, and applications},
    citeulike-article-id = {1367665},
    citeulike-linkout-0 = {http://portal.acm.org/citation.cfm?id=286966},
    citeulike-linkout-1 = {http://dx.doi.org/10.1145/286936.286966},
    doi = {10.1145/286936.286966},
    issn = {0362-1340},
    keywords = {visualization},
    pages = {271--283},
    posted-at = {2007-12-03 19:09:58},
    priority = {0},
    publisher = {ACM Press},
    title = {Visualizing dynamic software system information through high-level models},
    url = {http://dx.doi.org/10.1145/286936.286966},
    year = {1998}
}

@inproceedings{lang2007:umlshort,
    address = {Washington, DC, USA},
    author = {Lange, Christian F. J. and Wijns, Martijn A. M. and Chaudron, Michel R. V.},
    booktitle = {HICSS '07: Proc. of the 40th Annual Hawaii Intl. Conf. on System Sciences},
    citeulike-article-id = {2033541},
    citeulike-linkout-0 = {http://portal.acm.org/citation.cfm?id=1255807},
    citeulike-linkout-1 = {http://dx.doi.org/10.1109/HICSS.2007.44},
    doi = {10.1109/HICSS.2007.44},
    keywords = {uml, visualization},
    posted-at = {2007-12-01 00:00:17},
    priority = {2},
    publisher = {IEEE Computer Society},
    title = {A Visualization Framework for Task-Oriented Modeling Using UML},
    url = {http://dx.doi.org/10.1109/HICSS.2007.44},
    year = {2007}
}

@article{price1993:taxonomy,
    author = {Price, Blaine A. and Baecker, Ronald M. and Small, Ian S.},
    citeulike-article-id = {2033442},
    journal = {Journal of Visual Languages and Computing},
    keywords = {software\_engineering, visualization},
    month = {September},
    number = {3},
    pages = {211--266},
    posted-at = {2007-11-30 23:43:26},
    priority = {0},
    title = {A principled taxonomy of software visualization},
    volume = {4},
    year = {1993}
}

@article{lang2007:umllong,
    address = {Orlando, FL, USA},
    author = {Lange, Christian F. J. and Wijns, Martijn A. M. and Chaudron, Michel R. V.},
    citeulike-article-id = {2033394},
    citeulike-linkout-0 = {http://portal.acm.org/citation.cfm?id=1290239},
    citeulike-linkout-1 = {http://dx.doi.org/10.1016/j.jvlc.2007.07.002},
    doi = {10.1016/j.jvlc.2007.07.002},
    journal = {J. Vis. Lang. Comput.},
    keywords = {uml, visualization},
    month = {August},
    number = {4},
    pages = {399--419},
    posted-at = {2007-11-30 23:35:59},
    priority = {1},
    publisher = {Academic Press, Inc.},
    title = {Supporting task-oriented modeling using interactive UML views},
    url = {http://dx.doi.org/10.1016/j.jvlc.2007.07.002},
    volume = {18},
    year = {2007}
}

@inproceedings{citeulike:962218,
    abstract = {Software visualization is the use of interactive computer graphics, typography, graphic design, animation, and cinematography to enhance the interface between the software engineer or the computer science student and their programs. Although several taxonomies of software visualization have been proposed, they use few dimensions and do not span the space of important distinctions between systems. We attempt to fill this gap in the literature by proposing a novel and systematic taxonomy of six...},
    author = {Price, Blaine A. and Small, Ian S. and Baecker, Ronald M.},
    booktitle = {Proc. 25th Hawaii Int. Conf. System Sciences},
    citeulike-article-id = {962218},
    citeulike-linkout-0 = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.28.8296},
    comment = {The journal version of this paper can be found here:
http://www.citeulike.org/user/creswick/article/2033442},
    keywords = {visualization},
    posted-at = {2007-11-30 23:34:30},
    priority = {0},
    title = {A Taxonomy of Software Visualization},
    url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.28.8296},
    year = {1992}
}

@proceedings{citeulike:488161,
    abstract = {A number of taxonomies to classify and categorize software visualization systems have been proposed in the past. Most notable are those presented by Price (1993) and Roman (1993). While these taxonomies are an accurate representation of software visualization issues, they are somewhat skewed with respect to current research areas on software visualization. We revisit this important work and propose a number of re-alignments with respect to addressing the software engineering tasks of large-scale development and maintenance. We propose a framework to emphasize the general tasks of understanding and analysis during development and maintenance of large-scale software systems. Five dimensions relating to the what, where, how, who, and why of software visualization make up this framework. The focus of this work is not so much as to classify software visualization system, but to point out the need for matching the method with the task. Finally, a number of software visualization systems are examined under our framework to highlight the particular problems each addresses.},
    author = {Maletic, J. I. and Marcus, A. and Collard, M. L.},
    citeulike-article-id = {488161},
    citeulike-linkout-0 = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=1019792},
    journal = {Visualizing Software for Understanding and Analysis, 2002. Proceedings. First Intl. Workshop on},
    keywords = {visualization},
    pages = {32--40},
    posted-at = {2007-11-30 23:31:53},
    priority = {2},
    title = {A task oriented view of software visualization},
    url = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=1019792},
    year = {2002}
}

@proceedings{citeulike:638252,
    abstract = {Visualizing software evolution is essential for identifying design erosion that has occurred over the past releases. Making evolutionary aspects explicit via visual representations can help the engineer to focus on particular software parts to identify such hot-spots. Although many tools exist that provide zooming-in and -out within the hierarchical decomposition of a software system, only very few allow an engineer to view a system through a kind of lens view. Our approach called EvoLens is a visualization approach for explorations of evolution data across multiple dimensions. EvoLens is based on temporal lens views. But the graphical representation of this visualization integrates enhanced zooming by navigating through software hierarchies with arbitrary selectable groups of software parts across module or package boundaries. EvoLens allows an engineer to define a focal point for the lens view and navigate along the time dimension by user-defined sliding time windows. The comprehension is supported by using color for metrics of classes. The EvoLens prototype tool has been developed and tested on basis of a large Java application consisting of 580000 LOC that was studied over 18 months of its evolution lifetime.},
    author = {Ratzinger, J. and Fischer, M. and Gall, H.},
    citeulike-article-id = {638252},
    citeulike-linkout-0 = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=1572314},
    journal = {Principles of Software Evolution, Eighth Intl. Workshop on},
    keywords = {visualization},
    pages = {103--112},
    posted-at = {2007-11-30 23:20:15},
    priority = {2},
    title = {EvoLens: lens-view visualizations of evolution data},
    url = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=1572314},
    year = {2005}
}

@article{citeulike:1295275,
    abstract = {The World Wide Web is increasingly becoming an integrated extension of users' computing environments, with content indexed and retrieved through Web browsers. Web browsers are increasingly being used as computer science curriculum delivery mechanism, for both books delivered as local content on CD ROMs as well as server-based material.Traditional computer science curriculum has often been presented through static printed media. What has been printed ahead of time in books or handouts can not be changed. Any changes would have to be attached externally by way of scribbled notes or explanations on the board. The Web gives us the technological affordances to change that, both for students and teachers.The original page remains unchanged, however the user's version of the page is filtered, adding, subtracting and changing web page elements, giving an enhanced view. A demonstration version of the Web Annotator can be downloaded from: http://logos.cs.uic.edu/Annotator.},
    address = {New York, NY, USA},
    author = {Reed, Dale and John, Sam},
    citeulike-article-id = {1295275},
    citeulike-linkout-0 = {http://portal.acm.org/citation.cfm?id=792548.612014},
    citeulike-linkout-1 = {http://dx.doi.org/10.1145/792548.612014},
    doi = {10.1145/792548.612014},
    isbn = {158113648X},
    journal = {SIGCSE Bulletin},
    keywords = {annotation},
    month = {January},
    number = {1},
    pages = {386--390},
    posted-at = {2007-05-14 21:17:25},
    priority = {4},
    publisher = {ACM Press},
    title = {Web annotator},
    url = {http://dx.doi.org/10.1145/792548.612014},
    volume = {35},
    year = {2003}
}

@misc{citeulike:1286083,
    abstract = {The number of knowledge-based systems that build on Bayesian belief networks is increasing.
The construction of such a network however requires a large number of probabilities in
numerical form. This is often considered a major obstacle, one of the reasons being that
experts are reluctant to provide numerical probabilities. The use of verbal probability
expressions as an additional method of eliciting probabilistic information may to some extent
remove this obstacle. In this paper, we review...},
    author = {Renooij, S. and Witteman, C.},
    citeulike-article-id = {1286083},
    citeulike-linkout-0 = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.33.3966},
    keywords = {bayesian, expert\_systems},
    posted-at = {2007-05-09 19:16:31},
    priority = {2},
    title = {Talking probabilities: communicating probabilistic information with words and numbers},
    url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.33.3966},
    year = {1999}
}

@inproceedings{citeulike:1090415,
    abstract = {We introduce Peekaboom, an entertaining web-based game that can help computers locate objects in images. People play the game because of its entertainment value, and as a side effect of them playing, we collect valuable image metadata, such as which pixels belong to which object in the image. The collected data could be applied towards constructing more accurate computer vision algorithms, which require massive amounts of training and testing data not currently available. Peekaboom has been played by thousands of people, some of whom have spent over 12 hours a day playing, and thus far has generated millions of data points. In addition to its purely utilitarian aspect, Peekaboom is an example of a new, emerging class of games, which not only bring people together for leisure purposes, but also exist to improve artificial intelligence. Such games appeal to a general audience, while providing answers to problems that computers cannot yet solve.},
    address = {New York, NY, USA},
    author = {von Ahn, Luis and Liu, Ruoran and Blum, Manuel},
    booktitle = {CHI '06: Proc. of the SIGCHI conf. on Human Factors in computing systems},
    citeulike-article-id = {1090415},
    citeulike-linkout-0 = {http://portal.acm.org/citation.cfm?id=1124782},
    citeulike-linkout-1 = {http://dx.doi.org/10.1145/1124772.1124782},
    doi = {10.1145/1124772.1124782},
    isbn = {1-59593-372-7},
    keywords = {computer\_vision, mech\_turks},
    location = {Montr\'{e}al, Qu\'{e}bec, Canada},
    pages = {55--64},
    posted-at = {2007-05-09 19:13:59},
    priority = {2},
    publisher = {ACM},
    title = {Peekaboom: a game for locating objects in images},
    url = {http://dx.doi.org/10.1145/1124772.1124782},
    year = {2006}
}

@misc{citeulike:1286073,
    abstract = {Using a participant-observer research design, a team of modelers at Strathclyde
University designed and implemented modeling guidelines to link semantically rich
scenario maps to a formal causal influence diagram that was in turn used as the basis for
a formal simulation model. These guidelines were used in a case, which involved a
system dynamics model of the promotion of renewable energy sources in the UK electric
power market. By using these guidelines, the team posits a way to  ” bridge the gap”
between qualitative word-and-arrow diagrams (in this case scenario maps) and formal
system dynamics models. This work informs on-going research in group model building,
strategy modeling (especially using scenarios) and the on-going debate about qualitative
vs. quantitative system dynamics.},
    author = {Howick, Susan and Ackermann, Fran and Andersen, David},
    booktitle = {University of Strathclyde working paper series No. 2004/1},
    citeulike-article-id = {1286073},
    organization = {University of Strathclyde Business School},
    posted-at = {2007-05-09 19:11:13},
    priority = {2},
    title = {Bridging the Gap: Linking Qualitative scenario Maps to Quantitative Model Structures},
    year = {2004}
}

@inproceedings{citeulike:1285806,
    abstract = {An approach to investigating the human decision
cycle, particularly that employed by individuals
and organizations during crisis, is presented.
The collaborative approach described
here is especially beneficial in today's world
of rapidly evolving, global situations within
which U.S. security policies and operational
plans are generated.
This paper continues the documentation of
research in the field of Influence Net modeling.
Specifically, we will address the capabilities
required of an automated system to encourage
and facilitate the collaboration, both
real-time and evolutionary, of decision makers
and their supporting experts. We present our
research results that extend traditional Bayesian
inference net structure to allow for interactive
use by modelers unfamiliar with probability
theory or who are unwilling to spend
the excessive time required to specify the traditional
Bayesian model. The results of this
research, called Causal Strengths (CAST)
Logic, have been implemented as software
applications by the authors and their colleagues.},
    address = {Monterey, CA},
    author = {Rosen, Julie A. and Smith, Wayne L.},
    booktitle = {Proc. of the Command and Control Research Symposium},
    citeulike-article-id = {1285806},
    organization = {Naval Postgraduate School},
    pages = {699--708},
    posted-at = {2007-05-09 19:05:23},
    priority = {3},
    title = {Influence Net Modeling with Causal Strengths: an Evolutionary Approach},
    year = {1996}
}

@proceedings{citeulike:76587,
    author = {Bergin, Joseph},
    booktitle = {ITiCSE '02: Proc. of the 7th annual conf. on Innovation and technology in computer science education},
    citeulike-article-id = {76587},
    citeulike-linkout-0 = {http://portal.acm.org/citation.cfm?id=544473},
    citeulike-linkout-1 = {http://dx.doi.org/10.1145/544414.544473},
    doi = {10.1145/544414.544473},
    isbn = {1581134991},
    keywords = {wiki},
    month = {September},
    number = {3},
    pages = {195},
    posted-at = {2007-05-07 23:41:27},
    priority = {2},
    publisher = {ACM Press},
    title = {Teaching on the wiki web},
    url = {http://dx.doi.org/10.1145/544414.544473},
    volume = {34},
    year = {2002}
}

@inproceedings{citeulike:833283,
    address = {New York, NY, USA},
    author = {Majchrzak, Ann and Wagner, Christian and Yates, Dave},
    booktitle = {WikiSym'06: Proc. of the intl. symposium on Symposium on Wikis},
    citeulike-article-id = {833283},
    citeulike-linkout-0 = {http://portal.acm.org/citation.cfm?id=1149472},
    citeulike-linkout-1 = {http://dx.doi.org/10.1145/1149453.1149472},
    doi = {10.1145/1149453.1149472},
    isbn = {1595934138},
    keywords = {survey, wiki},
    pages = {99--104},
    posted-at = {2007-05-07 23:19:32},
    priority = {3},
    publisher = {ACM Press},
    title = {Corporate wiki users: results of a survey},
    url = {http://dx.doi.org/10.1145/1149453.1149472},
    year = {2006}
}

@book{citeulike:114322,
    abstract = {{Suitable for system administrators or managers seeking an affordable content-management solution, <I>The Wiki Way</I> shows off how to take advantage of Wiki collaborative software, which allows users to post and edit content remotely. This book is all you need to get up and running with this exciting (and free) way to build and manage content.<p>  This text is first and foremost a guide to what Wiki software is and how to install, customize, and administer it within your organization. Early sections discuss the advantages of Wiki Web sites, which allow all users to add and edit content. While it might sound like a free-for-all, the authors suggest such Web sites have been used successfully in research, business, and education to document project designs, for brainstorming, and for otherwise creating content in a collaborative fashion. Case studies for such organizations as Georgia Tech, New York Times Digital, and Motorola give a glimpse of Wiki used in real settings, so you will get a sense of what to expect.<p>  This book is also a guide to the nuts and bolts of downloading and installing Wiki and customizing it for your site. Sections on basic tweaks to Wiki's Perl scripts will let you customize your site to match your organization's needs. Standout material includes almost three dozen customization tips. This volume is illustrated with actual screen shots of Wiki, so you can get a sense of what it is like for users to work together in such an unrestricted fashion. <p>  Throughout the text, the authors are suitably upbeat about Wiki's prospects for wider adoption, but they are realistic enough to note compromises (such as requiring passwords and restricting edit rights) required in business settings. They also survey the field of Wiki open-source projects and clones, as well as other similar content-management solutions (such as Zope and the emerging WebDAV standard).<p>  While it's hard to predict whether Wiki-based Web sites are for everyone, this book presents the pros and cons of a potentially exciting and useful tool that promotes collaborative content creation. This title can help any organization get going with a Wiki Web site, from the standpoint of planning, deployment, and basic administration. <I>--Richard Dragan</I><p>  <B>Topics covered:</B><ul><li>Collaboration tools explained <li>Web-based collaboration <li>WebDAV <li>Introduction to Wiki <li>User conventions with Wiki <li>Survey of Wiki open-source projects and clones <li>Installing Wiki (including Apache Web Server and security issues) <li>Using Wiki (making notes, Wiki used as a PIM, content management and links, page editing) <li>How to structure Wiki content (suggested default structure: pros and cons) <li>Customizing Wiki <li>Tour of Wiki Perl scripts and tips for customizing your Wiki site <li>Wiki add-ons (including spellchecking and uploading files) <li>Administration in Wiki (viewing events, controlling access and authentication, database administration, and debugging techniques) <li>Guidelines for Wiki projects (dos and don'ts) <li>Wiki case studies for education <li>Business and research</ul> }},
    author = {Leuf, Bo and Cunningham, Ward},
    citeulike-article-id = {114322},
    citeulike-linkout-0 = {http://www.amazon.ca/exec/obidos/redirect?tag=citeulike09-20\&amp;path=ASIN/020171499X},
    citeulike-linkout-1 = {http://www.amazon.de/exec/obidos/redirect?tag=citeulike01-21\&amp;path=ASIN/020171499X},
    citeulike-linkout-2 = {http://www.amazon.fr/exec/obidos/redirect?tag=citeulike06-21\&amp;path=ASIN/020171499X},
    citeulike-linkout-3 = {http://www.amazon.jp/exec/obidos/ASIN/020171499X},
    citeulike-linkout-4 = {http://www.amazon.co.uk/exec/obidos/ASIN/020171499X/citeulike00-21},
    citeulike-linkout-5 = {http://www.amazon.com/exec/obidos/redirect?tag=citeulike07-20\&path=ASIN/020171499X},
    citeulike-linkout-6 = {http://www.worldcat.org/isbn/020171499X},
    citeulike-linkout-7 = {http://books.google.com/books?vid=ISBN020171499X},
    citeulike-linkout-8 = {http://www.amazon.com/gp/search?keywords=020171499X\&index=books\&linkCode=qs},
    citeulike-linkout-9 = {http://www.librarything.com/isbn/020171499X},
    day = {03},
    howpublished = {Paperback},
    isbn = {020171499X},
    keywords = {wiki},
    month = {April},
    posted-at = {2007-05-07 23:05:42},
    priority = {1},
    publisher = {{Addison-Wesley Professional}},
    title = {The Wiki Way: Collaboration and Sharing on the Internet},
    url = {http://www.amazon.com/exec/obidos/redirect?tag=citeulike07-20\&path=ASIN/020171499X},
    year = {2001}
}

@inproceedings{citeulike:1098057,
    address = {New York, NY, USA},
    author = {Rafaeli, Sheizaf},
    booktitle = {WikiSym '06: Proc. of the 2006 intl. symposium on Wikis},
    citeulike-article-id = {1098057},
    citeulike-linkout-0 = {http://portal.acm.org/citation.cfm?id=1149453.1149461},
    citeulike-linkout-1 = {http://dx.doi.org/10.1145/1149453.1149461},
    comment = {This isn't an article, it's just a description of a panel, and not a reliable reference for research relating to wikis in teaching / learning.},
    doi = {10.1145/1149453.1149461},
    isbn = {1595934138},
    keywords = {wiki},
    pages = {15--16},
    posted-at = {2007-05-07 23:00:37},
    priority = {0},
    publisher = {ACM Press},
    title = {Wiki uses in teaching and learning},
    url = {http://dx.doi.org/10.1145/1149453.1149461},
    year = {2006}
}

@inproceedings{citeulike:539807,
    abstract = {A growing population of users want to extract a growing variety of information
from on-line texts. Unfortunately, current information extraction systems typically
require experts to hand-build dictionaries of extraction patterns for each new type of
information to be extracted. This paper presents a system that can learn dictionaries
of extraction patterns directly from user-provided examples of texts and events to
be extracted from them. The system, called LIEP, learns patterns that recognize
relationships between key constituents based on local syntax. Sets of patterns learned
by LIEP for a sample extraction task perform nearly at the level of a hand-built
dictionary of patterns.},
    author = {Huffman, Scott B.},
    booktitle = {Learning for Natural Language Processing},
    citeulike-article-id = {539807},
    citeulike-linkout-0 = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.49.8210},
    keywords = {information\_extraction},
    pages = {246--260},
    posted-at = {2007-05-04 21:41:21},
    priority = {2},
    title = {Learning information extraction patterns from examples},
    url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.49.8210},
    year = {1995}
}

@inproceedings{citeulike:1277866,
    abstract = {Previous research in information science and in human-computer interaction has shown that people tend to use different terms to describe a similar concept. Due to the unique backgrounds, training, and experiences of different people, the chance of two collaborators using the same term to describe a concept or an object for a common task is quite low. This vocabulary difference has created difficulties for both synchronous and asynchronous collaborations. Bridging the gap between vocabularies of different collaborators is one of the most pressing challenges for computer-supported cooperative work (CSCW) system designers. In this research we propose a concept space approach and describe its associated algorithms for solving the vocabulary problem. For illustration purposes, we present two implementation examples. The first implementation involved extracting and linking C. elegans worm-specific vocabularies for assisting molecular biology researchers in information retrieval and information sharing. The second example describes a system which helped resolve meeting participants' vocabulary problem during a group brainstorming and idea organization process. By adopting automatic indexing, cluster analysis, and neural network classification techniques, this research has shown the feasibility of an algorithmic approach to solving the vocabulary problem in collaboration.},
    address = {Arlington, VA},
    author = {Chen, Hsinchun},
    booktitle = {Proc. National Science Foundation Coordination Theory and Collaboration Technology Workshop},
    citeulike-article-id = {1277866},
    citeulike-linkout-0 = {http://dlist.sir.arizona.edu/521/},
    day = {20},
    keywords = {collaboration},
    month = {September},
    posted-at = {2007-05-04 20:10:14},
    priority = {2},
    title = {Approach on the Vocabulary Problem in Collaboration},
    url = {http://dlist.sir.arizona.edu/521/},
    year = {1993}
}

@article{citeulike:835512,
    abstract = {Research on cross-language information retrieval (CLIR) has typically been restricted to settings using binary relevance assessments. In this paper, we present evaluation results for dictionary-based CLIR using graded relevance assessments in a best match retrieval environment. A text database containing newspaper articles and a related set of 35 search topics were used in the tests. First, monolingual baseline queries were automatically formed from the topics. Secondly, source language topics (in English, German, and Swedish) were automatically translated into the target language (Finnish), using structured target queries. The effectiveness of the translated queries was compared to that of the monolingual queries. Thirdly, pseudo-relevance feedback was used to expand the original target queries. CLIR performance was evaluated using three relevance thresholds: stringent, regular, and liberal. When regular or liberal threshold was used, a reasonable performance was achieved. Using stringent threshold, equally high performance could not be achieved. On all the relevance thresholds the performance of the translated queries was successfully raised by pseudo-relevance feedback based query expansion. However, the performance of the stringent threshold in relation to the other thresholds could not be raised by this method.},
    author = {Lehtokangas and Raija and Keskustalo and Heikki and Jarvelin and Kalervo},
    citeulike-article-id = {835512},
    citeulike-linkout-0 = {http://dx.doi.org/10.1007/s10791-006-6389-1},
    citeulike-linkout-1 = {http://www.ingentaconnect.com/content/klu/inrt/2006/00000009/00000004/00006389},
    doi = {10.1007/s10791-006-6389-1},
    issn = {1386-4564},
    journal = {Information Retrieval},
    keywords = {information\_retrieval, relevance\_feedback},
    month = {September},
    number = {4},
    pages = {421--433},
    posted-at = {2007-05-04 19:19:09},
    priority = {2},
    publisher = {Springer},
    title = {Experiments with dictionary-based CLIR using graded relevance assessments: Improving effectiveness by pseudo-relevance feedback},
    url = {http://dx.doi.org/10.1007/s10791-006-6389-1},
    volume = {9},
    year = {2006}
}

@article{citeulike:163711,
    abstract = {Relevance feedback is an automatic process, introduced over 20 years ago, designed to produce improved query formulations following an initial retrieval operation.  The principal relevance feedback methods described over the years are examined briefly, and evaluation data are included to demonstrate the effectiveness of the various methods.  Prescriptions are given for conducting text retrieval operations iteratively using relevance feedback.},
    address = {San Francisco, CA, USA},
    author = {Salton, Gerard and Buckley, Chris},
    citeulike-article-id = {163711},
    citeulike-linkout-0 = {http://portal.acm.org/citation.cfm?id=275712},
    isbn = {1558604545},
    keywords = {information\_retrieval, relevance\_feedback},
    pages = {355--364},
    posted-at = {2007-05-04 19:13:48},
    priority = {2},
    publisher = {Morgan Kaufmann Publishers Inc.},
    title = {Improving retrieval performance by relevance feedback},
    url = {http://portal.acm.org/citation.cfm?id=275712},
    year = {1997}
}

@article{citeulike:397741,
    abstract = {In this paper, we present five user experiments on incorporating behavioral information into the relevance feedback process. In particular, we concentrate on ranking terms for query expansion and selecting new terms to add to the user's query. Our experiments are an attempt to widen the evidence used for relevance feedback from simply the relevant documents to include information on how users are searching. We show that this information can lead to more successful relevance feedback techniques. We also show that the presentation of relevance feedback to the user is important in the success of relevance feedback.},
    author = {Ruthven, Ian and Lalmas, Mounia and van Rijsbergen, Keith},
    citeulike-article-id = {397741},
    citeulike-linkout-0 = {http://portal.acm.org/citation.cfm?id=779042.779049},
    citeulike-linkout-1 = {http://dx.doi.org/10.1002/asi.10240},
    citeulike-linkout-2 = {http://www3.interscience.wiley.com/cgi-bin/abstract/103520098/ABSTRACT},
    day = {25},
    doi = {10.1002/asi.10240},
    issn = {1532-2890},
    journal = {Journal of the American Society for Information Science and Technology},
    keywords = {relevance\_feedback},
    month = {February},
    number = {6},
    pages = {529--549},
    posted-at = {2007-05-04 19:11:21},
    priority = {1},
    title = {Incorporating user search behavior into relevance feedback},
    url = {http://dx.doi.org/10.1002/asi.10240},
    volume = {54},
    year = {2003}
}

@book{ModernInfRetrieval,
    author = {Baeza-Yates, Ricardo and Ribeiro-Neto, Berthier},
    citeulike-article-id = {217178},
    citeulike-linkout-0 = {http://www.amazon.ca/exec/obidos/redirect?tag=citeulike09-20\&amp;path=ASIN/020139829X},
    citeulike-linkout-1 = {http://www.amazon.de/exec/obidos/redirect?tag=citeulike01-21\&amp;path=ASIN/020139829X},
    citeulike-linkout-2 = {http://www.amazon.fr/exec/obidos/redirect?tag=citeulike06-21\&amp;path=ASIN/020139829X},
    citeulike-linkout-3 = {http://www.amazon.jp/exec/obidos/ASIN/020139829X},
    citeulike-linkout-4 = {http://www.amazon.co.uk/exec/obidos/ASIN/020139829X/citeulike00-21},
    citeulike-linkout-5 = {http://www.amazon.com/exec/obidos/redirect?tag=citeulike07-20\&path=ASIN/020139829X},
    citeulike-linkout-6 = {http://www.worldcat.org/isbn/020139829X},
    citeulike-linkout-7 = {http://books.google.com/books?vid=ISBN020139829X},
    citeulike-linkout-8 = {http://www.amazon.com/gp/search?keywords=020139829X\&index=books\&linkCode=qs},
    citeulike-linkout-9 = {http://www.librarything.com/isbn/020139829X},
    day = {15},
    edition = {1st},
    howpublished = {Paperback},
    isbn = {020139829X},
    keywords = {book, information\_retrieval},
    month = {May},
    posted-at = {2007-05-02 20:55:13},
    priority = {1},
    publisher = {Addison Wesley},
    title = {Modern Information Retrieval},
    url = {http://www.amazon.com/exec/obidos/redirect?tag=citeulike07-20\&path=ASIN/020139829X},
    year = {1999}
}

@inproceedings{citeulike:110149,
    abstract = {Note: OCR errors may be found in this Reference List extracted from the full text article.  ACM has opted to expose the complete List rather than only correct and linked references.},
    address = {New York, NY, USA},
    author = {Hofmann, Thomas},
    booktitle = {SIGIR '99: Proc. of the 22nd annual intl. ACM SIGIR conf. on Research and development in information retrieval},
    citeulike-article-id = {110149},
    citeulike-linkout-0 = {http://portal.acm.org/citation.cfm?id=312649},
    citeulike-linkout-1 = {http://dx.doi.org/10.1145/312624.312649},
    comment = {[Hofmann, 1999] Hofmann, T, Probabilistic latent semantic indexing,
SIGIR '99: Proc. of the 22nd annual intl. ACM SIGIR
conf. on Research and development in information retrieval,
p. 50, 1999.},
    doi = {10.1145/312624.312649},
    isbn = {1-58113-096-1},
    keywords = {plsi, sigir99},
    location = {Berkeley, California, United States},
    pages = {50--57},
    posted-at = {2007-05-02 19:40:46},
    priority = {2},
    publisher = {ACM},
    title = {Probabilistic latent semantic indexing},
    url = {http://dx.doi.org/10.1145/312624.312649},
    year = {1999}
}

@inproceedings{citeulike:675036,
    abstract = {Latent Dirichlet Allocation (LDA) is a fully generative approach to language modelling which overcomes the inconsistent generative semantics of Probabilistic Latent Semantic Indexing (PLSI). This paper shows that PLSI is a  maximum a posteriori  estimated LDA model under a uniform Dirichlet prior, therefore the perceived shortcomings of PLSI can be resolved and elucidated within the LDA framework.},
    address = {New York, NY, USA},
    author = {Girolami, Mark and Kaban, Ata},
    booktitle = {SIGIR '03: Proc. of the 26th annual intl. ACM SIGIR conf. on Research and development in informaion retrieval},
    citeulike-article-id = {675036},
    citeulike-linkout-0 = {http://portal.acm.org/citation.cfm?id=860537},
    citeulike-linkout-1 = {http://dx.doi.org/10.1145/860435.860537},
    comment = {[Girolami and Kaban, 2003] Girolami, M and Kaban, Ata, On an
equivalence between PLSI and LDA, SIGIR '03: Proc. of the 26th
annual intl. ACM SIGIR conf. on Research and development
in informaion retrieval, p. 433, 2003.},
    doi = {10.1145/860435.860537},
    isbn = {1-58113-646-3},
    keywords = {information\_retrieval, lda, plsi, sigir03},
    location = {Toronto, Canada},
    pages = {433--434},
    posted-at = {2007-05-02 19:19:25},
    priority = {2},
    publisher = {ACM},
    title = {On an equivalence between PLSI and LDA},
    url = {http://dx.doi.org/10.1145/860435.860537},
    year = {2003}
}

@article{Turney2001:MiningPMIvsLSA,
    abstract = {This paper presents a simple unsupervised learning algorithm for recognizing synonyms, based on statistical data acquired by querying a Web search engine. The algorithm, called PMI-IR, uses Pointwise Mutual Information (PMI) and Information Retrieval (IR) to measure the similarity of pairs of words. PMI-IR is empirically evaluated using 80 synonym test questions from the Test of English as a Foreign Language (TOEFL) and 50 synonym test questions from a collection of tests for students of...},
    address = {London, UK},
    author = {Turney, Peter D.},
    citeulike-article-id = {401542},
    citeulike-linkout-0 = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.28.7394},
    journal = {Lecture Notes in Computer Science},
    keywords = {lsa, pmi-ir},
    pages = {491--502},
    posted-at = {2007-05-02 18:43:02},
    priority = {0},
    publisher = {Springer-Verlag},
    title = {Mining the Web for Synonyms: PMI--IR versus LSA on TOEFL},
    url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.28.7394},
    volume = {2167},
    year = {2001}
}

@inproceedings{citeulike:516523,
    abstract = {A predictive tool to simulate human visual search behavior would help interface designers inform and validate their design. Such a tool would benefit from a semantic component that would help predict search behavior even in the absence of exact textual matches between goal and target. This paper discusses a comparison of three semantic systems-LSA, WordNet and PMI-IR-to evaluate their performance in predicting the link that people would select given an information goal and a webpage. PMI-IR best predicted human performance as observed in a user study.},
    address = {New York, NY, USA},
    author = {Kaur, Ishwinder and Hornof, Anthony J.},
    booktitle = {CHI '05: Proc. of the SIGCHI conf. on Human factors in computing systems},
    citeulike-article-id = {516523},
    citeulike-linkout-0 = {http://portal.acm.org/citation.cfm?id=1054980},
    citeulike-linkout-1 = {http://dx.doi.org/10.1145/1054972.1054980},
    comment = {[Kaur and Hornof, 2005] Kaur, I and Hornof, A. J., A comparison of LSA, wordNet and PMI-IR for predicting user click behavior, CHI '05: Proc. of the SIGCHI conf. on Human factors in computing systems, p. 51, 2005.},
    doi = {10.1145/1054972.1054980},
    isbn = {1-58113-998-5},
    keywords = {chi05, interfaces, lsa, pmi-ir, prediction},
    location = {Portland, Oregon, USA},
    pages = {51--60},
    posted-at = {2007-05-02 17:47:26},
    priority = {2},
    publisher = {ACM},
    title = {A comparison of LSA, wordNet and PMI-IR for predicting user click behavior},
    url = {http://dx.doi.org/10.1145/1054972.1054980},
    year = {2005}
}

@inproceedings{citeulike:1082836,
    abstract = {In this paper we address the problem of discovering missing hypertext links in Wikipedia. The method we propose consists of two steps: first, we compute a cluster of highly similar pages around a given page, and then we identify candidate links from those similar pages that might be missing on the given page. The main innovation is in the algorithm that we use for identifying similar pages, LTRank, which ranks pages using co-citation and page title information. Both LTRank and the link discovery method are manually evaluated and show acceptable results, especially given the simplicity of the methods and conservativeness of the evaluation criteria.},
    address = {New York, NY, USA},
    author = {Adafre, Sisay F. and de Rijke, Maarten},
    booktitle = {LinkKDD '05: Proc. of the 3rd intl. workshop on Link discovery},
    citeulike-article-id = {1082836},
    citeulike-linkout-0 = {http://portal.acm.org/citation.cfm?id=1134284},
    citeulike-linkout-1 = {http://dx.doi.org/10.1145/1134271.1134284},
    doi = {10.1145/1134271.1134284},
    isbn = {1-59593-215-1},
    keywords = {document\_linking, kdid05, wiki},
    location = {Chicago, Illinois},
    pages = {90--97},
    posted-at = {2007-04-30 22:26:08},
    priority = {0},
    publisher = {ACM},
    title = {Discovering missing links in Wikipedia},
    url = {http://dx.doi.org/10.1145/1134271.1134284},
    year = {2005}
}

@inbook{citeulike:1230405,
    abstract = {Case-based reasoning (CBR) is now a mature subfield of artificial intelligence. The fundamental principles of case-based reasoning have been established, and numerous applications have demonstrated its role as a useful technology. Recent progress has also revealed new opportunities and challenges for the field. This book presents experiences in CBR that illustrate the state of the art, the lessons learned from those experiences, and directions for the future.

True to the spirit of CBR, this book examines the field in a primarily case-based way. Its chapters provide concrete examples of how key issues---including indexing and retrieval, case adaptation, evaluation, and application of CBR methods---are being addressed in the context of a range of tasks and domains. These issue-oriented case studies of experiences with particular projects provide a view of the principles of CBR, what CBR can do, how to attack problems with case-based reasoning, and how new challenges are being addressed. The case studies are supplemented with commentaries from leaders in the field providing individual perspectives on the state of CBR and its future impact.

This book provides experienced CBR practitioners with a reference to recent progress in case-based reasoning research and applications. It also provides an introduction to CBR methods and the state of the art for students, AI researchers in other areas, and developers starting to build case-based reasoning systems. It presents experts and non-experts alike with visions of the most promising directions for new progress and for the roles of the next generation of CBR systems.},
    author = {Leake, David B.},
    booktitle = {Case-Based Reasoning: Experiences, Lessons, and Future Directions},
    chapter = {1},
    citeulike-article-id = {1230405},
    comment = {This chapeter is online: http://www.cs.indiana.edu/\~{}leake/papers/a-96-01.html},
    editor = {Leake, David B.},
    keywords = {book, case\_based\_reasoning},
    organization = {Indiana University},
    posted-at = {2007-04-17 00:38:25},
    priority = {2},
    publisher = {AAAI Press/MIT Press},
    title = {CBR in Context: The Present and Future}
}

@inproceedings{DourishAndBellotti92:sharedWorkspaces,
    abstract = {Awareness of individual and group activities is critical to
successful collaboration and is commonly supported in
CSCW systems by active, information generation
mechanisms separate from the shared workspace. These
mechanisms penalise information providers, presuppose
relevance to the recipient, and make access difficult. We
discuss a study of shared editor use which suggests that
awareness information provided and exploited passively
through the shared workspace, allows users to move
smoothly between close and loose collaboration, and to
assign and coordinate work dynamically. Passive awareness
mechanisms promise effective support for collaboration
requiring this sort of behaviour, whilst avoiding problems
with active approaches.},
    address = {Toronto, Ontario},
    author = {Dourish, Paul and Bellotti, Victoria},
    booktitle = {Proc. of the ACM Conf. on Computer Supported Cooperative Work (CSCW'92)},
    citeulike-article-id = {1222706},
    citeulike-linkout-0 = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.57.72},
    keywords = {collaboration, cscw92},
    pages = {107--114},
    posted-at = {2007-04-12 21:59:36},
    priority = {2},
    publisher = {ACM Press},
    title = {Awareness and coordination in shared workspaces},
    url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.57.72},
    year = {1992}
}

@inproceedings{pascaIJCAI07,
    abstract = {Within the larger area of automatic acquisition of
knowledge from the Web, we introduce a method
for extracting relevant attributes, or quantifiable
properties, for various classes of objects. The
method extracts attributes such as capital city and
President for the class Country, or cost, manufacturer
and side effects for the class Drug, without relying
on any expensive language resources or complex
processing tools. In a departure from previous
approaches to large-scale information extraction,
we explore the role of Web query logs, rather than
Web documents, as an alternative source of class
attributes. The quality of the extracted attributes
recommends query logs as a valuable, albeit little
explored, resource for information extraction.},
    address = {Hyderabad, India},
    author = {Pasca, Marius and Van Durme, Benjamin},
    booktitle = {Proc. of the 20th Intl. Joint Conf. on Artificial Intelligence (IJCAI-07)},
    citeulike-article-id = {1218322},
    keywords = {clustering, ijcai07, information\_extraction, search},
    month = {February},
    pages = {2832--2837},
    posted-at = {2007-04-09 22:44:57},
    priority = {0},
    title = {What You Seek is What You Get: Extraction of Class Attributes from Query Logs},
    year = {2007}
}

@inproceedings{WeiCroft2006:LDAadHocRetrieval,
    abstract = {Search algorithms incorporating some form of topic model have a long history in information retrieval. For example, cluster-based retrieval has been studied since the 60s and has recently produced good results in the language model framework. An approach to building topic models based on a formal generative model of documents, Latent Dirichlet Allocation (LDA), is heavily cited in the machine learning literature, but its feasibility and effectiveness in information retrieval is mostly unknown. In this paper, we study how to efficiently use LDA to improve ad-hoc retrieval. We propose an LDA-based document model within the language modeling framework, and evaluate it on several TREC collections. Gibbs sampling is employed to conduct approximate inference in LDA and the computational complexity is analyzed. We show that improvements over retrieval using cluster-based models can be obtained with reasonable efficiency.},
    address = {New York, NY, USA},
    author = {Wei, Xing and Croft, Bruce W.},
    booktitle = {SIGIR '06: Proc. of the 29th annual intl. ACM SIGIR conf. on Research and development in information retrieval},
    citeulike-article-id = {1109893},
    citeulike-linkout-0 = {http://dx.doi.org/http://doi.acm.org/10.1145/1148170.1148204},
    citeulike-linkout-1 = {http://portal.acm.org/citation.cfm?id=1148204},
    citeulike-linkout-2 = {http://dx.doi.org/10.1145/1148170.1148204},
    doi = {http://doi.acm.org/10.1145/1148170.1148204},
    isbn = {1-59593-369-7},
    keywords = {document\_linking, information\_retrieval, lda, sigir06},
    location = {Seattle, Washington, USA},
    pages = {178--185},
    posted-at = {2007-04-03 22:57:43},
    priority = {0},
    publisher = {ACM},
    title = {LDA-based document models for ad-hoc retrieval},
    url = {http://dx.doi.org/http://doi.acm.org/10.1145/1148170.1148204},
    year = {2006}
}

@article{BleiNgJordan2003:LDA,
    abstract = {We describe latent Dirichlet allocation (LDA), a generative probabilistic model for collections of discrete data such as text corpora. LDA is a three-level hierarchical Bayesian model, in which each item of a collection is modeled as a finite mixture over an underlying set of topics. Each topic is, in turn, modeled as an infinite mixture over an underlying set of topic probabilities. In the context of text modeling, the topic probabilities provide an explicit representation of a document. We present efficient approximate inference techniques based on variational methods and an EM algorithm for empirical Bayes parameter estimation. We report results in document modeling, text classification, and collaborative filtering, comparing to a mixture of unigrams model and the probabilistic LSI model.},
    address = {Cambridge, MA, USA},
    author = {Blei, David M. and Ng, Andrew Y. and Jordan, Michael I.},
    citeulike-article-id = {353473},
    citeulike-linkout-0 = {http://portal.acm.org/citation.cfm?id=944937},
    citeulike-linkout-1 = {http://dx.doi.org/10.1162/jmlr.2003.3.4-5.993},
    doi = {10.1162/jmlr.2003.3.4-5.993},
    issn = {1532-4435},
    journal = {Journal of Machine Learning Research},
    keywords = {document\_clustering, information\_retrieval, jmlr03, lda},
    month = {March},
    number = {4-5},
    pages = {993--1022},
    posted-at = {2007-04-03 20:33:51},
    priority = {0},
    publisher = {JMLR.org},
    title = {Latent dirichlet allocation},
    url = {http://dx.doi.org/10.1162/jmlr.2003.3.4-5.993},
    volume = {3},
    year = {2003}
}

@misc{citeulike:484544,
    abstract = {This paper presents the results of an experimental study of some common document clustering techniques. In particular, we compare the two main approaches to document clustering, agglomerative hierarchical clustering and K-means. (For K-means we used a \&quot;standard\&quot; K-means algorithm and a variant of K-means, \&quot;bisecting\&quot; K-means.) Hierarchical clustering is often portrayed as the better quality clustering approach, but is limited because of its quadratic time complexity. In contrast, K-means and...},
    author = {Steinbach, M. and Karypis, G. and Kumar, V.},
    citeulike-article-id = {484544},
    citeulike-linkout-0 = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.34.1505},
    keywords = {clustering, document\_clustering},
    posted-at = {2007-03-26 16:25:34},
    priority = {3},
    title = {A comparison of document clustering techniques},
    url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.34.1505},
    year = {2000}
}

@article{citeulike:933736,
    abstract = {Conventional programming languages are growing ever more enormous, but not stronger. Inherent defects at the most basic level cause them to be both fat and weak: their primitive word-at-a-time style of programming inherited from their common ancestor\&mdash;the von Neumann computer, their close coupling of semantics to state transitions, their division of programming into a world of expressions and a world of statements, their inability to effectively use powerful combining forms for building new programs from existing ones, and their lack of useful mathematical properties for reasoning about programs.   An alternative functional style of programming is founded on the use of combining forms for creating programs. Functional programs deal with structured data, are often nonrepetitive and nonrecursive, are hierarchically constructed, do not name their arguments, and do not require the complex machinery of procedure declarations to become generally applicable. Combining forms can use high level programs to build still higher level ones in a style not possible in conventional languages.   Associated with the functional style of programming is an algebra of programs whose variables range over programs and whose operations are combining forms. This algebra can be used to transform programs and to solve equations whose \&ldquo;unknowns\&rdquo; are programs in much the same way one transforms equations in high school algebra. These transformations are given by algebraic laws and are carried out in the same language in which programs are written. Combining forms are chosen not only for their programming power but also for the power of their associated algebraic laws. General theorems of the algebra give the detailed behavior and termination conditions for large classes of programs.   A new class of computing systems uses the functional programming style both in its programming language and in its state transition rules. Unlike von Neumann languages, these systems have semantics loosely coupled to states\&mdash;only one state transition occurs per major computation.},
    address = {New York, NY, USA},
    author = {Backus, John},
    citeulike-article-id = {933736},
    citeulike-linkout-0 = {http://portal.acm.org/citation.cfm?id=359579},
    citeulike-linkout-1 = {http://dx.doi.org/10.1145/359576.359579},
    doi = {10.1145/359576.359579},
    issn = {0001-0782},
    journal = {Commun. ACM},
    keywords = {functional\_programming, programming},
    month = {August},
    number = {8},
    pages = {613--641},
    posted-at = {2007-03-20 17:38:55},
    priority = {5},
    publisher = {ACM},
    title = {Can programming be liberated from the von Neumann style?: a functional style and its algebra of programs},
    url = {http://dx.doi.org/10.1145/359576.359579},
    volume = {21},
    year = {1978}
}

@proceedings{citeulike:82538,
    abstract = {Category theorists invented monads in the 1960's to concisely express certain aspects of universal algebra. Functional programmers invented list comprehensions in the 1970's to concisely express certain programs involving lists. This paper shows how list comprehensions may be generalised to an arbitrary monad, and how the resulting programming feature can concisely express in a pure functional language some programs that manipulate state, handle exceptions, parse text, or invoke continuations....},
    address = {New York, NY},
    author = {Wadler, P. L.},
    booktitle = {Proc. of the 1990 ACM Conf. on LISP and Functional Programming, Nice},
    citeulike-article-id = {82538},
    citeulike-linkout-0 = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.33.5381},
    keywords = {category-theory, functional\_programming},
    pages = {61--78},
    posted-at = {2007-03-19 15:48:04},
    priority = {4},
    publisher = {ACM},
    title = {Comprehending Monads},
    url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.33.5381},
    year = {1990}
}

@article{LandauerEtAl1998:IntroToLSA,
    abstract = {Latent Semantic Analysis (LSA) is a theory and method for extracting and representing the
contextual-usage meaning of words by statistical computations applied to a large corpus of
text (Landauer and Dumais, 1997). The underlying idea is that the aggregate of all the word
contexts in which a given word does and does not appear provides a set of mutual
constraints that largely determines the similarity of meaning of words and sets of words to
each other. The adequacy of LSA's reflection of human knowledge has been established in
a variety of ways. For example, its scores overlap those of humans on standard vocabulary
and subject matter tests; it mimics human word sorting and category judgments; it simulates
word–word and passage–word lexical priming data; and, as reported in 3 following articles
in this issue, it accurately estimates passage coherence, learnability of passages by
individual students, and the quality and quantity of knowledge contained in an essay.},
    author = {Landauer, Thomas K. and Foltz, Peter W. and Laham, Darrell},
    citeulike-article-id = {1152071},
    comment = {[Landauer et al., 1998] Landauer TK, Foltz PW and Laham D, An
Introduction to Latent Semantic Analysis, Discourse Processes, p. 259,
1998.},
    journal = {Discourse Processes},
    keywords = {concept\_extraction, document\_clustering, document\_linking, information\_extraction, information\_retrieval, lsa},
    pages = {259--284},
    posted-at = {2007-03-09 23:22:24},
    priority = {0},
    title = {An Introduction to Latent Semantic Analysis},
    volume = {25},
    year = {1998}
}

@article{citeulike:894074,
    abstract = {Note: OCR errors may be found in this Reference List extracted from the full text article.  ACM has opted to expose the complete List rather than only correct and linked references.},
    address = {New York, NY, USA},
    author = {Henglein, Fritz},
    citeulike-article-id = {894074},
    citeulike-linkout-0 = {http://portal.acm.org/citation.cfm?id=169692},
    citeulike-linkout-1 = {http://dx.doi.org/10.1145/169701.169692},
    doi = {10.1145/169701.169692},
    issn = {0164-0925},
    journal = {ACM Trans. Program. Lang. Syst.},
    month = {April},
    number = {2},
    pages = {253--289},
    posted-at = {2007-03-08 00:17:21},
    priority = {4},
    publisher = {ACM},
    title = {Type inference with polymorphic recursion},
    url = {http://dx.doi.org/10.1145/169701.169692},
    volume = {15},
    year = {1993}
}

@inproceedings{citeulike:1144369,
    abstract = {This paper investigates effects of interface style and cognitive style on problem solving
performance. It is often assumed that performance improves when information is externalized onto
the interface. Although relieving working memory this may discourage planning, understanding
and knowledge acquisition. When information is not externalized, it must be internalized, stored
in the user's memory, requiring more planning and thinking, perhaps leading to better
performance and knowledge. Another variable influencing behavior is the cognitive style of users.
We included  ” Need for Cognition” (NFC), the tendency to engage in cognitive tasks. We
investigated the effects of interface style and NFC using planning tasks. The internalization
interface led to more planful behavior and smarter solutions, but NFC had no effect.
Understanding reactions to interface information is crucial in designing software aimed at
education and learning. To facilitate active learning and provoke better performance, designers
should take care in giving users (too) much assistance.},
    author = {van Nimwegen, Christof and van Oostendorp, Herre and Burgos, Daniel and Koper, Rob},
    booktitle = {ICLS '06: Proc. of the 7th intl. conf. on Learning sciences},
    citeulike-article-id = {1144369},
    citeulike-linkout-0 = {http://portal.acm.org/citation.cfm?id=1150148},
    isbn = {0805861742},
    keywords = {interface\_design},
    pages = {785--791},
    posted-at = {2007-03-07 17:10:39},
    priority = {4},
    publisher = {Intl. Society of the Learning Sciences},
    title = {Does an interface with less assistance provoke more thoughtful behavior?},
    url = {http://portal.acm.org/citation.cfm?id=1150148},
    year = {2006}
}

@inproceedings{Novatchev2006:FXSL_XSLT,
    abstract = {This article describes the added support in FXSL 2.0 for writing higher-order functions in
XSLT 2.0 based on new fundamental XPath 2.0/XSLT 2.0 features such as the sequence
datatype, strong typing and writing functions natively in XSLT. FXSL 2.0 makes nearly all
standard XPath 2.0/XSLT 2.0 functions and operators higher-order by providing in the
FXSL namespace the definition of their identically named higher-order wrappers and
partial applications. The author argues that in effect, this makes XSLT 2.0 + FXSL a
higher-order strongly-typed functional programming system.
This paper demonstrates how based on the even higher degree of abstraction and code
reuse many challenging problems have now more compact or even one-line solutions.},
    address = {Montr\'{e}al, Qu\'{e}bec},
    author = {Novatchev, Dimitre},
    booktitle = {Extreme Markup Languages 2006},
    citeulike-article-id = {1137372},
    day = {7-11},
    keywords = {eml2006, functional\_programming, languages, programming, xml},
    month = {August},
    posted-at = {2007-03-02 23:19:35},
    priority = {4},
    title = {Higher-Order Functional Programming with
XSLT 2.0 and FXSL},
    year = {2006}
}

@inproceedings{lauEtAl2004:Sheepdog,
    abstract = {Technical support procedures are typically very complex. Users often have trouble following printed instructions describing how to perform these procedures, and these instructions are difficult for support personnel to author clearly. Our goal is to learn these procedures by demonstration, watching multiple experts performing the same procedure across different operating conditions, and produce an executable procedure that runs interactively on the user's desktop. Most previous programming by demonstration systems have focused on simple programs with regular structure, such as loops with fixed-length bodies. In contrast, our system induces complex procedure structure by aligning multiple execution traces covering different paths through the procedure. This paper presents a solution to this alignment problem using Input/Output Hidden Markov Models. We describe the results of a user study that examines how users follow printed directions. We present Sheepdog, an implemented system for capturing, learning, and playing back technical support procedures on the Windows desktop. Finally, we empirically evalute our system using traces gathered from the user study and show that we are able to achieve 73\% accuracy on a network configuration task using a procedure trained by non-experts.},
    address = {New York, NY, USA},
    author = {Lau, Tessa and Bergman, Lawrence and Castelli, Vittorio and Oblinger, Daniel},
    booktitle = {IUI '04: Proc. of the 9th intl. conf. on Intelligent user interfaces},
    citeulike-article-id = {1123301},
    citeulike-linkout-0 = {http://portal.acm.org/citation.cfm?id=964464},
    citeulike-linkout-1 = {http://dx.doi.org/10.1145/964442.964464},
    doi = {10.1145/964442.964464},
    isbn = {1581138156},
    keywords = {authoring, iui04, mixed\_initiative, pbd},
    pages = {109--116},
    posted-at = {2007-02-26 19:12:16},
    priority = {0},
    publisher = {ACM Press},
    title = {Sheepdog: learning procedures for technical support},
    url = {http://dx.doi.org/10.1145/964442.964464},
    year = {2004}
}

@inproceedings{BergmanEtAl2005:DocWizards,
    abstract = {Traditional documentation for computer-based procedures is difficult to use: readers have trouble navigating long complex instructions, have trouble mapping from the text to display widgets, and waste time performing repetitive procedures. We propose a new class of improved documentation that we call follow-me documentation wizards. Follow-me documentation wizards step a user through a script representation of a procedure by highlighting portions of the text, as well application UI elements. This paper presents algorithms for automatically capturing follow-me documentation wizards by demonstration, through observing experts performing the procedure. We also present our DocWizards implementation on the Eclipse platform. We evaluate our system with an initial user study that showing that most users have a marked preference for this form of guidance over traditional documentation.},
    address = {New York, NY, USA},
    author = {Bergman, Lawrence and Castelli, Vittorio and Lau, Tessa and Oblinger, Daniel},
    booktitle = {UIST '05: Proc. of the 18th annual ACM symposium on User interface software and technology},
    citeulike-article-id = {1123282},
    citeulike-linkout-0 = {http://portal.acm.org/citation.cfm?id=1095067},
    citeulike-linkout-1 = {http://dx.doi.org/10.1145/1095034.1095067},
    doi = {10.1145/1095034.1095067},
    isbn = {1595932712},
    keywords = {authoring, pbd, uist05},
    pages = {191--200},
    posted-at = {2007-02-26 18:51:16},
    priority = {2},
    publisher = {ACM Press},
    title = {DocWizards: a system for authoring follow-me documentation wizards},
    url = {http://dx.doi.org/10.1145/1095034.1095067},
    year = {2005}
}

@article{citeulike:1118977,
    address = {New York, NY, USA},
    author = {Basili, Victor R. and Briand, Lionel C. and Melo, Walc\&\#233;lio L.},
    citeulike-article-id = {1118977},
    citeulike-linkout-0 = {http://portal.acm.org/citation.cfm?id=236184},
    citeulike-linkout-1 = {http://dx.doi.org/10.1145/236156.236184},
    doi = {10.1145/236156.236184},
    issn = {0001-0782},
    journal = {Commun. ACM},
    keywords = {oop, programming},
    month = {October},
    number = {10},
    pages = {104--116},
    posted-at = {2007-02-23 17:47:24},
    priority = {3},
    publisher = {ACM Press},
    title = {How reuse influences productivity in object-oriented systems},
    url = {http://dx.doi.org/10.1145/236156.236184},
    volume = {39},
    year = {1996}
}

@inproceedings{citeulike:1118879,
    address = {London, UK},
    author = {Reynolds, John C.},
    booktitle = {Semantics-Directed Compiler Generation, Proc. of a Workshop},
    citeulike-article-id = {1118879},
    citeulike-linkout-0 = {http://portal.acm.org/citation.cfm?id=727090},
    comment = {"The basic intuition is that a good design principle for a language with implicit conversions is that whatever order of conversions the language takes, you should get the same result. He then formalizes that by giving a category of types and conversions, and demanding that everything commute properly. And these give just the conditions the language designer has to check to make sure that he or she hasn't screwed anything up." -- http://lambda-the-ultimate.org/node/2078},
    isbn = {3540102507},
    keywords = {category\_theory, programming},
    pages = {211--258},
    posted-at = {2007-02-23 17:46:22},
    priority = {3},
    publisher = {Springer-Verlag},
    title = {Using category theory to design implicit conversions and generic operators},
    url = {http://portal.acm.org/citation.cfm?id=727090},
    year = {1980}
}

@inproceedings{citeulike:989320,
    address = {New York, NY, USA},
    author = {Song, Xiaodan and Tseng, Belle L. and Lin, Ching-Yung and Sun, Ming-Ting},
    booktitle = {SIGIR '06: Proc. of the 29th annual intl. ACM SIGIR conf. on Research and development in information retrieval},
    citeulike-article-id = {989320},
    citeulike-linkout-0 = {http://portal.acm.org/citation.cfm?id=1148170.1148258},
    citeulike-linkout-1 = {http://dx.doi.org/10.1145/1148170.1148258},
    doi = {10.1145/1148170.1148258},
    isbn = {1595933697},
    keywords = {recommendation},
    pages = {509--516},
    posted-at = {2007-02-23 17:45:25},
    priority = {4},
    publisher = {ACM Press},
    title = {Personalized recommendation driven by information flow},
    url = {http://dx.doi.org/10.1145/1148170.1148258},
    year = {2006}
}

@inproceedings{MinkovEtAl2006:ContextSearchInEmail,
    abstract = {Similarity measures for text have historically been an impor-
tant tool for solving information retrieval problems. In many
interesting settings, however, documents are often closely
connected to other documents, as well as other non-textual
objects: for instance, email messages are connected to other
messages via header information. In this paper we consider
extended similarity metrics for documents and other objects
embedded in graphs, facilitated via a lazy graph walk. We
provide a detailed instantiation of this framework for email
data, where content, social networks and a timeline are in-
tegrated in a structural graph. The suggested framework
is evaluated for two email-related problems: disambiguating
names in email documents, and threading. We show that
reranking schemes based on the graph-walk similarity mea-
sures often outperform baseline methods, and that further
improvements can be obtained by use of appropriate learn-
ing methods.},
    address = {New York, NY, USA},
    author = {Minkov, Einat and Cohen, William W. and Ng, Andrew Y.},
    booktitle = {SIGIR '06: Proc. of the 29th annual intl. ACM SIGIR conf. on Research and development in information retrieval},
    citeulike-article-id = {1118011},
    citeulike-linkout-0 = {http://portal.acm.org/citation.cfm?id=1148179},
    citeulike-linkout-1 = {http://dx.doi.org/10.1145/1148170.1148179},
    doi = {10.1145/1148170.1148179},
    isbn = {1-59593-369-7},
    keywords = {disambiguation, email, search, sigir06},
    location = {Seattle, Washington, USA},
    month = {August},
    pages = {27--34},
    posted-at = {2007-02-23 00:56:01},
    priority = {2},
    publisher = {ACM},
    title = {Contextual search and name disambiguation in email using graphs},
    url = {http://dx.doi.org/10.1145/1148170.1148179},
    year = {2006}
}

@inproceedings{StoreyEtAl2006:SharedWaypoints,
    abstract = {This paper presents the conceptual design of TagSEA, a
collaborative tool to support asynchronous software
development. Our goal is to develop a lightweight source code
annotation tool that enhances navigation, coordination, and
capture of knowledge relevant to a software development team.
Our design is inspired by combining  ” waypoints” from
geographical navigation with  ” social tagging” from social
bookmarking software to support coordination and
communication among software developers. We describe the
motivation behind this work, walk through the design and
implementation, and report early feedback on how this
lightweight tool supports collaborative software engineering
activities. Finally, we suggest a number of new research
directions that this topic exposes.},
    address = {New York, NY, USA},
    author = {Storey, Margaret-Anne and Cheng, Li-Te and Bull, Ian and Rigby, Peter},
    booktitle = {CSCW '06: Proc. of the 2006 20th anniversary conf. on Computer supported cooperative work},
    citeulike-article-id = {966554},
    citeulike-linkout-0 = {http://portal.acm.org/citation.cfm?id=1180906},
    citeulike-linkout-1 = {http://dx.doi.org/10.1145/1180875.1180906},
    comment = {Storey et al present a system for tagging java elements with arbitrary tags, and the means to search / jump to / and store sequences of tags to facilitate navigation. 

The system is one of minimal functionality, but large applicability, as tags and routes are very flexible.  It seems likely to grow into something very useful as imaginative users start playing with it.},
    doi = {10.1145/1180875.1180906},
    isbn = {1-59593-249-6},
    keywords = {code\_navigation, collaboration, cscw06, tagging},
    location = {Banff, Alberta, Canada},
    pages = {195--198},
    posted-at = {2007-02-21 21:43:18},
    priority = {0},
    publisher = {ACM},
    title = {Shared waypoints and social tagging to support collaboration in software development},
    url = {http://dx.doi.org/10.1145/1180875.1180906},
    year = {2006}
}

@inproceedings{SandersonCroft99:DerivingConcepts,
    abstract = {This paper presents a means of automatically deriving a hierarchical organization of concepts from a set of documents without use of training data or standard clustering techniques. Instead, salient words and phrases extracted from the documents are organized hierarchically using a type of co-occurrence known as subsumption. The resulting structure is displayed as a series of hierarchical menus. When generated from a set of retrieved documents, a user browsing the menus is provided with a detailed overview of their content in a manner distinct from existing overview and summarization techniques. The methods used to build the structure are simple, but appear to be effective: a smallscale user study reveals that the generated hierarchy possesses properties expected of such a structure in that general terms are placed at the top levels leading to related and more specific terms below. The formation and presentation of the hierarchy is described along with the user study and some other informal evaluations.},
    address = {New York, NY, USA},
    author = {Sanderson, Mark and Croft, Bruce},
    booktitle = {SIGIR '99: Proc. of the 22nd annual intl. ACM SIGIR conf. on Research and development in information retrieval},
    citeulike-article-id = {564012},
    citeulike-linkout-0 = {http://portal.acm.org/citation.cfm?id=312679},
    citeulike-linkout-1 = {http://dx.doi.org/10.1145/312624.312679},
    comment = {The authors present a method for extracting a hierarchy of terms from a set of documents, primarily based on a subsubption (a term x subsumes a term y if y almost always co-occurs in documents with x, and x occurs in documents with out y.) measure of extracted terms.  The terms are found by examining an initial search query and co-occurrence of terms with the initially extracted query terms.},
    day = {15-19},
    doi = {10.1145/312624.312679},
    isbn = {1581130961},
    keywords = {clustering, concept\_extraction, information\_extraction, sigir99},
    month = {August},
    pages = {206--213},
    posted-at = {2007-02-20 18:40:14},
    priority = {0},
    publisher = {ACM Press},
    title = {Deriving concept hierarchies from text},
    url = {http://dx.doi.org/10.1145/312624.312679},
    year = {1999}
}

@article{Schmidt92:TakingCSCWSeriously,
    abstract = {The topic of Computer Supported Cooperative Work (CSCW) has attracted much attention in the last few years. While the field is obviously still in the process of development, there is a marked ambiguity about the exact focus of the field. This lack of focus may hinder its further development and lead to its dissipation. In this paper we set out an approach to CSCW as a field of research which we believe provides a coherent conceptual framework for this area, suggesting that it should be concerned with thesupport requirements of cooperative work arrangements. This provides a more principled, comprehensive, and, in our opinion, more useful conception of the field than that provided by the conception of CSCW as being focused on computer support for groups. We then investigate the consequences of taking this alternative conception seriously, in terms of research directions for the field. As an indication of the fruits of this approach, we discuss the concept of 'articulation workr' and its relevance to CSCW. This raises a host of interesting problems that are marginalized in the work on small group support but critical to the success of CSCW systems 'in the large', i. e., that are designed to meet current work requirements in the everyday world.},
    author = {Schmidt, Kjeld and Bannon, Liam},
    citeulike-article-id = {1114563},
    citeulike-linkout-0 = {http://dx.doi.org/10.1007/BF00752449},
    citeulike-linkout-1 = {http://www.springerlink.com/content/p184xtj800225154},
    day = {1},
    doi = {10.1007/BF00752449},
    journal = {Computer Supported Cooperative Work (CSCW)},
    keywords = {collaboration},
    month = {March},
    number = {1},
    pages = {7--40},
    posted-at = {2007-02-20 18:30:42},
    priority = {2},
    title = {Taking CSCW seriously},
    url = {http://dx.doi.org/10.1007/BF00752449},
    volume = {1},
    year = {1992}
}

@book{citeulike:200721,
    abstract = {{Programmers are craftspeople trained to use a certain set of tools (editors, object managers, version trackers) to generate a certain kind of product (programs) that will operate in some environment (operating systems on hardware assemblies). Like any other craft, computer programming has spawned a body of wisdom, most of which isn't taught at universities or in certification classes. Most programmers arrive at the so-called tricks of the trade over time, through independent experimentation. In <I>The Pragmatic Programmer</I>, Andrew Hunt and David Thomas codify many of the truths they've discovered during their respective careers as designers of software and writers of code.<p> Some of the authors' nuggets of pragmatism are concrete, and the path to their implementation is clear. They advise readers to learn one text editor, for example, and use it for everything. They also recommend the use of version-tracking software for even the smallest projects, and promote the merits of learning regular expression syntax and a text-manipulation language. Other (perhaps more valuable) advice is more light-hearted. In the debugging section, it is noted that, "if you see hoof prints think horses, not zebras." That is, suspect everything, but start looking for problems in the most obvious places. There are recommendations for making estimates of time and expense, and for integrating testing into the development process. You'll want a copy of <I>The Pragmatic Programmer</I> for two reasons: it displays your own accumulated wisdom more cleanly than you ever bothered to state it, and it introduces you to methods of work that you may not yet have considered. Working programmers will enjoy this book. <I>--David Wall</I><p> <B>Topics covered</B>: A useful approach to software design and construction that allows for efficient, profitable development of high-quality products. Elements of the approach include specification development, customer relations, team management, design practices, development tools, and testing procedures. This approach is presented with the help of anecdotes and technical problems.}},
    author = {Hunt, Andrew and Thomas, David},
    citeulike-article-id = {200721},
    citeulike-linkout-0 = {http://www.amazon.ca/exec/obidos/redirect?tag=citeulike09-20\&amp;path=ASIN/020161622X},
    citeulike-linkout-1 = {http://www.amazon.de/exec/obidos/redirect?tag=citeulike01-21\&amp;path=ASIN/020161622X},
    citeulike-linkout-2 = {http://www.amazon.fr/exec/obidos/redirect?tag=citeulike06-21\&amp;path=ASIN/020161622X},
    citeulike-linkout-3 = {http://www.amazon.jp/exec/obidos/ASIN/020161622X},
    citeulike-linkout-4 = {http://www.amazon.co.uk/exec/obidos/ASIN/020161622X/citeulike00-21},
    citeulike-linkout-5 = {http://www.amazon.com/exec/obidos/redirect?tag=citeulike07-20\&path=ASIN/020161622X},
    citeulike-linkout-6 = {http://www.worldcat.org/isbn/020161622X},
    citeulike-linkout-7 = {http://books.google.com/books?vid=ISBN020161622X},
    citeulike-linkout-8 = {http://www.amazon.com/gp/search?keywords=020161622X\&index=books\&linkCode=qs},
    citeulike-linkout-9 = {http://www.librarything.com/isbn/020161622X},
    day = {30},
    howpublished = {Paperback},
    isbn = {020161622X},
    keywords = {book, programming, software\_engineering},
    month = {October},
    posted-at = {2007-02-20 18:29:46},
    priority = {0},
    publisher = {Addison-Wesley Professional},
    title = {The Pragmatic Programmer: From Journeyman to Master},
    url = {http://www.amazon.com/exec/obidos/redirect?tag=citeulike07-20\&path=ASIN/020161622X},
    year = {1999}
}

@book{citeulike:115158,
    abstract = {\_Design Patterns\_ is based on the idea that there are only so many design
problems in computer programming. This book identifies some common program-
design problems--such as adapting the interface of one object to that of
another object or notifying an object of a change in another object's state--
and explains the best ways (not always the obvious ways) that the authors know
to solve them. The idea is that you can use the authors' sophisticated design
ideas to solve problems that you often waste time solving over and over again
in your own programming.

The authors have come up with some ingenious ways to solve some common
vexations among object-oriented programmers. Want to build a page-layout
program that embeds inline images among characters of various sizes? How about
building a program that converts files of one format to another? Chances are,
some programmer already has thought of a better solution than you will and the
recipes you need are here. Solutions are presented in generalised diagrams of
data and logic structures. The idea is that you can take the concepts
presented here and adapt them--in whatever language you use--to your
individual situation. You may have to read some of the chapters several times
before you fully understand them, but when you find a solution in this book,
it will make your job easier and your results more elegant. --\_Jake Bond\_},
    author = {Gamma, Erich and Helm, Richard and Johnson, Ralph and Vlissides, John M.},
    citeulike-article-id = {115158},
    citeulike-linkout-0 = {http://www.amazon.ca/exec/obidos/redirect?tag=citeulike09-20\&amp;path=ASIN/0201633612},
    citeulike-linkout-1 = {http://www.amazon.de/exec/obidos/redirect?tag=citeulike01-21\&amp;path=ASIN/0201633612},
    citeulike-linkout-2 = {http://www.amazon.fr/exec/obidos/redirect?tag=citeulike06-21\&amp;path=ASIN/0201633612},
    citeulike-linkout-3 = {http://www.amazon.jp/exec/obidos/ASIN/0201633612},
    citeulike-linkout-4 = {http://www.amazon.co.uk/exec/obidos/ASIN/0201633612/citeulike00-21},
    citeulike-linkout-5 = {http://www.amazon.com/exec/obidos/redirect?tag=citeulike07-20\&path=ASIN/0201633612},
    citeulike-linkout-6 = {http://www.worldcat.org/isbn/0201633612},
    citeulike-linkout-7 = {http://books.google.com/books?vid=ISBN0201633612},
    citeulike-linkout-8 = {http://www.amazon.com/gp/search?keywords=0201633612\&index=books\&linkCode=qs},
    citeulike-linkout-9 = {http://www.librarything.com/isbn/0201633612},
    day = {10},
    edition = {1},
    howpublished = {Hardcover},
    isbn = {0201633612},
    keywords = {book, design\_patterns, oop, software\_design},
    month = {November},
    posted-at = {2007-02-20 18:29:07},
    priority = {0},
    publisher = {Addison-Wesley Professional},
    title = {Design Patterns: Elements of Reusable Object-Oriented Software},
    url = {http://www.amazon.com/exec/obidos/redirect?tag=citeulike07-20\&path=ASIN/0201633612},
    year = {1994}
}

@inproceedings{citeulike:1108550,
    address = {New York, NY, USA},
    author = {Mandl, Thomas},
    booktitle = {HYPERTEXT '06: Proc. of the seventeenth conf. on Hypertext and hypermedia},
    citeulike-article-id = {1108550},
    citeulike-linkout-0 = {http://portal.acm.org/citation.cfm?id=1149957},
    citeulike-linkout-1 = {http://dx.doi.org/10.1145/1149941.1149957},
    doi = {10.1145/1149941.1149957},
    isbn = {1595934170},
    keywords = {information\_retrieval},
    pages = {73--84},
    posted-at = {2007-02-15 20:37:57},
    priority = {2},
    publisher = {ACM Press},
    title = {Implementation and evaluation of a quality-based search engine},
    url = {http://dx.doi.org/10.1145/1149941.1149957},
    year = {2006}
}

@article{citeulike:658201,
    abstract = {Research tools critical for exploratory search success involve the creation of new interfaces that move the process beyond predictable fact retrieval.},
    address = {New York, NY, USA},
    author = {Marchionini, Gary},
    citeulike-article-id = {658201},
    citeulike-linkout-0 = {http://portal.acm.org/citation.cfm?id=1121979},
    citeulike-linkout-1 = {http://dx.doi.org/10.1145/1121949.1121979},
    doi = {10.1145/1121949.1121979},
    issn = {0001-0782},
    journal = {Commun. ACM},
    keywords = {information\_retrieval},
    month = {April},
    number = {4},
    pages = {41--46},
    posted-at = {2007-02-15 20:37:41},
    priority = {2},
    publisher = {ACM},
    title = {Exploratory search: from finding to understanding},
    url = {http://dx.doi.org/10.1145/1121949.1121979},
    volume = {49},
    year = {2006}
}

@article{citeulike:1108693,
    address = {New York, NY, USA},
    author = {Stacy, Webb and Macmillan, Jean},
    citeulike-article-id = {1108693},
    citeulike-linkout-0 = {http://portal.acm.org/citation.cfm?id=203256},
    citeulike-linkout-1 = {http://dx.doi.org/10.1145/203241.203256},
    doi = {10.1145/203241.203256},
    issn = {0001-0782},
    journal = {Commun. ACM},
    keywords = {software\_engineering},
    month = {June},
    number = {6},
    pages = {57--63},
    posted-at = {2007-02-15 20:36:56},
    priority = {3},
    publisher = {ACM Press},
    title = {Cognitive bias in software engineering},
    url = {http://dx.doi.org/10.1145/203241.203256},
    volume = {38},
    year = {1995}
}

@article{citeulike:755360,
    abstract = {Radio frequency identification is a wireless communication technology that lets computers read the identity of inexpensive electronic tags from a distance without requiring a battery in the tags. As RFID technology matures, it will likely unleash a new wave of applications that will exploit inexpensive and highly available automatic identification.},
    author = {Nath, B. and Reynolds, F. and Want, R.},
    citeulike-article-id = {755360},
    citeulike-linkout-0 = {http://dx.doi.org/10.1109/MPRV.2006.13},
    citeulike-linkout-1 = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=1593567},
    comment = {This is a 3-page article that introduces some other papers in the same issue of the journal.  This isn't much of a paper in its self, but it does mention some ideas of the applications of RFID technology.},
    doi = {10.1109/MPRV.2006.13},
    journal = {Pervasive Computing, IEEE},
    keywords = {context\_aware, rfid, summary},
    number = {1},
    pages = {22--24},
    posted-at = {2007-02-15 20:32:36},
    priority = {0},
    title = {RFID Technology and Applications},
    url = {http://dx.doi.org/10.1109/MPRV.2006.13},
    volume = {5},
    year = {2006}
}

@article{citeulike:1105071,
    abstract = {They believe that planes come from paradise---their ancestors sent them. But the white man, a crafty pirate, manages to get his hands on them by attracting them into a big trap of an airport. You build your plane too, and wait with faith. Sooner of later, your ancestors will discover the white man's trap and will guide the planes on your landing strip. Then you will be rich and happy. ---Narration from the film   Mondo Cane , 1962},
    address = {New York, NY, USA},
    author = {Holmquist, Lars E.},
    citeulike-article-id = {1105071},
    citeulike-linkout-0 = {http://portal.acm.org/citation.cfm?id=1052465},
    citeulike-linkout-1 = {http://dx.doi.org/10.1145/1052438.1052465},
    comment = {This paper discusses the problems with presenting prototypes that appear to be very real under the guise of a finished product.  One example given is the idea of a context-aware cell phone:  It's trivial to concoct a wizard-of-oz prototype that looks and acts just as the real phone should, but the actual technology is to implement it autonomously is not yet available, and can lead to problems of misconception or unrealistic expectations.},
    doi = {10.1145/1052438.1052465},
    issn = {1072-5520},
    journal = {interactions},
    keywords = {interface\_design, prototyping},
    number = {2},
    pages = {48--54},
    posted-at = {2007-02-13 21:43:58},
    priority = {0},
    publisher = {ACM},
    title = {Prototyping: generating ideas or cargo cult designs?},
    url = {http://dx.doi.org/10.1145/1052438.1052465},
    volume = {12},
    year = {2005}
}

@inproceedings{citeulike:664041,
    abstract = {The panel will explore the relevance of the emerging tagging systems (Flickr, Del.icio.us, RawSugar and more). Why do they seem to work? What kinds of incentives are required for users to participate? Will tagging survive and scale to mass adoption? What are the behavioral, economic, and social models that underlie each tagging system? What are the dynamics of those systems, and how are they derived from the specific application's design and affordances?.We will demand answers to these questions and others from some of the pioneering practitioners and academics in the field. Bring your wireless laptop to participate in a live tagging experiment! The experiment results will be shown and discussed at the end of the panel. To add to the fun, parts of the discussion will be motivated by short video segments.},
    address = {New York, NY, USA},
    author = {Furnas, George W. and Fake, Caterina and von Ahn, Luis and Schachter, Joshua and Golder, Scott and Fox, Kevin and Davis, Marc and Marlow, Cameron and Naaman, Mor},
    booktitle = {CHI '06: CHI '06 extended abstracts on Human factors in computing systems},
    citeulike-article-id = {664041},
    citeulike-linkout-0 = {http://portal.acm.org/citation.cfm?id=1125451.1125462},
    citeulike-linkout-1 = {http://dx.doi.org/10.1145/1125451.1125462},
    doi = {10.1145/1125451.1125462},
    isbn = {1-59593-298-4},
    keywords = {tagging},
    location = {Montr\'{e}al, Qu\'{e}bec, Canada},
    pages = {36--39},
    posted-at = {2007-02-06 18:03:46},
    priority = {2},
    publisher = {ACM},
    title = {Why do tagging systems work?},
    url = {http://dx.doi.org/10.1145/1125451.1125462},
    year = {2006}
}

@inproceedings{citeulike:617156,
    abstract = {We describe a social bookmarking service de-signed for a large enterprise. We discuss design principles addressing online identity, privacy, information discovery (including search and pivot browsing), and service extensi-bility based on a web-friendly architectural style. In addi-tion we describe the key design features of our implementa-tion. We provide the results of an eight week field trial of this enterprise social bookmarking service, including a de-scription of user activities, based on log file analysis. We share the results of a user survey focused on the benefits of the service. The feedback from the user trial, comprising survey results, log file analysis and informal communica-tions, is quite positive and suggests several promising en-hancements to the service. Finally, we discuss potential extension and integration of social bookmarking services with other corporate collaborative applications.},
    address = {New York, NY, USA},
    author = {Millen, David R. and Feinberg, Jonathan and Kerr, Bernard},
    booktitle = {CHI '06: Proc. of the SIGCHI conf. on Human Factors in computing systems},
    citeulike-article-id = {617156},
    citeulike-linkout-0 = {http://portal.acm.org/citation.cfm?id=1124792},
    citeulike-linkout-1 = {http://dx.doi.org/10.1145/1124772.1124792},
    comment = {Corporate tagged bookmarking system at IBM.},
    doi = {10.1145/1124772.1124792},
    isbn = {1-59593-372-7},
    keywords = {chi06, collaboration, tagging},
    location = {Montr\'{e}al, Qu\'{e}bec, Canada},
    pages = {111--120},
    posted-at = {2007-02-06 18:03:25},
    priority = {0},
    publisher = {ACM},
    title = {Dogear: Social bookmarking in the enterprise},
    url = {http://dx.doi.org/10.1145/1124772.1124792},
    year = {2006}
}

@inproceedings{MarlowEtAl2006:TaggingPaper,
    abstract = {In recent years, tagging systems have become increasingly popular. These systems enable users to add keywords (i.e., "tags") to Internet resources (e.g., web pages, images, videos) without relying on a controlled vocabulary. Tagging systems have the potential to improve search, spam detection, reputation systems, and personal organization while introducing new modalities of social communication and opportunities for data mining. This potential is largely due to the social structure that underlies many of the current systems.Despite the rapid expansion of applications that support tagging of resources, tagging systems are still not well studied or understood. In this paper, we provide a short description of the academic related work to date. We offer a model of tagging systems, specifically in the context of web-based systems, to help us illustrate the possible benefits of these tools. Since many such systems already exist, we provide a taxonomy of tagging systems to help inform their analysis and design, and thus enable researchers to frame and compare evidence for the sustainability of such systems. We also provide a simple taxonomy of incentives and contribution models to inform potential evaluative frameworks. While this work does not present comprehensive empirical results, we present a preliminary study of the photo-sharing and tagging system Flickr to demonstrate our model and explore some of the issues in one sample system. This analysis helps us outline and motivate possible future directions of research in tagging systems.},
    address = {New York, NY, USA},
    author = {Marlow, Cameron and Naaman, Mor and Boyd, Danah and Davis, Marc},
    booktitle = {HYPERTEXT '06: Proc. of the seventeenth conf. on Hypertext and hypermedia},
    citeulike-article-id = {816066},
    citeulike-linkout-0 = {http://portal.acm.org/citation.cfm?id=1149949},
    citeulike-linkout-1 = {http://dx.doi.org/10.1145/1149941.1149949},
    comment = {Briefly discusses the implications of different tagging strategies (eg: free-for-all tagging, using viewable and/or suggested tags, etc.)},
    doi = {10.1145/1149941.1149949},
    isbn = {1-59593-417-0},
    keywords = {collaboration, ht06, tagging},
    location = {Odense, Denmark},
    pages = {31--40},
    posted-at = {2007-02-06 17:05:53},
    priority = {0},
    publisher = {ACM},
    title = {HT06, tagging paper, taxonomy, Flickr, academic article, to read},
    url = {http://dx.doi.org/10.1145/1149941.1149949},
    year = {2006}
}

@inproceedings{PrabakerEtAl2006:DocWizards,
    abstract = {Much existing documentation is informal and serves to communicate "how-to" knowledge among restricted working groups. Using current practices, such documentation is both difficult to maintain and difficult to use properly.In this paper, we propose a documentation system, called DocWizards, that uses programming by demonstration to support low-cost authoring and guided walkthrough techniques to improve document usability.We report a comparative study between the use of DocWizards and traditional techniques for authoring and following documentation. The study participants showed significant gains in efficiency and reduction in error rates when using DocWizards. In addition, they expressed a clear preference for using the DocWizards tool, both for authoring and for following documentation.},
    address = {New York, NY, USA},
    author = {Prabaker, Madhu and Bergman, Lawrence and Castelli, Vittorio},
    booktitle = {CHI '06: Proc. of the SIGCHI conf. on Human Factors in computing systems},
    citeulike-article-id = {1084681},
    citeulike-linkout-0 = {http://portal.acm.org/citation.cfm?id=1124809},
    citeulike-linkout-1 = {http://dx.doi.org/10.1145/1124772.1124809},
    doi = {10.1145/1124772.1124809},
    isbn = {1595933727},
    keywords = {authoring, chi06, pbd},
    pages = {241--250},
    posted-at = {2007-02-02 17:34:23},
    priority = {2},
    publisher = {ACM Press},
    title = {An evaluation of using programming by demonstration and guided walkthrough techniques for authoring and utilizing documentation},
    url = {http://dx.doi.org/10.1145/1124772.1124809},
    year = {2006}
}

@mastersthesis{Creswick2004:Assertions,
    address = {Corvallis, OR},
    author = {Creswick, Eugene R.},
    citeulike-article-id = {1084667},
    keywords = {assertions, end\_user\_programmers, graphs, software\_engineering, thesis},
    month = {October},
    posted-at = {2007-02-02 17:19:40},
    priority = {0},
    school = {School of Electrical Engineering and Computer Science, Oregon State University},
    title = {Dynamic, Incremental Assertion Propagation in End User Programming},
    year = {2004}
}

@techreport{CreswickEtAl2005:MacroIllustrator,
    abstract = {This paper presents an approach that enlivens existing documentation,
to efficiently support users performing procedural
tasks. We describe an approach that automatically creates,
in real-time, correspondences between the actions performed
by users and the steps described in online documentation
for the procedure being performed. Using these correspondences,
our Macro Illustrator system can highlight the
relevant portions of the documentation and provide the user
with a visual indication of the progress being made. Users
are therefore offered in-context documentation, which helps
them track their current position in the procedure, and answers
questions about the next steps to be taken. This approach
works with existing documentation, requiring no additional
markup on the part of the documentation author. We
present an algorithm for extracting actions from documentation
and aligning these extracted document actions with observed
user actions through a system of similarity metrics.
Empirical evaluation of this algorithm shows that it performs
significantly better than a strawman approach.},
    address = {Hawthorne, NY},
    author = {Creswick, Eugene R. and Bergman, Lawrence and Lau, Tessa A. and Castelli, Vittorio and Oblinger, Daniel A.},
    citeulike-article-id = {1084663},
    institution = {IBM TJ Watson},
    keywords = {concept\_extraction, pbd},
    month = {August},
    posted-at = {2007-02-02 17:14:58},
    priority = {0},
    title = {Illustrating Macros with Existing Documentation},
    volume = {RC236696},
    year = {2005}
}

@article{RuthruffEtAl2005:InteractiveVisFL,
    abstract = {End-user programmers are writing an unprecedented number of programs, primarily using languages and environments that incorporate a number of interactive and visual programming techniques. To help these users debug these programs, we have developed an entirely visual, interactive approach to fault localization. This paper presents the approach. We also present the results of a think-aloud study that examined interactive, human-centric issues that arise in end-user debugging using a fault localization approach. Our results provide insights into the contributions such approaches can make to the end-user debugging process.},
    author = {Ruthruff, Joseph R. and Prabhakararao, Shreenivasarao and Reichwein, James and Cook, Curtis and Creswick, Eugene R. and Burnett, Margaret},
    citeulike-article-id = {1084566},
    comment = {http://csce.unl.edu/\~{}ruthruff/papers/jvlc05-faultloc/JVLC05-FaultLoc.pdf},
    journal = {Journal of Visual Languages and Computing},
    keywords = {end\_user\_programmers, fault\_localization, jvlc05, software\_engineering},
    month = {February},
    number = {1-2},
    pages = {3--40},
    posted-at = {2007-02-02 17:06:33},
    priority = {0},
    title = {Interactive, Visual Fault Localization Support for End-User Programmers},
    volume = {16},
    year = {2005}
}

@article{citeulike:163216,
    abstract = {Note: OCR errors may be found in this Reference List extracted from the full text article.  ACM has opted to expose the complete List rather than only correct and linked references.},
    address = {New York, NY, USA},
    author = {Shneiderman, Ben and Maes, Pattie},
    citeulike-article-id = {163216},
    citeulike-linkout-0 = {http://portal.acm.org/citation.cfm?id=267514},
    citeulike-linkout-1 = {http://dx.doi.org/10.1145/267505.267514},
    doi = {10.1145/267505.267514},
    issn = {1072-5520},
    journal = {interactions},
    number = {6},
    pages = {42--61},
    posted-at = {2007-02-01 22:38:29},
    priority = {3},
    publisher = {ACM},
    title = {Direct manipulation vs. interface agents},
    url = {http://dx.doi.org/10.1145/267505.267514},
    volume = {4},
    year = {1997}
}

@inproceedings{Prabhakararao2003:FL,
    address = {Auckland, New Zealand},
    author = {Prabhakararao, Shreenivasarao and Cook, Curtis and Ruthruff, Joseph and Creswick, Eugene R. and Main, Martin and Durham, Michael and Burnett, Margaret},
    booktitle = {HCC '03: Proc. of the 2003 IEEE Symposium on Human Centric Computing Languages and Environments},
    citeulike-article-id = {1082461},
    citeulike-linkout-0 = {http://portal.acm.org/citation.cfm?id=1153961},
    day = {28-31},
    isbn = {0780382250},
    keywords = {end\_user\_programmers, fault\_localization, hcc03, software\_engineering},
    month = {October},
    pages = {15--22},
    posted-at = {2007-02-01 20:26:30},
    priority = {0},
    publisher = {IEEE Computer Society},
    title = {Strategies and behaviors of end-user programmers with interactive fault localization},
    url = {http://portal.acm.org/citation.cfm?id=1153961},
    year = {2003}
}

@inproceedings{ruthruffEtAl2003:SoftViz,
    address = {San Diego, CA, USA},
    author = {Ruthruff, Joseph and Creswick, Eugene R. and Burnett, Margaret and Cook, Curtis and Prabhakararao, Shreenivasarao and Ii, Marc F. and Main, Martin},
    booktitle = {SoftVis '03: Proc. of the 2003 ACM symposium on Software visualization},
    citeulike-article-id = {1082460},
    citeulike-linkout-0 = {http://portal.acm.org/citation.cfm?id=774851},
    citeulike-linkout-1 = {http://dx.doi.org/10.1145/774833.774851},
    day = {11-13},
    doi = {10.1145/774833.774851},
    isbn = {1581136420},
    keywords = {end\_user\_programmers, fault\_localization, softvis03, software\_engineering, visualization},
    month = {June},
    pages = {123--132},
    posted-at = {2007-02-01 20:25:23},
    priority = {0},
    publisher = {ACM Press},
    title = {End-user software visualizations for fault localization},
    url = {http://dx.doi.org/10.1145/774833.774851},
    year = {2003}
}

@electronic{citeulike:1074519,
    abstract = {This paper gives an overview of current trends in manual indexing on the Web.
Along with a general rise of user generated content there are more and more
tagging systems that allow users to annotate digital resources with tags
(keywords) and share their annotations with other users. Tagging is frequently
seen in contrast to traditional knowledge organization systems or as something
completely new. This paper shows that tagging should better be seen as a
popular form of manual indexing on the Web. Difference between controlled and
free indexing blurs with sufficient feedback mechanisms. A revised typology of
tagging systems is presented that includes different user roles and knowledge
organization systems with hierarchical relationships and vocabulary control. A
detailed bibliography of current research in collaborative tagging is included.},
    archivePrefix = {arXiv},
    author = {Voss, Jakob},
    citeulike-article-id = {1074519},
    citeulike-linkout-0 = {http://arxiv.org/abs/cs/0701072},
    citeulike-linkout-1 = {http://arxiv.org/pdf/cs/0701072},
    day = {26},
    eprint = {cs/0701072},
    keywords = {collaboration, tagging},
    month = {Jan},
    posted-at = {2007-02-01 20:17:00},
    priority = {2},
    title = {Tagging, Folksonomy \&amp; Co - Renaissance of Manual Indexing?},
    url = {http://arxiv.org/abs/cs/0701072},
    year = {2007}
}

@misc{citeulike:1082439,
    abstract = {In this paper we present a novel, customizable

IE paradigm that takes advantage

of predicate-argument structures. We

also introduce a new way of automatically

identifying predicate argument structures,

which is central to our IE paradigm. It is

based on: (1) an extended set of features;

and (2) inductive decision tree learning.},
    author = {Surdeanu, M. and Harabagiu, S. and Williams, J. and Aarseth, P.},
    citeulike-article-id = {1082439},
    citeulike-linkout-0 = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.5.8916},
    keywords = {information\_extraction, information\_retrieval},
    posted-at = {2007-02-01 20:05:36},
    priority = {3},
    title = {Using predicate-argument structures for information extraction},
    url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.5.8916},
    year = {2003}
}

@inproceedings{WolfmanEtAl2001:MixedInitiativeSMARTEdit,
    abstract = {Applications of machine learning can be viewed as teacher-student interactions in which the teacher provides training examples and the student learns a generalization of the training examples. One such application of great interest to the IUI community is adaptive user interfaces. In the traditional learning interface, the scope of teacher-student interactions consists solely of the teacher/user providing some number of training examples to the student/learner and testing the learned model on new examples. Active learning approaches go one step beyond the traditional interaction model and allow the student to propose new training examples that are then solved by the teacher. In this paper, we propose that interfaces for machine learning should even more closely resemble human teacher-student relationships. A teacher's time and attention are precious resources. An intelligent student must proactively contribute to the learning process, by reasoning about the quality of its knowledge, collaborating with the teacher, and suggesting new examples for her to solve. The paper describes a variety of rich interaction modes that enhance the learning process and presents a decision-theoretic framework, called DIAManD, for choosing the best interaction. We apply the framework to the SMARTedit programming by demonstration system and describe experimental validation and preliminary user feedback.},
    address = {New York, NY, USA},
    author = {Wolfman, Steven A. and Lau, Tessa and Domingos, Pedro and Weld, Daniel S.},
    booktitle = {IUI '01: Proc. of the 6th intl. conf. on Intelligent user interfaces},
    citeulike-article-id = {1082440},
    citeulike-linkout-0 = {http://portal.acm.org/citation.cfm?id=360332},
    citeulike-linkout-1 = {http://dx.doi.org/10.1145/359784.360332},
    doi = {10.1145/359784.360332},
    isbn = {1581133251},
    keywords = {mixed\_initiative, pbd, version\_spaces},
    pages = {167--174},
    posted-at = {2007-02-01 20:02:46},
    priority = {0},
    publisher = {ACM Press},
    title = {Mixed initiative interfaces for learning tasks: SMARTedit talks back},
    url = {http://dx.doi.org/10.1145/359784.360332},
    year = {2001}
}

@inproceedings{tlauEtAl2003:LearningPrograms,
    abstract = {While existing learning techniques can be viewed as inducing programs from examples, most research has focused on rather narrow classes of programs, e.g., decision trees or logic rules. In contrast, most of today's programs are written in languages such as C++ or Java. Thus, many tasks we wish to automate (e.g. programming by demonstration and software reverse engineering) might be best formulated as induction of code in a procedural language. In this paper we apply version space algebra [10] to learn such procedural programs given execution traces. We consider two variants of the problem (whether or not program-step information is included in the traces) and evaluate our implementation on a corpus of programs drawn from introductory computer science textbooks. We show that our system can learn correct programs from few traces.},
    address = {New York, NY, USA},
    author = {Lau, Tessa and Domingos, Pedro and Weld, Daniel S.},
    booktitle = {K-CAP '03: Proc. of the 2nd intl. conf. on Knowledge capture},
    citeulike-article-id = {1082438},
    citeulike-linkout-0 = {http://portal.acm.org/citation.cfm?id=945654},
    citeulike-linkout-1 = {http://dx.doi.org/10.1145/945645.945654},
    comment = {Learns python programs by watching variables in memory, iirc.},
    doi = {10.1145/945645.945654},
    isbn = {1581135831},
    keywords = {k-cap03, pbd, version\_spaces},
    pages = {36--43},
    posted-at = {2007-02-01 19:59:29},
    priority = {0},
    publisher = {ACM Press},
    title = {Learning programs from traces using version space algebra},
    url = {http://dx.doi.org/10.1145/945645.945654},
    year = {2003}
}

@article{tlauEtAl2003:VersionSpaceAlgebra,
    abstract = {Programming by demonstration enables users to easily personalize their applications, automating repetitive tasks simply by executing a few examples. We formalize programming by demonstration as a machine learning problem: given the changes in the application state that result from the user's demonstrated actions, learn the general program that maps from one application state to the next. We present a methodology for learning in this space of complex functions. First we extend version spaces to learn arbitrary functions, not just concepts. Then we introduce the version space algebra, a method for composing simpler version spaces to construct more complex spaces. Finally, we apply our version space algebra to the text-editing domain and describe an implemented system called SMARTedit that learns repetitive text-editing procedures by example. We evaluate our approach by measuring the number of examples required for the system to learn a procedure that works on the remainder of examples, and by an informal user study measuring the effort users spend using our system versus performing the task by hand. The results show that SMARTedit is capable of generalizing correctly from as few as one or two examples, and that users generally save a significant amount of effort when completing tasks with SMARTedit's help.},
    address = {Hingham, MA, USA},
    author = {Lau, Tessa and Wolfman, Steven A. and Domingos, Pedro and Weld, Daniel S.},
    citeulike-article-id = {1082435},
    citeulike-linkout-0 = {http://portal.acm.org/citation.cfm?id=940878},
    issn = {0885-6125},
    journal = {Machine Learning},
    keywords = {ml03, pbd, version\_spaces},
    number = {1-2},
    pages = {111--156},
    posted-at = {2007-02-01 19:56:55},
    priority = {0},
    publisher = {Kluwer Academic Publishers},
    title = {Programming by Demonstration Using Version Space Algebra},
    url = {http://portal.acm.org/citation.cfm?id=940878},
    volume = {53},
    year = {2003}
}

@inproceedings{AllanJ1996:HypetextLinking,
    abstract = {HyPursuit is a new hierarchical network search engine
that clusters hypertext documents to structure a given
information space for browsing and search activities.
Our content-link clustering algorithm is based on the
semantic information embedded in hyperlink structures
and document contents. HyPursuit admits multiple,
coexisting cluster hierarchies based on dierent prin-
ciples for grouping documents, such as the Library of
Congress catalog scheme and automatically created hy-
pertext clusters.
HyPursuit's abstraction functions summarize cluster con-
tents to support scalable query processing. The abstrac-
tion functions satisfy system resource limitations with
controlled information loss. The result of query pro-
cessing operations on a cluster summary approximates
the result of performing the operations on the entire in-
formation space. We constructed a prototype system
comprising 100 leaf World Wide Web sites and a hier-
archy of 42 servers that route queries to the leaf sites.
Experience with our system suggests that abstraction
functions based on hypertext clustering can be used to
construct meaningful and scalable cluster hierarchies.
We are also encouraged by preliminary results on clus-
tering based on both document contents and hyperlink
structures.},
    address = {New York, NY, USA},
    author = {Allan, James},
    booktitle = {HYPERTEXT '96: Proc. of the the seventh ACM conf. on Hypertext},
    citeulike-article-id = {1082429},
    citeulike-linkout-0 = {http://dx.doi.org/http://doi.acm.org/10.1145/234828.234833},
    day = {16},
    doi = {http://doi.acm.org/10.1145/234828.234833},
    keywords = {clustering, document\_clustering, document\_linking, ht96, wiki},
    month = {March},
    pages = {42--52},
    posted-at = {2007-02-01 19:48:12},
    priority = {0},
    publisher = {ACM Press},
    title = {Automatic hypertext link typing},
    url = {http://dx.doi.org/http://doi.acm.org/10.1145/234828.234833},
    year = {1996}
}

@inproceedings{OrenEtAl2006:AnnotationInWiki,
    abstract = {Semantic Wikis allow users to semantically annotate their
Wiki content. The particular annotations can differ in expressive power,
simplicity, and meaning. We present an elaborate conceptual model for
semantic annotations, introduce a unique and rich Wiki syntax for these
annotations, and discuss how to best formally represent the augmented
Wiki content. We improve existing navigation techniques to automatically
construct faceted browsing for semistructured data. By utilising
the Wiki annotations we provide greatly enhanced information retrieval.
Further we report on our ongoing development of these techniques in our
prototype SemperWiki.},
    author = {Oren, Eyal and Delbru, Renaud and Moller, Knud and Volkel, Max and Handschuh, Siegfried},
    booktitle = {Proc. of the First Workshop on Semantic Wikis},
    citeulike-article-id = {1082427},
    comment = {http://www.eswc2006.org/technologies/usb/proceedings-workshops/eswc2006-workshop-semantic-wikis.pdf

(page 66 of above pdf)
excerpt: 

"We improve existing navigation techniques to automatically construct faceted browsing for semistructured data" (from the abstract). Makes use of formal (RDF) annotations of each page, imposing quite a few restrictions on how you can annotate, and presenting somewhat of a barrier to creating annotations."},
    day = {15},
    editor = {Volkel, Max},
    keywords = {annotation, semantics, semwiki06, wiki},
    month = {May},
    posted-at = {2007-02-01 19:45:54},
    priority = {0},
    title = {Annotation and Navigation in Semantic Wikis},
    year = {2006}
}

@proceedings{SemWiki2006,
    abstract = {Dear Reader, The community of Semantic Wiki researchers has probably first met at the dinner table of the Semantic Desktop Workshop, ISWC 2005 in Galway, Ireland. It was that very night, were the idea of the ”First Workshop on Semantic Wikis” and a mailing list were born. Since then, much has happened. The Topic of Semantic Wikis has evolved from a an obscure side-topic to one of interest for a broad community. Our mailing list1 has grown from twenty to over hundred subscribers. As the diversity of papers at this workshop shows, the field of Semantic Wiki research is quite diverse. We see papers on semantic wiki engines, a multitude of ways to combine wiki and semantic web ideas, and application of semantic wikis to bioscience, mathematics, e-learning, and multimedia. Semantic Wikis are currently explored from two sides: Wikis augmented with Semantic Web technology and Semantic Web applications being wiki-fied. In essence, wikis are portals with an editing component. Semantic Wikis can close the ”annotation bottleneck” of the Semantic Web – currently, we have many techniques and tools, but few data to apply them. We will change that. We wish to thank all authors that spend their nights contributing to this topic and thereby made the workshop possible. The high number of good submissions made the work for the programm committee members even more difficult – thank you all for your work. Many thanks also to the ESWC organisation team, which decided in the last minute to grant us a full-day workshop. Let's continue to bring the wiki-spirit to the Semantic Web and enjoy reading the proceedings. Karlsruhe, May 2006 Max V¨olkel, Sebastian Schaffert and Stefan Decker},
    booktitle = {First Workshoop on Semantic Wikis},
    citeulike-article-id = {1082423},
    comment = {http://www.eswc2006.org/technologies/usb/proceedings-workshops/eswc2006-workshop-semantic-wikis.pdf

This is the full proceedings.},
    day = {15},
    editor = {Volkel, Max},
    keywords = {proceedings, semantics, semwiki06, wiki},
    month = {May},
    posted-at = {2007-02-01 19:40:09},
    priority = {0},
    title = {The first Workshop on Semantic Wikis - From Wiki to Semantics},
    year = {2006}
}

@techreport{Andrews-Rajman2004:ThematicAnnotation,
    abstract = {Semantic document annotation may be useful for many tasks. In particular, in
the framework of the MDM project1, topical annotation – i.e. the annotation of
document segments with tags identifying the topics discussed in the segments – is
used to enhance the retrieval of multimodal meeting records. Indeed, with such
an annotation, meeting retrieval can integrate topics in the search criteria offered
to the users.
Contrarily to standard approaches to topic annotation, the technique used in
this work does not centraly rely on some sort of – possibly statistical – keyword
extraction. In fact, the proposed annotation algorithm uses a large scale semantic
database – the EDR Electronic Dictionary2 – that provides a concept hierarchy
based on hyponym and hypernym relations.This concept hierarchy is used to gen-
erate a synthetic representation of the document by aggregating the words present
in topically homogeneous document segments into a set of concepts best preserving
the document's content.
The identification of the topically homogeneous segments – often called Text
Tiling – is performed to ease the computation as the algorithm will work on
smaller text fragments. In addition, it is believed to improve the precision of the
extraction as it is performed on topically homogeneous segments. For this task,
a standard techniques – proposed by [Hea94] – relying on similarity computation
based on vector space representations have been implemented. Hence, the main
challenge in the project was to create a novel topic identification algorithm, based
on the available semantic resource, that produces good results when applied on
the automatically generated segments.
This new extraction technique uses an unexplored approach to topic selection.
Instead of using semantic similarity measures based on a semantic resource, the
later is processed to extract the part of the conceptual hierarchy relevant to the
document content. Then this conceptual hierarchy is searched to extract the most
relevant set of concepts to represent the topics discussed in the document. Notice
that this algorithm is able to extract generic concepts that are not directly present
in the document.
The segmentation algorithm was evaluated on the Reuters corpus, composed
of 806'791 news items. These items were aggregated to construct a single virtual
document where the algorithm had to detect boundaries. These automatically generated segments were then compared to the initial news items and a metric
has been developed to evaluate the accuracy of the algorithm.
The proposed approach for topic extraction was experimentally tested and eval-
uated on a database of 238 documents corresponding to bibliographic descriptions
extracted from the INSPEC database3. A novel evaluation metric was designed to
take into account the fact that the topics associated with the INSPEC descriptions
– taken as the golden truth for the evaluation – were not produced based on the
EDR dictionary, and therefore needed to be approximated by the available EDR
entries.
All together, the combination of existing document segmentation methods – i.e
text tiling – with novel topic identification ones leads to an additional document
annotation useful for more robust retrieval.},
    author = {Andrews, Pierre and Rajman, Martin},
    citeulike-article-id = {1082417},
    comment = {http://arxiv.org/pdf/cs.CL/0412117

    *  Uses a semantic resource (a set of dictionaries and a DAG of concept relationships) to do topic extraction from segmented documents.
          o EDR Electronic Dictionary: (the semantic resource used, similar to WordNet) http://www.iijnet.or.jp/edr/ 
    * document segmentation is done via a bag-of-words approach devised by M. Hearst [Marti Hearst. Multi-paragraph segmentation of expository text. in 32nd Anual meeting of the Association for Computational Linguistics, pp. 9-16, 1994] },
    howpublished = {Technical Report},
    institution = {Swiss Federal Institute of Technology, Lausanne},
    keywords = {concept\_extraction, information\_retrieval},
    month = {August},
    posted-at = {2007-02-01 19:34:07},
    priority = {0},
    title = {Thematic Annotation: extracting concepts out of documents.},
    year = {2004}
}

